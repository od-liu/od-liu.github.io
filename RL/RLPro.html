<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>核心优化问题 - OD&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="OD&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="OD&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="1. 强化学习的最终目标无论我们如何衡量回报（无论是无限时间范围的折扣回报，还是有限时间范围的无折扣回报），也无论我们选择何种策略，强化学习（RL）的最终目标始终是：  找到一个策略，当智能体依据这个策略行动时，能够最大化其期望回报。  这个目标是所有强化学习算法驱动力的核心。 2. 理解期望回报：轨迹的概率为了讨论“期望回报”，我们首先必须能够量化一个“轨迹”（trajectory）发生的可能性"><meta property="og:type" content="blog"><meta property="og:title" content="核心优化问题"><meta property="og:url" content="http://example.com/RL/RLPro.html"><meta property="og:site_name" content="OD&#039;s blog"><meta property="og:description" content="1. 强化学习的最终目标无论我们如何衡量回报（无论是无限时间范围的折扣回报，还是有限时间范围的无折扣回报），也无论我们选择何种策略，强化学习（RL）的最终目标始终是：  找到一个策略，当智能体依据这个策略行动时，能够最大化其期望回报。  这个目标是所有强化学习算法驱动力的核心。 2. 理解期望回报：轨迹的概率为了讨论“期望回报”，我们首先必须能够量化一个“轨迹”（trajectory）发生的可能性"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:published_time" content="2025-07-07T13:15:45.000Z"><meta property="article:modified_time" content="2025-07-07T13:23:43.688Z"><meta property="article:author" content="Jiamin Liu"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/RL/RLPro.html"},"headline":"核心优化问题","image":["http://example.com/img/og_image.png"],"datePublished":"2025-07-07T13:15:45.000Z","dateModified":"2025-07-07T13:23:43.688Z","author":{"@type":"Person","name":"Jiamin Liu"},"publisher":{"@type":"Organization","name":"OD's blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"1. 强化学习的最终目标无论我们如何衡量回报（无论是无限时间范围的折扣回报，还是有限时间范围的无折扣回报），也无论我们选择何种策略，强化学习（RL）的最终目标始终是：  找到一个策略，当智能体依据这个策略行动时，能够最大化其期望回报。  这个目标是所有强化学习算法驱动力的核心。 2. 理解期望回报：轨迹的概率为了讨论“期望回报”，我们首先必须能够量化一个“轨迹”（trajectory）发生的可能性"}</script><link rel="canonical" href="http://example.com/RL/RLPro.html"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="OD&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-07-07T13:15:45.000Z" title="2025/7/7 21:15:45">2025-07-07</time>发表</span><span class="level-item"><time dateTime="2025-07-07T13:23:43.688Z" title="2025/7/7 21:23:43">2025-07-07</time>更新</span><span class="level-item"><a class="link-muted" href="/"></a></span><span class="level-item">5 分钟读完 (大约822个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">核心优化问题</h1><div class="content"><h4 id="1-强化学习的最终目标"><a href="#1-强化学习的最终目标" class="headerlink" title="1. 强化学习的最终目标"></a>1. 强化学习的最终目标</h4><p>无论我们如何衡量回报（无论是无限时间范围的折扣回报，还是有限时间范围的无折扣回报），也无论我们选择何种策略，强化学习（RL）的最终目标始终是：</p>
<blockquote>
<p><strong>找到一个策略，当智能体依据这个策略行动时，能够最大化其期望回报。</strong></p>
</blockquote>
<p>这个目标是所有强化学习算法驱动力的核心。</p>
<h4 id="2-理解期望回报：轨迹的概率"><a href="#2-理解期望回报：轨迹的概率" class="headerlink" title="2. 理解期望回报：轨迹的概率"></a>2. 理解期望回报：轨迹的概率</h4><p>为了讨论“期望回报”，我们首先必须能够量化一个“轨迹”（trajectory）发生的可能性。一个轨迹 $τ$ 是一个状态和动作的序列，例如 $τ = (s_0, a_0, s_1, a_1, …)$。</p>
<p>假设环境的动态（状态转移）和智能体的策略都是随机的。在这种情况下，一个包含 $T$ 个时间步的轨迹 $τ$ 发生的概率，取决于三个关键部分：</p>
<ol>
<li><strong>初始状态的概率</strong></li>
<li><strong>每一步状态转移的概率</strong></li>
<li><strong>每一步选择动作的概率</strong></li>
</ol>
<p>将这三者结合，我们得到一个轨迹的完整概率公式：</p>
<p>$P(\tau|\pi) = \rho_0(s_0) \prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \pi(a_t | s_t)$</p>
<p>让我们分解这个公式的每个部分：</p>
<ul>
<li>$\rho_0(s_0)$: 初始状态分布。它描述了在最开始时，环境处于状态 $s_0$ 的概率。</li>
<li>$P(s_{t+1} | s_t, a_t)$: 环境动力学模型。它描述了在状态 $s_t$ 下执行动作 $a_t$ 后，环境转移到下一个状态 $s_{t+1}$ 的概率。<strong>这部分由环境决定，智能体无法改变。</strong></li>
<li>$π(a_t | s_t)$: <strong>智能体的策略</strong>。它描述了在状态 $s_t$ 下，智能体选择动作 $a_t$ 的概率。<strong>这是我们通过学习来优化的部分。</strong></li>
</ul>
<h4 id="3-期望回报的正式定义"><a href="#3-期望回报的正式定义" class="headerlink" title="3. 期望回报的正式定义"></a>3. 期望回报的正式定义</h4><p>有了轨迹的概率，我们就可以正式地定义<strong>期望回报</strong> $J(π)$。它是在策略 $π$ 下，所有可能轨迹的回报 $R(τ)$ 的加权平均值，权重就是每个轨迹发生的概率 $P(τ|π)$。</p>
<p>其数学表达有两种等价形式：</p>
<ol>
<li><p><strong>积分形式</strong>:<br>$J(\pi) = \int_{\tau} P(\tau|\pi) R(\tau)$<br>这表示对<strong>所有可能</strong>的轨迹 $τ$ 进行积分（或求和），将每个轨迹的回报 $R(τ)$ 与其发生的概率 $P(τ|π)$ 相乘。</p>
</li>
<li><p><strong>期望形式</strong> (更紧凑的写法):<br>$J(\pi) = \underset{\tau \sim \pi}{\mathbb{E}}[R(\tau)]$</p>
</li>
</ol>
<h4 id="4-RL的核心优化问题"><a href="#4-RL的核心优化问题" class="headerlink" title="4. RL的核心优化问题"></a>4. RL的核心优化问题</h4><p>综上所述，强化学习的中心优化问题可以被简洁地表述为：</p>
<p>$\pi^{*} = \arg \max_{\pi} J(\pi)$</p>
<p>这里的符号含义是：</p>
<ul>
<li>$\pi^{*}$：代表<strong>最优策略 (optimal policy)</strong>。</li>
<li>$\arg \max_{\pi}$：这个算子意味着我们要找到一个参数（在这里是策略 $π$），使得后面的表达式（在这里是期望回报 $J(π)$）达到最大值。</li>
</ul>
<p><strong>简而言之，强化学习的全部工作，就是寻找那个能使期望回报 $J(π)$ 最大化的最优策略 $\pi^{*}$。</strong></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>核心优化问题</p><p><a href="http://example.com/RL/RLPro.html">http://example.com/RL/RLPro.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Jiamin Liu</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-07-07</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-07-07</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><!--!--></article></div><!--!--><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "70097fee92956d235bfb2d82fe17b52f",
            repo: "od-liu.github.io",
            owner: "od-Liu",
            clientID: "Ov23liWi9TgDXgT7hhEB",
            clientSecret: "054ac0bc0bc8afaac9a807be4b6af2274ee9bc10",
            admin: ["od-liu"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/1.png" alt="Jiamin Liu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jiamin Liu</p><p class="is-size-6 is-block">别卷我别卷我别卷我👊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">0</p></a></div></div></nav></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="OD&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2025 Jiamin Liu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>