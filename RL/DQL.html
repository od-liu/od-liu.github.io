<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>值函数的近似 - OD&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="OD&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="OD&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="idea: Approximate the state and action values using parameterized functions: $\hat{v}(s,w)\approx v_\pi(s)$, where $w\in R^m$ is the parameter vector. Advantage:(1) Storage: The dimension of $w$ may b"><meta property="og:type" content="blog"><meta property="og:title" content="值函数的近似"><meta property="og:url" content="http://example.com/RL/DQL.html"><meta property="og:site_name" content="OD&#039;s blog"><meta property="og:description" content="idea: Approximate the state and action values using parameterized functions: $\hat{v}(s,w)\approx v_\pi(s)$, where $w\in R^m$ is the parameter vector. Advantage:(1) Storage: The dimension of $w$ may b"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:published_time" content="2025-07-03T12:25:04.000Z"><meta property="article:modified_time" content="2025-07-07T13:47:03.949Z"><meta property="article:author" content="Jiamin Liu"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/RL/DQL.html"},"headline":"值函数的近似","image":["http://example.com/img/og_image.png"],"datePublished":"2025-07-03T12:25:04.000Z","dateModified":"2025-07-07T13:47:03.949Z","author":{"@type":"Person","name":"Jiamin Liu"},"publisher":{"@type":"Organization","name":"OD's blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"idea: Approximate the state and action values using parameterized functions: $\\hat{v}(s,w)\\approx v_\\pi(s)$, where $w\\in R^m$ is the parameter vector. Advantage:(1) Storage: The dimension of $w$ may b"}</script><link rel="canonical" href="http://example.com/RL/DQL.html"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="OD&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-07-03T12:25:04.000Z" title="2025/7/3 20:25:04">2025-07-03</time>发表</span><span class="level-item"><time dateTime="2025-07-07T13:47:03.949Z" title="2025/7/7 21:47:03">2025-07-07</time>更新</span><span class="level-item"><a class="link-muted" href="/"></a></span><span class="level-item">10 分钟读完 (大约1529个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">值函数的近似</h1><div class="content"><p><strong>idea</strong>: Approximate the state and action values using parameterized functions: $\hat{v}(s,w)\approx v_\pi(s)$, where $w\in R^m$ is the parameter vector.</p>
<p><strong>Advantage</strong>:<br>(1) Storage: The dimension of $w$ may be much less than $|\mathcal{S}|$.<br>(2) Generalization: the learned value can generated to unvisited states.</p>
<h3 id="Algorithm-for-state-value-estimation"><a href="#Algorithm-for-state-value-estimation" class="headerlink" title="Algorithm for state value estimation"></a>Algorithm for state value estimation</h3><p><strong>Objective function</strong><br>$v_\pi(s)$: the true value.<br>$\hat{v}(s,w)$: a function of approximation.</p>
<p>The objective function is $J(w)=E[(v_\pi(S)-\hat{v}(S,w))^2]$</p>
<ul>
<li>goal: to find the best $w$ that can minimize $J(w)$</li>
</ul>
<ol>
<li><p>the first way is to use a uniform distribution.<br>setting the probability of each state as $1/|S|$.<br>the objective function becomes $J(w)=1/|S|\sum_{s\in S}(v_\pi(S)-\hat{v}(S,w))^2$<br>缺点：所有状态同等重要（并不符合实际情况）</p>
</li>
<li><p>The second way is to use the stationary distribution.</p>
<ul>
<li>it describes the long-run behavior.</li>
<li>Let $\{d_\pi(s)\}_{s\in S}$ denote the <strong>stationary distribution</strong> of the Markov process unger policy $\pi$.<br>反映了达到平稳状态后某个状态被访问的概率为多大(需要算法运行很长时间后才可以达到)</li>
<li>The objective function can be rewritten as $J(w)=E[(v_\pi(S)-\hat{v}(S,w))^2]=\sum_{s\in S}d_\pi(s)(v_\pi(s)-\hat{v}(s,w))^2$.(这里是加权平均，权重大小是该状态在稳态过程中被访问的概率大小)</li>
</ul>
</li>
</ol>
<p><strong>Optimization algorithm</strong><br>该算法用于估计一个给定策略的state value.<br>Use the graident-descent algorithm: $w_{k+1}=w_k-\alpha_k\nabla_w J(w_k)$<br>The true gradient is </p>
<div>
$$\begin{aligned}
\nabla_w J(w)&=\nabla_w E[(v_\pi(S)-\hat{v}(S,w))^2]\\
&= E[\nabla_w(v_\pi(S)-\hat{v}(S,w))^2]\\
&=2 E[(v_\pi(S)-\hat{v}(S,w))(-\nabla_w\hat{v}(S,w))]
\\&=-2 E[(v_\pi(S)-\hat{v}(S,w))(\nabla_w\hat{v}(S,w))]
\end{aligned}
$$
</div>

<p>use stochastic gradient to replace the true gradient:<br>$w_{t+1}=w_t+\alpha_t (v_\pi(s_t)-\hat{v}(s_t,w_t))\nabla_w\hat{v}(s_t,w_t)$<br>这个算法需要$v_\pi(s_t)$的真实值，所以是不可行的<br>Usa Monte Carlo or TD learning with function approximation to approximate $v_t(s_t)$</p>
<p>Pseudocode:TD learning with function approximation:</p>
<ul>
<li>Initialization: A function $\hat{v}(s,w)$ that is a differentiable in $w$. Iniitial parameter $w_0$.</li>
<li>Aim: Approximate the true state values of a given policy $\pi$.</li>
</ul>
<p>For each epsiode generated following the policy $\pi$, do<br>&emsp;For each step $(s_t,r_{t+1},s_{t+1})$, do<br>&emsp;&emsp;In general case, $w_{t+1}=w_{t}+\alpha_t[r_{t+1}+\gamma\hat{v}(s_{t+1},w_t)-\hat{v}(s_t,w_t)]\nabla_w \hat{v}(s_t,w_t)$<br>&emsp;&emsp;In linear case, $w_{t+1}=w_{t}+\alpha_t[r_{t+1}+\gamma \phi ^T(s_{t+1})w_t-\phi^T(s_t)w_t]\phi(s_t)$.</p>
<p><strong>Selection of function approximators</strong></p>
<ol>
<li>use a linear function $\hat{v}(s,w)=\phi^T(s)w$ to approximate approximate $\hat{v}(s,w)$<br>$\nabla_w \hat{v(s,w)}=\phi(s)$.<br>劣势：需要选取何时的feature vectors.<br>优势：理论性质可以被比较好的分析；有比较强的表征能力，可以表征大部分函数.</li>
<li>神经网络（非线性）</li>
</ol>
<h3 id="Sarsa-with-function-approximation"><a href="#Sarsa-with-function-approximation" class="headerlink" title="Sarsa with function approximation"></a>Sarsa with function approximation</h3><p>估计action value.<br>The sarsa algorithm with value function approximation is:<br>$w_{t+1}=w_t+\alpha_t[r_{t+1}+\gamma \hat{q}(s_{t+1},a_{t+1},w_t)-\hat{q}(s_t,a_t,w_t)]\nabla_w\hat q(s_t,a_t,w_t)$.</p>
<p>Pseudocode:Sarsa with function approximation:</p>
<ul>
<li>Aim: Search a policy that can lead the agent to the target from an initial state-action pair $(s_0,a_0)$.</li>
</ul>
<p>For each epsiode do<br>&emsp;If the current $s_t$ is not the target state, do<br>&emsp;&emsp;Take action $a_t$ following $\pi_t(s_t)$, generate $r_{t+1},s_{t+1}$, and then take action $a_{t+1}$ following $\pi_t(s_{t+1})$.<br>&emsp;&emsp;Value update (parameter update):<br>&emsp;&emsp;$w_{t+1}=w_t+\alpha_t[r_{t+1}+\gamma \hat{q}(s_{t+1},a_{t+1},w_t)-\hat{q}(s_t,a_t,w_t)]\nabla_w\hat q(s_t,a_t,w_t)$.<br>&emsp;&emsp;Policy update:<br>&emsp;&emsp;$\pi_{t+1}(a|s_t)=1-\frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)|-1)$ if $a=argmax_{a\in\mathcal{A}(s_t)}\hat q(s_t,a,w_{t+1})$.<br>&emsp;&emsp;$\pi_{t+1}(a|s_t)=\frac{\epsilon}{|\mathcal{A}(s)|}$ otherwise.</p>
<h3 id="Q-learning-with-function-approximation"><a href="#Q-learning-with-function-approximation" class="headerlink" title="Q-learning with function approximation"></a>Q-learning with function approximation</h3><p>The q-value update rule is $w_{t+1}=w_t+\alpha_t[r_{t+1}+\gamma \underset{a\in\mathcal{A}(s_{t+1})}\max\hat{q}(s_{t+1},a,w_t)-\hat{q}(s_t,a_t,w_t)]\nabla_w\hat q(s_t,a_t,w_t)$.</p>
<p>Pseudocode:Sarsa with function approximation:</p>
<ul>
<li>Aim: Search a policy that can lead the agent to the target from an initial state-action pair $(s_0,a_0)$.</li>
</ul>
<p>For each epsiode do<br>&emsp;If the current $s_t$ is not the target state, do<br>&emsp;&emsp;Take action $a_t$ following $\pi_t(s_t)$, generate $r_{t+1},s_{t+1}$.<br>&emsp;&emsp;Value update (parameter update):<br>&emsp;&emsp;$w_{t+1}=w_t+\alpha_t[r_{t+1}+\gamma \underset{a\in\mathcal{A}(s_{t+1})}\max\hat{q}(s_{t+1},a,w_t)-\hat{q}(s_t,a_t,w_t)]\nabla_w\hat q(s_t,a_t,w_t)$.<br>&emsp;&emsp;Policy update:<br>&emsp;&emsp;$\pi_{t+1}(a|s_t)=1-\frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)|-1)$ if $a=argmax_{a\in\mathcal{A}(s_t)}\hat q(s_t,a,w_{t+1})$.<br>&emsp;&emsp;$\pi_{t+1}(a|s_t)=\frac{\epsilon}{|\mathcal{A}(s)|}$ otherwise.</p>
<h3 id="Deep-Q-learning-DQN"><a href="#Deep-Q-learning-DQN" class="headerlink" title="Deep Q-learning(DQN)"></a>Deep Q-learning(DQN)</h3><p>最成功的将深度神经网络引入强化学习的方法.<br>神经网络扮演的角色是非线性函数.</p>
<p>DQN aims to minimize the objective function/loss function:<br>$J(w)=E\Big[\Big(R+\gamma\underset{a\in\mathcal{A(S’)}}\sum \hat q(S’,a,w)-\hat q(S,A,w)\Big)^2\Big]$, where $(S,A,R,S’)$ are random variables.<br>Let $y:=R+\gamma\underset{a\in\mathcal{A(S’)}}\sum \hat q(S’,a,w)$. For the sake of simplicity, we can assume that $w$ in $y$ is fixed(at least for a while) when we calculate the gradient.</p>
<p>To do that, we can introduce 2 networks:</p>
<ul>
<li>a main network representing $\hat q(s,a,w)$.</li>
<li>a target network $\hat q(s,a,w_T)$.<br>The objective function in this case degenerated to $J=E\Big[\Big(R+\gamma\underset{a\in\mathcal{A(S’)}}\sum \hat q(S’,a,w_T)-\hat q(S,A,w)\Big)^2\Big]$ where $w_T$ is the target network parameter.<br>When $w_T$ is fixed, the gradient of $J$ can be easily obtained as<br>$\nabla_w J=E\Big[\Big(R+\gamma\underset{a\in\mathcal{A(S’)}}\sum \hat q(S’,a,w_T)-\hat q(S,A,w)\Big)\nabla_w\hat{q}(S,A,w)\Big]$</li>
</ul>
<p>Implementation details:</p>
<ul>
<li>Let $w$ and $w_T$ denote the parameters of the main and target networks, respectively. They are set to be the same initially.</li>
<li>In every iteration, we draw a mini-batch of samples $\{(s,a,r,s’)\}$ from the replay buffer.</li>
<li>The inputs of the networks include state $s$ and action $a$. The target output is $y_T=R+\gamma \max_{a\in\mathcal{A}}\hat q(s’,a,w_T)$. Then we directly minimize the TD error or called loss function $(y_T-\hat q(s,a,w))^2$ over the mini-match $\{(s,a,y_T)\}$.</li>
</ul>
<p>Experience replay:<br>将所有数据放在一个集合中$\mathcal{B}:=\{(s,a,r,s’)\}$<br>每次需要训练神经网络，我们从该集合中取数据.<br>为什么要用经验回放：<br>$J(w)=E\Big[\Big(R+\gamma\underset{a\in\mathcal{A(S’)}}\sum \hat q(S’,a,w)-\hat q(S,A,w)\Big)^2\Big]$</p>
<ul>
<li>$(S,A)\sim d$</li>
<li>$R\sim p(R|S,A),S’\sim p(S’|S,A)$</li>
<li>The distribution of $(S,A)$ is assumed to be uniform. But the samples are not uniformly collected. So we can break the correlation between consequent samples by experience replay method.</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>值函数的近似</p><p><a href="http://example.com/RL/DQL.html">http://example.com/RL/DQL.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Jiamin Liu</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-07-03</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-07-07</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><!--!--></article></div><!--!--><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "7b7feba5dd223adf80c1d347c8a9a6fc",
            repo: "od-liu.github.io",
            owner: "od-Liu",
            clientID: "Ov23liWi9TgDXgT7hhEB",
            clientSecret: "054ac0bc0bc8afaac9a807be4b6af2274ee9bc10",
            admin: ["od-liu"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/1.png" alt="Jiamin Liu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jiamin Liu</p><p class="is-size-6 is-block">别卷我别卷我别卷我👊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">0</p></a></div></div></nav></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="OD&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2025 Jiamin Liu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>