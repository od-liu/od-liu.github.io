{"posts":[{"title":"首页","text":"👋 欢迎来到我的专栏以下是我整理的数学和计算机部分课程学习笔记。如有理解不当的地方，欢迎指正与交流~ 麻烦暂时忽略左侧的文章数和分类数，主包还没学会怎么搞这个功能。。。 核心专栏 高等代数 - MATH1408 课堂笔记整理 常微分方程 - MATH2501 一个不重要的专栏：《咸鱼ode自救计划》 组合，概率和算法 - 算法学习中的概率方法与思想 强化学习 - 学习记录帖","link":"/"}],"tags":[],"categories":[],"pages":[{"title":"Application:A Randomized Algorithm for 3-SAT","text":"基础版算法//.bordered { border: 1px solid #000; padding: 10px; } 3-SAT 算法（基础版）: 1. 初始化：对变量随机赋值. 2. 将以下过程重复m次，如果所有子句都被满足则停止： (a) 选择任意一个未被满足的子句 (b) 随机选择子句中的一个变量并改变其值 3. 如果找到了一组满足表达式的赋值，返回该赋值 4. 否则返回该表达式无法被满足. 标记与符号同2-SAT算法 注意到这个基础班3-SAT算法与2-SAT算法高度类似，但是众所周知3-SAT是NP-hard，所以这个算法的时间复杂度是指数级而非多项式 算法复杂度分析分析方法与2-SAT问题类似，以下部分保留关键步骤： 假设输入表达式是可以被满足的，当$1\\leq j\\leq n-1$时，有 $$ \\begin{aligned} Pr(X_{i+1}=j+1|X_i=j)\\geq 1/3 \\\\ Pr(X_{i+1}=j-1|X_i=j)\\leq 2/3 \\end{aligned} $$ 将以上约束条件转化为马尔科夫链（消极化处理） $$\\begin{aligned} Pr(Y_{i+1}=1|Y_i=0) &=1\\\\ Pr(Y_{i+1}=j+1|Y_i=j) &=1/3\\\\ Pr(Y_{i+1}=j-1|Y_i=j) &=2/3 \\end{aligned} $$ 得到递推关系式： $$ h_j=2^{n+2}-2^{j+2}-3(n-j) $$ 所以这个算法的平均时间复杂度是$\\Theta(2^n)$ 寻求改进观察基础版算法不难有以下两个发现： 因为初始化时我们对变量随机进行赋值，所以$A_0$与$S$赋值匹配的变量个数服从二项分布. 初始化后$A_0$与$S$赋值匹配的变量个数显著多于$n/2$虽然是指数级小概率事件，但不应该被忽视 当算法开始运行，$A_i$与$S$的匹配数倾向于向0移动而非向n移动. 这说明在完成初始化赋值后，算法运行轮次越多，我们获得正确结果的概率越小. 一个更好的解决方式是进行多次初始化赋值，并减少每次初始化后循环次数 基于以上事实，可以提出改进版3-SAT算法 改进版算法 3-SAT 算法（改进版）: 1. 重复以下过程m次，如果所有子句都被满足则终止： (a) 初始化：对变量随机进行赋值 (b) 重复以下过程3n次，如果所有子句都被满足则终止： i. 随机选择一个未被满足的子句 ii.随机选择子句中的一个变量并改变其值 3. 如果找到了一组满足表达式的赋值，返回该赋值 4. 否则返回该表达式无法被满足. 算法复杂度分析首先定义两个符号： $q$: 初始化后在$3n$轮之内达到$S$(或其他满足表达式的赋值)的概率 $q_j$: 初始化$A_0$恰好有$j$个变量与$S$赋值不同时，在$3n$轮之内达到$S$的概率下限 在每一步中，有$1/3$的概率匹配数加1，有$2/3$的概率匹配数减1. 所以可以用 $$ {j+2k\\choose k}\\Big(\\frac{2}3{}\\Big)^k\\Big(\\frac{1}{3}\\Big)^{j+k} $$ 表示通过$k$轮减少，$j+k$轮增加达到$n$的概率.因此得到关于$q_j$的表达式： $$ \\begin{aligned} q_j \\geq \\max_{\\substack{k=0, \\dots, j \\\\ j+2k\\leq 3n}}{j+2k \\choose k}\\Big(\\frac{2}{3}\\Big)^k\\Big(\\frac{1}{3}\\Big)^{j+k} \\end{aligned} $$ 如果选取$k=j$，则可以得到$q_j$的一个下限： $$ q_j \\geq {3j \\choose j}\\Big(\\frac{2}{3}\\Big)^j\\Big(\\frac{1}{3}\\Big)^{2j} $$ 接下来的讨论需要用到Stirling’s Formula. $(Stirling’s\\ Formula)\\ For\\ m&gt;0,\\ m!=\\sqrt{2\\pi m}\\Big(\\frac{m}{e}\\Big)^m(1\\pm o(1))$ 特别地，对于$m&gt;0$，有$\\sqrt{2\\pi m}\\Big(\\frac{m}{e}\\Big)^m\\leq m!\\leq 2\\sqrt{2\\pi m}\\Big(\\frac{m}{e}\\Big)^m$ (Stirling’s Formula证明感兴趣的可以看一下) 因此，当$j&gt;0$， $$\\begin{aligned} {3j\\choose j} &=\\frac{(3j)!}{j!(2j)!}\\\\ &\\geq\\frac{\\sqrt{2\\pi(3j)}}{4\\sqrt{2\\pi j}\\sqrt{2\\pi(2j)}}\\Big(\\frac{3j}{e}\\Big)^{3j}\\Big(\\frac{e}{2j}\\Big)^{2j}\\Big(\\frac{e}{j}\\Big)^j\\\\ &=\\frac{\\sqrt{3}}{8\\sqrt{\\pi j}}\\Big(\\frac{27}{4}\\Big)^j \\\\&=\\frac{c}{\\sqrt{j}}\\Big(\\frac{27}{4}\\Big)^j,\\ 取\\ c=\\frac{\\sqrt{3}}{8\\sqrt{\\pi}} \\end{aligned} $$ 所以在$j&gt;0$时，可以得到 $$\\begin{aligned} q_j &\\geq{3j \\choose j}\\Big(\\frac{2}{3}\\Big)^j\\Big(\\frac{1}{3}\\Big)^{2j} \\\\&\\geq\\frac{c}{\\sqrt{j}}\\Big(\\frac{27}{4}\\Big)^{j}\\Big(\\frac{2}{3}\\Big)^{j}\\Big(\\frac{1}{3}\\Big)^{2j}\\\\ &\\geq \\frac{c}{\\sqrt{j}}\\frac{1}{2^j} \\end{aligned} $$ 同时，$q_0=0$ 现在我们可以得到$q$的下限 $$\\begin{aligned} q &\\geq\\sum_{j=0}^nPr(a\\ random\\ assignment\\ has\\ j\\ mismatches\\ with\\ S)\\cdot q_j\\\\ &\\geq \\frac{1}{2^n}+\\sum_{j=1}^n{n\\choose j}\\Big(\\frac{1}{2}\\Big)^n\\frac{c}{\\sqrt{j}}\\frac{1}{2^j}\\\\ &\\geq \\frac{c}{\\sqrt{n}}\\Big(\\frac{1}{2}\\Big)^n\\sum_{j=0}^{n}{n\\choose j}\\Big(\\frac{1}{2}\\Big)^j(1)^{n-j}\\\\ &=\\frac{c}{\\sqrt{n}}\\Big(\\frac{1}{2}\\Big)^n\\Big(\\frac{3}{2}\\Big)^n \\\\&=\\frac{c}{\\sqrt{n}}\\Big(\\frac{3}{4}\\Big)^n \\end{aligned} $$ 假设3-SAT表达式存在一组满足条件的变量赋值，在找到这组变量赋值前，算法尝试的随机变量赋值次数满足几何分布. 所以赋值次数的数学期望为$1/q$. 而在每组随机赋值中，算法最多循环$3n$次，所以得到算法的平均时间复杂度$O(n^{3/2}(4/3)^n)$. Remark：注意这里是bounded by，而不是exactly！并且这里并没有用$m$来控制外循环次数，只是单纯在讨论算法平均时间复杂度的上限","link":"/AAC/7-1-2-3SAT.html"},{"title":"Application:A Randomized Algorithm for 2-SAT","text":"2-SAT 问题定义与算法 2-SAT: 每个子句有且仅有两个文字 算法: 初始化：随机对所有变量赋值 重复以下操作$2mn^2$次, 如果所有子句都被满足则终止: (a) 任意选择一个未满足的子句 (b) 随机选择子句中的一个变量并改变其值 如果找到了一组满足表达式的赋值，返回该赋值 否则返回该表达式无法被满足. $n$: 变量总数 $m$: 与算法正确率有关的一个参数 复杂度分析注：以下讨论均假设输入表达式是可以被满足的 符号与标记 $S$:一个满足表达式的赋值 $A_i$: 第$i$轮之后变量的取值 $X_i$: 第$i$轮后$A_i$与$S$赋值相同的变量个数，$X_i=n$表明算法得到一组满足表达式的变量赋值并终止 算法复杂度分析当$X_i=0$，无论改变哪个变量的值，都可以得出：$Pr(X_{i+1}=1|X_i=0)=1$ 当$1\\leq X_i\\leq n-1$，在每一步中，我们选择一个未满足的语句（该语句中至少有一个变量的赋值与$S$不同），因为这个语句最多只有两个变量，所以至少有$1/2$的概率将匹配数提高$1$；如果$A_i$与$S$关于该语句两个变量的赋值都不一样，则有$1$的概率将匹配数提升$1$. 由此可知匹配数减少$1$的概率小于等于$1/2$. 于是得到以下关系式： $$ \\begin{aligned} Pr(X_{i+1}=j+1|X_i=j)\\geq 1/2\\\\ Pr(X_{i+1}=j-1|X_i=j)\\leq 1/2 \\end{aligned} $$ 注意到随机过程$X_0,X_1,X_2…$并不构成马尔科夫链：过去的行动会影响当前转移概率矩阵但是可以考虑以下马尔科夫链： $$ \\begin{aligned} Y_O &=X_0\\\\ Pr(Y_{i+1}=1|Y_i=0) &=1\\\\ Pr(Y_{i+1}=j+1|Y_i=j) &= 1/2\\\\ Pr(Y_{i+1}=j-1|Y_i=j) &= 1/2 \\end{aligned} $$ 这个马尔可夫链$Y_0,Y_1,Y_2,…$实际上是$X_0,X_1,X_2,…$的消极情况. 显然，无论初始状态是什么，随机过程$Y$相比随机过程$X$都将要花费更多时间达到$Y_i=n$ 为马尔可夫链$Y$构建图$G=(V,E)$,$0\\sim n$为$G$中顶点，顶点$i$与顶点$i-1，i+1$分别有边连接，用$h_j$表示从顶点$j$开始到达$n$的期望轮数.显然有$h_0=h_1+1,h_n=0$.对于$1\\leq j\\leq n-1$，有 $$ h_j=\\frac{h_{j-1}+1}{2}+\\frac{h_{j+1}+1}{2}=\\frac{h_{j-1}}{2}+\\frac{h_{j+1}}{2}+1 $$ 于是得到以下等式： $$ \\begin{aligned} h_0 &=h_1+1\\\\ h_j &=\\frac{h_{j-1}+1}{2}+\\frac{h_{j+1}+1}{2}=\\frac{h_{j-1}}{2}+\\frac{h_{j+1}}{2}+1\\\\ h_n &= 0 \\end{aligned} $$ 推导得到： $$ h_j=h_{j+1}+2j+1 $$ 因此得出结论： $$ \\begin{aligned} h_0&=h_1+1=h_2+1+3=...=n^2\\\\ h_j&=n^2-j^2 \\end{aligned} $$ Lemma 1 假设2-SAT表达式可以被满足，那么该算法找到满足条件的赋值所需要的期望轮数小于等于$n^2$Lemma 2 如果2-SAT表达式无法被满足，则算法总是返回正确结果. 如果表达式可以被满足，那么有至少$1-2^{-m}$的概率该算法返回一组可以满足表达式的变量赋值；在其他情况下，该算法返回错误结果，认为该表达式是无法被满足的 证明： 当该表达式可以被满足时，将算法运行的每$2n^2$轮分成一组。假设进行完前$i-1$没有得到满足该2-SAT表达式的解，用$Z$表示算法从第$i$组开始直到最终找到一个满足表达式的解所用的轮数，运用马尔可夫不等式可得 $$\\begin{aligned} Pr(Z\\gt 2n^2)&\\leq\\frac{n^2}{2n^2}=\\frac{1}{2}\\\\ (Markov's inequality)\\ \\ Pr(X\\geq a)&\\leq \\frac{E[X]}{a},\\forall a\\gt 0\\end{aligned} $$ 因此$m$组循环内无法找到可行解的概率小于等于$(1/2)^m$","link":"/AAC/7-1-1-2SAT.html"},{"title":"Definitions and Representations","text":"stochastic process:$X=\\{X_t:t\\in T\\}$ $X_t$: the state of the process at time $t$. discrete space process: $X_t$ assumes values from a countably infinite set.finite process: $X_t$ assumes values from a finite set. discrete time process: $t$ is a countably infinite set. Markov chain: $X_0,X_1,X_2,…$ is a Markov chain if $$ \\begin{aligned} Pr(X_t=a_t|X_{t-1}=a_{t-1},X_{t-2}=a_{t-2},...,X_0=a_0)&=Pr(X_t=a_t|X_{t-1}=a_{t-1})\\\\ &= P_{a_{t-1},a_t} \\end{aligned} $$ Assume $P_{i,j}=Pr(X_t=j|X_{t-1}=i)$.transition matrix: $$ P=\\left[ \\begin{matrix} P_{0,0} & P_{0,1} & \\cdots & P_{0,j} & \\cdots \\\\ P_{1,0} & P_{1,1} & \\cdots & P_{1,j} & \\cdots\\\\ \\vdots & \\vdots & \\ddots & \\vdots &\\ddots\\\\ P_{i,0} & P_{i,1} & \\cdots & P_{i,j} &\\cdots \\\\ \\vdots & \\vdots & \\ddots & \\vdots &\\ddots \\end{matrix} \\right] $$","link":"/AAC/Chapter7.html"},{"title":"马尔科夫链的状态分类 &amp; 平稳分布","text":"状态分类定义7.2 称状态$j$从状态$i$是可达的，如果对于某些整数$n\\geq 0,P_{i,j}^n\\gt 0$. 如果两个状态$i,j$互相可达，则称他们是连通的（communicate），记作$i\\leftrightarrow j$.易知状态的连通性满足自反性，对称性和传递性，因此为等价关系. 定义7.3 称一个马尔科夫链是不可约的（irreducible），如果所有状态全部连通. 定理7.4 称一个状态是常返的（recurrent），如果$\\sum_{t\\geq 1}r_{i,i}^t=1$. 否则称为瞬时的（transient）.一个常返马尔科夫链要求其每一个状态都是常返的 定义7.5 一个常返状态$i$称为正常返如果$h_{i,i}\\lt\\infty$.否则称为零常返. 直观理解：正常返要求系统返回到状态的平均时间是有限的 定理7.5 对于一个有限状态马尔科夫链： 至少有一个状态是常返的； 所有常返状态均为正常返. 定义7.6 一个离散时间马尔科夫链中，状态$j$是周期（periodic）的如果存在整数$\\Delta&gt;1$，使得$Pr(X_{t+s}=j|X_t=j)=0,\\forall s,\\Delta\\nmid s$. 离散时间周期马尔科夫链要求每个状态都是周期的. 反之，称状态或链为非周期的（aperiodic）. 定义7.7一个非周期，正常返状态被称为各态历经状态（ergodic state）. 一个马尔可夫链是各态历经的要求所有状态都是各态历经的. 推论7.6 任何有限，不可约且非周期马尔科夫链都是各态历经的. 平稳分布定义7.8 平稳分布（stationary distribution）$\\bar{\\pi}$满足$\\bar{\\pi}=\\bar{\\pi}P$. 定理7.7 任何有限，不可约且各态历经马尔科夫链有以下性质： 该链有且仅有唯一平稳分布$\\bar{\\pi}=(\\pi_0,\\pi_1,\\dots,\\pi_n)$; 对于任意$i,j$，极限$\\lim_{t\\rightarrow\\infty}P_{i,j}^t$存在并且取值与$j$无关; $\\pi_i=\\lim_{t\\rightarrow\\infty}P_{i,i}^t=1/h_{i,i}$. 定理7.8 对于任意不可约，各态历经马尔科夫链的任意状态$i$，极限$\\lim_{t\\rightarrow\\infty}P_{i,i}^t$存在且满足$\\underset{t\\rightarrow\\infty}{lim}P_{i,i}^t=\\frac{1}{h_{i,i}}$.（证明先空着，有空补） 定理7.9 设$S$为有限，不可约，非周期马尔科夫链的某些状态的集合. 则在平稳分配中，链离开$S$的概率始终等于链进入$S$的概率. 定理7.10 任何一个不可约马尔科夫链均属于以下两类之一： 各态历经马尔科夫链:$\\forall i,j,\\underset{t\\rightarrow\\infty}{lim}P_{j,i}$存在且独立于$j$，且链有唯一平稳分布$\\pi_i=\\underset{t\\rightarrow\\infty}{lim}P_{j,i}^t\\gt 0$; 没有一个状态是正常返的马尔科夫链：$\\forall i,j,\\underset{t\\rightarrow\\infty}{lim}P_{j,i}^t=0$并且该链没有平稳分布.","link":"/AAC/7-23.html"},{"title":"无向图随机游走理论","text":"定义7.9 图上的随机游走是指一个粒子在图上的一列移动. 某个时间点，粒子在图中所处的位置即为此时系统所处状态. 如果粒子处于顶点$i$并且$i$的出度为$d(i)$，则该粒子从边$(i,j)$移动到邻居$j$的概率是$1/d(i)$ 定理7.12 无向图$G$上的随机游走是非周期的当且仅当$G$不是二分图. 定理7.13 无向图上的随机游走收敛于平稳分布$\\bar{\\pi}$，且满足$\\pi_v=\\frac{d(v)}{2|E|}$.证明：直接带入$\\bar{\\pi}=\\bar{\\pi}P$验证即可. 定义 图$G=(V,E)$中两顶点$u,v$间的往返时间定义为$h_{u,v}+h_{v,u}$.代表从顶点$u$出发到达$v$并再返回$u$的总时间期望，具有对称性. 定义7.10 图$G=(V,E)$的覆盖时间（cover time）是指从图中任一顶点出发，通过随机游走访问图中所有顶点的最大时间期望. 一般将图$G$的覆盖时间记为$C_G$，也称其为图$G$的命中时间. 尝试寻找图$G$覆盖时间上限： 定理7.14 如果$(u,v)\\in E$，则$h_{u,v}+h_{v,u}\\leq 2|E|$. 定理7.15 图$G=(V,E)$的覆盖时间满足$C_G\\leq 2|E|(|V|-1)$. 定义 $H(n)=\\sum_{i=1}^n 1/i\\approx \\ln{n}$ 定理7.16 对于有$n$个顶点的图$G=(V,E)$，有$C_G\\leq H(n-1)\\underset{u,v\\in V:u\\neq v}\\max h_{u,v}$.同样也可以得到$C_G\\geq H(n-1)\\underset{u,v\\in V:u\\neq v}\\min h_{u,v}$ . 但是这个命中时间下限一般很小，缺少使用价值.","link":"/AAC/7-4-randomwalks.html"},{"title":"AAC","text":"","link":"/AAC/index.html"},{"title":"Stirling&#39;s Formula及证明","text":"证明： $$ ln(n!)=\\sum_{j=1}^n\\ln{j} $$ 用积分近似： $$ \\sum_{j=1}^n\\ln j\\approx\\int_{1}^n\\ln{x}\\ dx=n\\ln{n}-n+1 $$ 如果用误差更小一点的积分梯形近似： $$ \\ln(n!)-\\frac{1}{2}\\ln{n}\\approx\\int_1^n \\ln{x}dx=n\\ln{n}-n+1 $$ 近似误差可由Euler-Maclaurin公式得到： $$ \\begin{aligned} \\ln{(n!)}-\\frac{1}{2}\\ln{n} &=\\frac{1}{2}\\ln1+\\ln2+\\dots+\\frac{1}{2}\\ln n\\\\ &=n\\ln n-n+1+\\sum_{k=2}^m\\frac{(-1)^kB_k}{k(k-1)}\\Big(\\frac{1}{n^{k-1}}-1\\Big)+R_{m,n} \\end{aligned} $$ 其中$B_k$是伯努利数，$R_{m,n}$是Euler-Maclaurin公式中的余数项 对表达式两端取极限并定义$y$为极限值，可以得到， $$ \\begin{aligned} y =\\lim_{n\\rightarrow\\infty}(\\ln(n!)-n\\ln n+n-\\frac{1}{2}\\ln n) =1-\\sum_{k=2}^m\\frac{(-1)^kB_k}{k(k-1)}+\\lim_{n\\rightarrow\\infty}R_{m,n} \\end{aligned} $$ 因为$R_{m,n}$满足 $$ \\begin{aligned} R_{m,n}=\\lim_{n\\rightarrow\\infty}R_{m,n}+O\\Big(\\frac{1}{n^m}\\Big) \\end{aligned} $$ 基于以上准备可以完成推导： $$ \\begin{aligned} \\ln(n!)-n\\ln n+n-\\frac{1}{2}\\ln n=\\Big(1-\\sum_{k=2}^m\\frac{(-1)^kB_k}{k(k-1)}\\Big)+\\Big(\\sum_{k=2}^m\\frac{(-1)^kB_k}{k(k-1)n^{k-1}})+R_{m,n} \\end{aligned} $$ $$ \\begin{aligned} 右边&=(y-\\lim_{n\\rightarrow R_{m,n}})+\\Big(\\sum_{k=2}^m\\frac{(-1)^kB_k}{k(k-1)n^{k-1}}+R_{m,n}\\Big)\\\\ &=y+\\Big(\\sum_{k=2}^m\\frac{(-1)^kB_k}{k(k-1)n^{k-1}})+(R_{m,n}-\\lim_{n\\rightarrow\\infty }R_{m,n})\\\\ &=y+\\sum_{k=2}^m\\frac{(-1)^kB_k}{k(k-1)n^{k-1}}+O\\Big(\\frac{1}{n^m}\\Big) \\end{aligned} $$ 于是得到近似公式 $$ \\begin{aligned} \\ln (n!)=n\\ln{\\Big(\\frac{n}{e}\\Big)}+\\frac{1}{2}\\ln{n}+y+\\sum_{k=2}^m\\frac{(-1)^kB_k}{k(k-1)n^{k-1}} \\end{aligned} $$ 取$m=1$得到 $$ \\begin{aligned} n!=e^y\\sqrt n\\Big(\\frac{n}{e}\\Big)^n\\Big(1+O\\Big(\\frac{1}{n}\\Big)\\Big) \\end{aligned} $$ 由于$e^y=\\sqrt{2\\pi}$，得到Stirling’ formula $$ \\begin{aligned} n!=\\sqrt{2\\pi n}\\Big(\\frac{n}{e}\\Big)\\Big(1+O\\Big(\\frac{1}{n}\\Big)\\Big) \\end{aligned} $$","link":"/AAC/Stirling.html"},{"title":"第一节：当你的一阶矩方法失效时，该怎么办？","text":"1. 核心问题与经典方法的局限性核心思想: 在许多“自然”的超图中，其（大的）独立集并非随机分布，而是呈现出“扎堆”或“聚集”的结构化特征。(注意本节中一切讨论未涉及超图，还是基于图（随机图）在进行讨论) 定义1 The extremal number of a graph $H$: 在一个有n个顶点的图中，不包含$H$作为子图的前提下，所能拥有的最大边数。记作$ex(n,H)$。 简单来说，$ex(n,H)$回答了：为了确保子图中一定出现一个$H$，至少需要多少条边？答案是$ex(n,H)+1$。 本节我们将要证明以下理论：定理2 [Frankl and Rodl]: 如果$p \\gg 1/\\sqrt{n}$，那么$ex(G(n,p),K_3)=\\Big(\\frac{1}{4}+o(1)\\Big)pn^2$。当$n$趋向无穷大时，上述等式以极高的概率成立。 要求图$G$不能太稀疏。 在确定的$n$顶点图中，不含三角形的图最多有约$n^2/4$条边（图兰定理）。 当我们想要证明以上定理时，第一个想法是利用概率论中的一阶矩方法： 思路：定义一个随机变量$X$，表示$G(n,p)$ 中大小为$m$的无三角形子图的数量。然后计算它的期望值$E[X]$。如果$E[X]=o(1)$，那么根据马尔可夫不等式，大小为$m$的无三角形子图存在的概率就趋近于$0$，即有很高的概率$ex(G(n,p),K_3)\\leq m$。 问题：讲义中指出，当你去计算这个期望值时，会发现它爆炸了，即$E[X]$非常大，远不趋近于 $0$。所以该方法失效。 经典一阶矩方法失效原因大致可以理解为无三角形图本身是“扎堆”或“聚集”的。许多无三角形图都非常相似（比如，都接近于同一个二分图），这导致它们在$G(n,p)$中出现的事件之间存在强烈的正相关性。这种正相关性使得简单的期望计算失效。 2. 容器方法的核心思想与应用既然经典方式走不通，就要采用一种新的策略来处理这种聚类现象，这个策略就是容器方法。核心思想变化是这次我们不再一个个地数无三角形图，而是先把它们分装到一些容器里，然后再去处理这些容器。这些容器由定理3来保证存在。 定理3 无三角形图容器定理 (The container theorem for triangle-free graphs): 对于$n$个顶点的图，存在一个图的集合$\\mathcal{G}$（我们称之为“容器集合”），它满足以下三个性质：(a) 容器数量很少: $|\\mathcal{G}|$的规模相对很小，满足 $|\\mathcal{G}|\\leq n^{O(n^{3/2})}$。(b) 容器自身很“干净”: $\\mathcal{G}$中的每个图$G$只包含$o(n^3)$个三角形。(c) 所有目标都被装下了: 每个不含三角形的图，都必然是$\\mathcal{G}$中某个容器图$G$的子图。 直观理解：平面上有无数个苹果（代表所有无三角形图）。我们想证明这些苹果的总重量不大。直接一个个称重很麻烦。容器方法告诉我们，我们可以找到少数几个大篮子（性质a），这些篮子本身都很轻（性质b），并且所有苹果都在这些篮子里面（性质c）。 接下来使用容器的思想来证明定理2：首先需要引入一个关键工具：定理4 三角形超饱和定理 (Supersaturation for triangles): 对于任何 $\\epsilon&gt;0$，存在 $\\delta &gt; 0$。如果一个$n$顶点图的边数$e(G) \\ge (1/4 + \\epsilon)n^2$，那么$G$包含的至少$\\delta n^3$个三角形。 这个定理的逆否命题很有用：如果一个有$n$个顶点的图的三角形数为$o(n^3)$，则该图至多只有$(1/4+o(1))n^2$条边。 现在可以使用容器方法证明定理2了。（注：此处原文为定理3，根据上下文应为证明定理2）回顾定理2的内容，我们需要证明在$G(n,p)$随机图中，不可能存在一个边数超过$(1/4+\\epsilon)pn^2$的无三角形子图$H$。考虑使用反证法，先假设存在一个这样的坏图$H$，然后推导出一个概率极小、几乎不可能发生的结论，从而证明假设错误。 假设：存在坏图$H$同时满足以下两个条件： $H$是无三角形的； $e(H) &gt; (1/4+\\epsilon)pn^2$。 装入容器：由容器定理（定理3），必然存在一个容器图$G$，满足$H \\subset G$。 分析容器 $G$ 的属性： 由容器定理性质(b)：容器$G$所含三角形数很少，只有$o(n^3)$个。 由超饱和定理（定理4）的逆否命题知：$e(G) \\le (1/4+o(1))n^2$条边。 概率论证：现在我们有一个确定的容器图$G$，和一个随机的图$G(n,p)$，假设的坏图$H$是$G$和$G(n,p)$的公共子图。将$e(G)$看作$e(G)$次独立试验：对于容器$G$的每个边位置，这条边是否出现在$G(n,p)$中是一个独立的概率为$p$的事件，所以“$G$从$G(n,p)$中捕获的边数”这个随机变量，服从二项分布$B(e(G),p)$。已知$e(G) \\approx n^2/4$，所以这个二项分布的数学期望大约为$p(n^2/4)$。但我们假设捕获的边数$m &gt; (1/4+\\epsilon)pn^2$，远大于期望值。Chernoff’s Inequality： 设 $X_1, X_2, \\dots, X_N$是一系列独立的伯努利随机变量，令 $X = \\sum X_i$，其期望值为$ \\mu = E[X]$。对于任何$ \\delta &gt; 0$，有：$P(X \\ge (1+\\delta)\\mu) \\le e^{-\\delta^2\\mu/3}, \\quad \\text{for } 0 &lt; \\delta \\le 1$ 使用Chernoff不等式得到发生这种大偏差事件的概率是$e^{-\\Omega(pn^2)}$。所以对于任何一个特定的容器$G$，它能“捕获”到我们假设的那个超级大的$H$的概率，小到可以忽略不计。 联合界：上面的讨论证明了单个容器捕获到$H$的概率是很小的，但当容器不止一个的时候，是否所有容器捕捉到$H$的概率之和仍然很小？ 定义坏容器（即捕获了至少$m$条边的容器）的数量为$Y_m$。我们想计算$P(Y_m \\ge 1)$，即“至少存在一个坏容器”的概率。 根据联合界，这个概率小于等于所有容器成为坏容器的概率之和，也就是：$P(Y_m \\ge 1) \\le E[Y_m] = \\sum_{G\\in \\mathcal{G}} P(e(G\\cap G(n,p))\\ge m) \\le |\\mathcal{G}| \\times e^{-\\Omega(pn^2)} = n^{O(n^{3/2})}e^{-\\Omega(pn^2)}$。比较这两项的大小只需要取对数，而后得到当$p \\gg \\log n/\\sqrt{n}$时，该概率趋向于0，可以证明得到定理2的结论（但这里对于$p$取值的要求稍微强于原定理）。 结论：以极高的概率，$G(n,p)$中不可能存在边数超过$(1/4+o(1))pn^2$的无三角形子图。 3. 超图容器引理及其证明为了证明定理3，需要将问题上升到更普适的3-一致超图（3-uniform hypergraphs）中。 定义5 编码三角形的超图 (the hypergraph encoding triangles in $K_n$): 构造一个$3$-一致超图$\\mathcal{H}$，满足： 顶点集 $V(\\mathcal{H})=E(K_n)$ 是完全图$K_n$的所有边。 超边集 $E(\\mathcal{H})$ 是$K_n$中所有构成三角形的三条边的集合。$E(\\mathcal{H})=\\{\\{f_1,f_2,f_3\\}\\subset E(K_n):\\{f_1,f_2,f_3\\}=E(K_3)\\}$。 在这个构造下，$\\mathcal{H}$的一个独立集（不包含任何超边的顶点集）就等价于$K_n$中一个不含三角形的边集，也就是一个无三角形图。 对于一个超图$\\mathcal{H}$，定义$\\Delta_\\ell(\\mathcal{H})=\\max\\{d_{\\mathcal{H}}(A):A\\subset V(\\mathcal{H}),|A|=\\ell\\}$。其中$d_{\\mathcal{H}}(A)=|\\{B\\in E(\\mathcal{H}):A\\subset B\\}|$，用$\\mathcal{I}(\\mathcal{H})$表示$\\mathcal{H}$中所有独立集的集合。 $3$-一致超图的容器引理: 对于一个$3$-一致超图$\\mathcal{H}$，如果其平均度$d$足够大使得$\\tau:=1/\\sqrt{d}\\leq\\delta$，且$\\Delta_1(\\mathcal{H})\\leq c\\cdot d,\\ \\Delta_2(\\mathcal{H})\\leq c\\cdot\\sqrt{d}$，那么存在一个由$V(\\mathcal{H})$的子集构成的容器集合$\\mathcal{C}$，具有以下性质：(a) 覆盖性: 对于任意独立集$I\\in \\mathcal{I}(\\mathcal{H})$，均存在某个容器$C\\in \\mathcal{C}$使得$I\\subset C$。(b) 容器更小: $|C| \\le (1-\\delta)v(\\mathcal{H}),\\forall C\\in \\mathcal{C}$（即比原顶点集小一个常数比例）。(c) 容器个数上界：$|\\mathcal{C}|\\le {v(\\mathcal{H})\\choose \\tau\\cdot v(\\mathcal{H})}$。 4. 定理3的证明现在回到由$K_n$编码三角形的超图$\\mathcal{H}$。注意图$\\mathcal{H}$的性质：$v(\\mathcal{H})={n\\choose 2},\\ d_{\\mathcal{H}}(v)=n-2,\\forall\\ v\\in V(\\mathcal{H})$。而后考虑$\\Delta_2(\\mathcal{H})$的计算，任取$\\mathcal{H}$中的两个顶点构成的点集$\\mathcal{A}$，$\\mathcal{H}$中的两点对应的$K_n$中的两条边有以下两种可能情况： 两边在$K_n$没有公共点，则两边无法与第三条边构成三角形，因而在$\\mathcal{H}$中$d_{\\mathcal{H}}(\\mathcal{A})=0$。 两边在$K_n$中有公共点，则这两条边会与唯一一条边一起构成一个$K_n$中的三角形，从而在$\\mathcal{H}$中$d_{\\mathcal{H}}(\\mathcal{A})=1$。于是得到：$\\Delta_2(\\mathcal{H})=1$。 令$c=1$，对$\\mathcal{H}$应用3-一致超图容器引理，得到$E(K_n)$的容器集合$\\mathcal{C}_0$。因为$\\tau=\\Theta(1/\\sqrt{n})$，由引理(c)及不等式${n\\choose k}\\leq\\Big(\\frac{en}{k}\\Big)^k$得到$|\\mathcal{C}_0|\\leq n^{O(n^{3/2})}$。于是定理3中数量少的要求已经被满足。接下来利用迭代思想构造一个“结构好”的容器集合$\\mathcal{G}$。 (1) 初始化: 将上面得到的$\\mathcal{C}$记为$\\mathcal{C}_0$。$\\mathcal{C}_0$中可能有很多坏容器（包含的三角形数太多了）。 (2) 迭代提纯: 在第$i$步，对容器集合$\\mathcal{C}_{i-1}$中的每一个容器$C$进行检查： 好容器: 若$t(C) &lt; \\epsilon n^3$，则$C$达标，放入最终集合$\\mathcal{G}$。 坏容器: 若$t(C) \\ge \\epsilon n^3$，则$C$需要被分解。对$C$构造诱导子超图$\\mathcal{H}[C]$。检查$\\mathcal{H}[C]$的平均度数：$v(\\mathcal{H}[C]) = |C| = O(n^2), e(\\mathcal{H}[C]) = t(C) \\ge \\epsilon n^3$，所以$d(\\mathcal{H}[C]) = 3 e(\\mathcal{H}[C]) / v(\\mathcal{H}[C]) \\ge 3\\epsilon n$。这个平均度足够大，我们可以令$c=1/\\epsilon$，在$\\mathcal{H}[C]$上再次应用超图容器引理，得到一个新的、更小的容器集合$\\mathcal{C}’(C)$。 (3) 更新: 用这个新的、更小的容器集合$\\mathcal{C}’(C)$来替换原来的那个坏容器$C$。 (4) 迭代终止: 每次分解，新容器的大小（边数）都会以$(1-\\delta)$的比例缩小。经过有限步（大约$O(\\log n)$步）后，所有容器都会因为不断变小而最终变成好容器。 (5) 最终结论:当迭代终止时，我们得到的最终集合$\\mathcal{G}$满足定理3的所有条件： 结构好: $\\mathcal{G}$中的每个容器的三角形数量都小于$\\epsilon n^3$（即$o(n^3)$）。 覆盖性: 任何一个无三角形图$I$在每次迭代中都会被新的子容器覆盖，所以最终一定被某个好容器覆盖。 数量少: 虽然每一步都会增加容器数量，但迭代步数有限。总容器数量仍然在$n^{O(n^{3/2})}$的量级，是极小的。 这样，通过迭代提纯的过程，我们就成功地构造出了满足定理3要求的容器集合。 5. 图的容器引理及其证明为了证明超图容器引理，我们先考虑一个简化版——图的容器引理。 图的容器引理 (The Graph Container Lemma): 对任意$c&gt;0$，存在$\\delta&gt;0$使得以下条件成立：对于一个平均度为$d$且$\\Delta(G)\\leq c\\cdot d$的图$G$，取$\\tau:=2\\delta/d$。可以找到一个$V(G)$的子集集合$\\mathcal{C}$，满足： 覆盖性：对于任意独立集$I\\in \\mathcal{I}(G)$，存在容器$C\\in \\mathcal{C}$使得$I\\subset C$。 容器更小：$|C|\\leq(1-\\delta)v(G),\\forall C\\in\\mathcal{C}$。 容器数量有限：$|\\mathcal{C}|\\leq{v(G)\\choose \\lceil\\tau\\cdot v(G)\\rceil}$。 定义6 (最大度顺序): 图$G$顶点的最大度排序是指将$V(G)$中的点排列为$(v_1,v_2,\\dots,v_n)$，使得$\\forall i\\in [n]$，$v_i$是$G[\\{v_i,\\dots,v_n\\}]$中度数最高的点。 图的容器算法:给定图$G$以及一个独立集$I\\in\\mathcal{I}(G)$，维护一个对$V(G)$的划分$V(G)=A\\cup S\\cup X$。 $A$: 活跃点集 $S$：当前的指纹集 $X$：被排除的点集 初始化$A=V(G),\\ S=X=\\emptyset$。当$|X|\\leq\\delta v(G)$时，重复： 按最大度顺序，找到第一个属于$I$的顶点$v$。 将$v$移入$S$。 将$v$的邻居$N(v)$和所有排在$v$之前的顶点移入$X$。 从$A$中移除新加入$S$和$X$的顶点。 引理证明思路: 覆盖性: 对于给定的独立集$I$，算法生成的排除集$X$中的点都确定不在$I$中，因此$I$必然被包含在$C(S) = V(G) \\setminus X$中。 容器更小: 算法循环结束条件是$|X| &gt; \\delta v(G)$，这直接保证了$|C(S)| \\le (1-\\delta)v(G)$。 数量可控: 关键在于证明指纹集$S$足够小，即$|S(I)| \\le \\lceil\\tau \\cdot v(G)\\rceil$。这通过证明“每向$S$中加入一个点，至少有$d/2$个点被加入到$X$”来实现。 这一步利用了$\\Delta(G) \\le c \\cdot d$的条件和反证法。假设某一步加入$S$的顶点$u$不能为$X$贡献足够多的新成员，那么可以推导出在当前的活跃子图$G[A]$中，平均度数仍然很高。因此，作为度数最高的顶点，$u$的度数必然很大，这与“不能为$X$贡献足够多成员”的假设矛盾。 6. 3-一致超图容器引理证明该证明将图容器算法的思想推广到超图。算法维护一个指纹$S$、一个可用的超图$A$和一个禁用对图$G$。 核心循环: 在可用超图$A$中按最大度顺序找到第一个属于独立集$I$的顶点$u$。 将$u$加入指纹$S$。 对于每条包含$u$的超边$\\{u, v, w\\}$，在$G$中添加一条边$\\{v,w\\}$，表示$v,w$不能共存。 将$u$和所有排在$u$前面的顶点从$A$中移除。 将$G$中度数过高的顶点从$A$中移除（为Plan A做准备）。 将包含$G$中边的超边从$A$中移除（强化约束）。 证明思路 (Plan A / Plan B策略):对于任意独立集$I$，算法运行后，必然落入以下两种情况之一： Plan A: 禁用对图$G$非常稠密$e(G(S_2)) \\ge \\frac{\\sqrt{d}\\cdot v(H)}{8c}$ 且 $\\Delta(G(S_2)) \\le 2c\\sqrt{d}$。此时，我们得到了一个稠密且最大度有界的普通图$G(S_2)$。由于$I$在其中也是独立集，我们可以对$G(S_2)$应用图的容器算法，找到一个小的图指纹$S_1$，从而生成一个满足条件的小容器$C$。 Plan B: 禁用对图$G$不够稠密$e(G(S_2)) &lt; \\frac{\\sqrt{d}\\cdot v(H)}{8c}$。此时，算法在生成“禁用对”方面不成功。通过反证法可以证明，这种情况的发生，必然是因为在算法的某个步骤中，有大量的顶点（至少$v(H)/16c$个）因为度数排序（第4步）或度数过高（第5步）而被移出$A$。这些被移除的顶点都不在$I$中（除了$S_2$中的少量成员）。因此，我们直接定义容器为 $C = V(\\mathcal{H}) \\setminus (\\text{这些被移除的顶点})$，这个容器也满足大小要求。 两种情况都导向了小容器的生成，从而完成了证明。 7. 加强版3-一致超图容器引理与应用加强版3-一致超图容器引理: 对于任意$c&gt;0$，存在$\\delta&gt;0$。如果3-一致超图$\\mathcal{H}$的平均度数为$d$，令$\\tau:=1/\\sqrt{d}$并假设$\\tau\\leq\\delta$，且$\\Delta_1(\\mathcal{H})\\leq c\\cdot d,\\ \\Delta_2(\\mathcal{H})\\leq c\\cdot \\sqrt{d}$，则存在一个$V(\\mathcal{H})$的容器集合$\\mathcal{C}$以及一个函数$f:\\mathcal{P}(V(\\mathcal{H}))\\rightarrow \\mathcal{C}$使得：(a) 对于任意$I\\in\\mathcal{I}(\\mathcal{H})$，存在$S \\subset I$满足$|S|\\leq\\tau\\cdot v(\\mathcal{H})$，且$I\\subset f(S)$。(b) $|C|\\leq(1-\\delta)v(\\mathcal{H}),\\forall C\\in \\mathcal{C}$。 与原版的区别: 主要在于结论(a)，它更明确地指出了“指纹”$S$是独立集$I$的子集，并且容器$f(S)$完全由这个指纹决定。 应用: 加强版引理的一个很有效的应用就是将定理2中$p$的约束条件从$p \\gg \\log n/\\sqrt{n}$优化到了$p \\gg 1/\\sqrt{n}$。这是因为在最终的概率计算中，我们不再需要对所有可能的容器求和，而是对所有可能的小指纹$S$求和。由于指纹$S$是$G(n,p)$的子图，它的出现本身就带有一个$p^{|S|}$的概率。这使得最终的联合界计算更加精细，从而放宽了对$p$的要求。","link":"/Com-Pro/hc1.html"},{"title":"第二节：更一般的容器定理","text":"引言：通用的超图容器引理在深入探讨具体应用之前，本节的核心工具——通用的超图容器引理。这个引理为各种组合对象（如图、整数集等）的计数和结构分析提供了统一的框架。 超图容器引理 (The Hypergraph Container Lemma) 对于任意 $k \\in \\mathbb{N}, c &gt; 0$，存在 $\\delta &gt; 0$ 使得如下命题成立：设 $\\mathcal{H}$ 是一个 $k$-一致超图，且参数 $\\tau \\in (0,1)$ 满足： \\Delta_\\ell(\\mathcal{H}) \\le c \\cdot \\tau^{\\ell-1} \\frac{e(\\mathcal{H})}{v(\\mathcal{H})}, \\quad \\forall 1 \\le \\ell \\le k（即超图的各阶度数是“良好分布”的）。则存在一个 $V(\\mathcal{H})$ 子集的集合 $\\mathcal{C}$（称为容器集合），以及一个函数 $f: \\mathcal{P}(V(\\mathcal{H})) \\to \\mathcal{C}$，满足： (a) 对于 $\\mathcal{H}$ 中任意一个独立集 $I \\in \\mathcal{I}(\\mathcal{H})$，都存在一个小的“指纹”子集 $S \\subset I$，满足 $|S| \\le \\tau \\cdot v(\\mathcal{H})$，且 $I \\subset f(S)$。 (b) 对于任意一个容器 $C \\in \\mathcal{C}$，它所含的超边数量很少，即 $e(\\mathcal{H}[C]) \\le (1-\\delta)e(\\mathcal{H})$。（注：讲义中给出的结论是 $|C| \\le (1-\\delta)v(\\mathcal{H})$，两者在特定条件下等价，都表明容器不是“满”的）。 核心思想：任何一个没有特定结构的组合对象（即超图中的一个独立集），都可以被一个“几乎”没有该结构（即包含的超边很少）的、更大的“容器”所包含。并且，这样的容器数量相对较少，可以被有效控制。 一、$k$项等差数列问题容器方法的第一个经典应用是解决数论中的组合问题，例如，一个整数子集中不含特定模式（如等差数列）的数量。 定理21 对于任意 $\\beta &gt; 0, k \\in \\mathbb{N}$，存在常数 $C$ 使得以下命题成立：对于每个足够大的 $n \\in \\mathbb{N}$，如果 $m \\ge C n^{1 - 1/(k-1)}$，那么在集合 $\\{1, \\dots, n\\}$ 中，不包含 $k$ 项等差数列的 $m$ 元子集，其数量至多为 ${\\beta n \\choose m}$。直观理解：一个足够大的整数子集，如果它不含 $k$-AP，那它是一种非常“罕见”的结构。 推论22 对于任意 $\\epsilon &gt; 0$，存在 $C &gt; 0$ 使得如果 $p \\ge Cn^{-1/(k-1)}$，那么以极高概率，随机子集 $[n]_p$ 中任意一个大小超过 $\\epsilon pn$ 的子集 $A$，都必然包含一个 $k$ 项等差数列。 这两个结论的证明，依赖于一个关于等差数列的超饱和定理。 定理23 (Szemerédi 定理的超饱和版本) 对于任意 $\\epsilon &gt; 0, k \\in [n]$，存在 $\\delta &gt; 0$ 使得以下命题成立：任何一个大小超过 $\\epsilon n$ 的子集 $A \\subset [n]$，都必然包含大量（至少 $\\delta n^2$ 个）的 $k$ 项等差数列。 证明思路: 编码: 将 $k$-AP 编码成一个 $k$-一致超图 $\\mathcal{H}$，其顶点集是 $[n]$，超边是所有 $k$-AP。 迭代应用容器引理: 从一个不含 $k$-AP 的集合 $B$ 开始，它对应 $\\mathcal{H}$ 的一个独立集。 反复应用超图容器引理。在每一步，如果当前容器 $C_t$ 仍然很大（$|C_t| &gt; \\epsilon n$），那么根据 Theorem 23，它必然包含大量的 $k$-AP，因此其诱导超图 $\\mathcal{H}[C_t]$ 是稠密的，满足再次应用容器引理的条件。 由于每次应用容器引理都会使容器大小按比例缩小，经过常数次迭代后，必然会得到一个大小不超过 $\\epsilon n$ 的最终容器。 计数: 我们证明了每个不含 $k$-AP 的集合 $B$ 都可以被一个大小不超过 $\\epsilon n$ 的容器所包含。因此，不含 $k$-AP 的 $m$ 元子集的总数，可以通过对所有可能的小指纹 $S$ 和小容器 $C$ 进行求和来估计。通过标准的组合不等式放缩，可以证明这个总数远小于总的 $m$ 元子集数量，得到 Theorem 21 的结论。推论22 则是通过马尔可夫不等式直接得出。 二、不含特定$H$子图的图 (Containers for $H$-free graphs)这部分内容将容器方法应用到经典的图论极值问题中，旨在刻画那些不含特定子图 $H$ 的图的典型结构和数量。 首先引入几个要用到的概念： 色数 $\\chi(H)$：对图 $H$ 的所有顶点进行染色，使得任意两个相邻顶点颜色不同所需的最少颜色数。 $k$-部图 ($k$-partite graph)：一个图的顶点集可以被分割为 $k$ 个独立的子集 $V_1, \\dots, V_k$（称为“部”），使得所有边都只连接不同子集中的顶点（即任何一个 $V_i$ 内部都没有边）。 $m_2(H)$：一个与图 $H$ 密度相关的参数，定义为 $m_2(H) = \\max \\left\\{ \\frac{e(F)-1}{v(F)-2} : F \\subset H, v(F) \\ge 3 \\right\\}$。它决定了在随机图中找到 $H$ 的概率阈值。 $\\epsilon$-接近 $k$-部图：一个图的顶点集可以被划分成 $k$ 个部分，使得那些“错误”的边（即连接同一个部分内部两点的边）的总数非常少。 接下来是本节的三个核心定理： 定理24 (极值数定理的随机版本) 对于任意满足 $\\Delta(H) \\ge 2$ 的图 $H$ 和任意 $\\epsilon &gt; 0$，存在常数 $C&gt;0$。如果 $p \\ge Cn^{-1/m_2(H)}$，则有很高的概率： ex(G(n,p), H) \\le \\left(1 - \\frac{1}{\\chi(H)-1} + \\epsilon\\right) p \\binom{n}{2}直观理解：随机图 $G(n,p)$ 中不含 $H$ 的最大子图，其边数不会比图兰定理预测的“最优”结构（即 $(\\chi(H)-1)$-部图）的边数多太多。 定理25 (稳定性定理) 对于任意满足 $\\Delta(H) \\ge 2$ 的图 $H$ 和任意 $\\epsilon &gt; 0$，存在常数 $C&gt;0, \\delta&gt;0$。如果 $p \\ge Cn^{-1/m_2(H)}$，则下述命题以极高概率成立：任何一个不含 $H$ 的子图 $G \\subset G(n,p)$，只要它的边数足够多（即 $e(G) \\ge \\left(1 - \\frac{1}{\\chi(H)-1} - \\delta\\right) p \\binom{n}{2}$），那么它就必然“长得像”一个 $(\\chi(H)-1)$-部图（即以 $\\epsilon pn^2$ 的代价可变为 $(\\chi(H)-1)$-部图）。 定理26 (计数稳定性定理) 对于任意满足 $\\chi(H) \\ge 3$ 的图 $H$ 和任意 $\\epsilon &gt; 0$，存在常数 $C&gt;0$。如果 $m \\ge Cn^{2-1/m_2(H)}$，那么几乎所有（在所有 $m$ 条边的图中占 $1-o(1)$ 的比例）不含 $H$ 的 $n$ 顶点、$m$ 边图都“长得像”一个 $(\\chi(H)-1)$-部图（即以 $\\epsilon m$ 的代价可变为 $(\\chi(H)-1)$-部图）。 这三个定理的证明都依赖于一个核心的容器定理 (Theorem 28)，而该定理的证明又需要一个超饱和定理 (Theorem 27)。 定理27 (Erdős-Simonovits 稳定性定理的超饱和版本) 对于任意图 $H$ 和任意 $\\epsilon &gt; 0$，存在 $\\delta &gt; 0$ 使得对所有 $n \\in \\mathbb{N}$，下述命题成立：如果一个 $n$ 顶点的图 $G$ 的边数足够多，即 $e(G) &gt; \\left(1 - \\frac{1}{\\chi(H)-1} - \\delta\\right) \\binom{n}{2}$，那么 $G$ 只有两种可能：(1) $G$ 是 $\\epsilon n^2$-接近 $(\\chi(H)-1)$-部图的；(2) $G$ 包含大量（至少 $\\epsilon n^{v(H)}$ 个）的 $H$ 副本。核心思想：一个稠密的、但又不像 $(\\chi(H)-1)$-部图的图，必然会“超饱和”地包含 $H$。 有了这个工具，我们就可以证明本节的基石： 定理28 (无-$H$图的容器定理) 对于任意图 $H$ 和任意 $\\epsilon&gt;0$，存在 $\\delta&gt;0, C&gt;0$ 使得以下命题成立。存在一个图的集合 $\\mathcal{G}$ 和一个函数 $f: \\mathcal{P}(E(K_n)) \\to \\mathcal{G}$ 使得：(a) 对任意 $G \\in \\mathcal{G}$，它只有两种可能： (1) $e(G) \\le \\left(1 - \\frac{1}{\\chi(H)-1} - \\delta\\right) \\binom{n}{2}$ (边数较少)； (2) $G$ 是 $\\epsilon n^2$-接近 $(\\chi(H)-1)$-部图的 (结构稳定)。(b) 对于每个不含 $H$ 的 $n$ 顶点图 $I$，都存在一个小的“指纹”子图 $S \\subset I$，满足 $e(S) \\le Cn^{2-1/m_2(H)}$ 且 $I \\subset f(S)$。 证明思路:证明采用了迭代方法。 编码: 将 $H$ 的副本编码成一个 $e(H)$-一致超图 $\\mathcal{H}$，其顶点集是 $E(K_n)$。不含 $H$ 的图就是 $\\mathcal{H}$ 的独立集。 启动: 对任意一个不含 $H$ 的图 $I$，首次应用超图容器引理（取 $\\tau=n^{-1/m_2(H)}$），得到一个指纹 $S_1$ 和一个比 $K_n$ 小的容器 $C_1$。 迭代: 对容器 $C_t$ 进行检查。 如果 $C_t$ 已经满足(a)中的两种情况之一，则迭代终止，将其放入最终的集合 $\\mathcal{G}$。 否则，根据 Theorem 27，$C_t$ 必然包含大量的 $H$ 副本。这意味着其对应的诱导超图 $\\mathcal{H}[C_t]$ 是稠密的，满足再次应用容器引理的条件。 收敛: 每次迭代，容器的大小 $e(C_t)$ 都会按比例缩小。因此，经过常数次迭代后，容器必然会缩小到满足条件(a)的程度。 有了 Theorem 28 这个强大的结构性结论，定理24、25、26 就可以通过标准的概率论证（如Chernoff界）和组合计数推导出来。 三、随机图的拉姆齐理论 (Ramsey properties of random graphs)这部分展示了容器方法如何解决随机环境下的拉姆齐问题，即在一个随机图 $G(n,p)$ 中寻找单色子结构。 定义30 ([r]-染色图) 一个 [r]-染色图 是一个图 $G$ 配上一个染色函数 $c: E(G) \\to \\mathcal{P}([r]) \\setminus \\{\\emptyset\\}$，它为每条边赋予一个或多个来自集合 $\\{1, \\dots, r\\}$ 的颜色。我们称这样的图是无单色$H$的，如果不存在一个 $H$ 的副本，其所有边的颜色集合的交集非空。 定理29 (Rödl–Ruciński, 1995) 对于任意图 $H$ 和任意 $r \\in \\mathbb{N}$，如果 $p \\gg n^{-1/m_2(H)}$，那么以高概率，对 $G(n,p)$ 的边进行任意的 $r$-染色，都必然会产生一个单色的 $H$ 副本。核心思想：一旦随机图的密度 $p$ 越过某个由 $m_2(H)$ 决定的阈值，它就不仅包含 $H$，而且是“Ramsey饱和”的，无法通过染色来避免单色的 $H$。 证明同样需要一个超饱和定理和一个容器定理。 定理31 (拉姆齐定理的超饱和版本) 对于任意图 $H$ 和 $r \\in \\mathbb{N}$，存在 $\\delta &gt; 0$ 使得对于所有足够大的 $n$：如果图 $G$ 的边数足够多（即 $e(G) &gt; (1/2 - \\delta)n^2$），那么对 $G$ 的任意 $[r]$-染色方案都包含大量（至少 $\\delta n^{v(H)}$ 个）的单色 $H$。 定理32 (无单色$H$染色图的容器定理) 对于任意图 $H, r \\in \\mathbb{N}, \\epsilon&gt;0$，存在 $\\delta&gt;0, C&gt;0$ 使得对于任意 $n \\in \\mathbb{N}$，存在一个 $[r]$-染色图的集合 $\\mathcal{G}$ 和一个函数 $f: \\mathcal{P}(E(K_n))^r \\to \\mathcal{G}$ 使得：(a) 对任意 $G \\in \\mathcal{G}$，其边数至多为 $(1-\\delta)\\binom{n}{2}$。(b) 对于每个无单色$H$的 $n$ 顶点 $[r]$-染色图 $I$，都存在一个小的“指纹”子染色 $S \\subset I$，满足 $e(S) \\le Cn^{2-1/m_2(H)}$ 且 $I \\subset f(S)$。 定理29的证明思路: 构造超图: 构造一个超图 $\\mathcal{H}$，其顶点是所有可能的“颜色-边”对（共 $r\\binom{n}{2}$ 个顶点），超边对应 $K_n$ 中所有可能的单色 $H$ 副本。一个无单色$H$的染色图就对应 $\\mathcal{H}$ 中的一个独立集。 应用容器定理: 利用 Theorem 31 作为超饱和工具，通过与前面类似的迭代过程，可以证明 Theorem 32。 概率论证: 假设 $G(n,p)$ 中存在一个无单色$H$的 $r$-染色 $I$。根据 Theorem 32，这个 $I$ 必须被某个容器 $G \\in \\mathcal{G}$ 包含。这意味着 $I$ 的指纹 $S$ 必须是 $G(n,p)$ 的子图，并且 $G(n,p)$ 的所有边都必须在 $G$ 中。 联合界: 对所有可能的指纹 $S$ 使用联合界（Union Bound）。$ P(\\text{坏事件}) \\le \\sum_{S} P(S \\subset G(n,p) \\text{ and } G(n,p) \\subset G(S)) $由于 $G(S)$ 的边数比 $\\binom{n}{2}$ 少一个正比例，而 $G(n,p)$ 的期望边数很大，事件 $G(n,p) \\subset G(S)$ 发生的概率极小（指数级小）。将这个极小的概率与所有可能指纹的数量相乘，只要 $p$ 足够大，最终结果将趋向于0。","link":"/Com-Pro/hc2.html"},{"title":"Chapter2_MATH1408","text":"线代回顾1. 向量空间与内积空间 (Vector Spaces and Inner Product Spaces)1.1 实向量空间 $R^n$ (Real Vector Space $R^n$) 定义: $R^n$ 是所有 $n$ 维实数列向量的集合。对于 $\\alpha, \\beta \\in R^n$, $\\alpha = (x_1, x_2, \\dots, x_n)^T$, $\\beta = (y_1, y_2, \\dots, y_n)^T$. 标准内积 (点积): $\\alpha \\cdot \\beta = \\alpha^T \\beta = x_1y_1 + x_2y_2 + \\dots + x_ny_n$. 向量的长度 (范数): $|\\alpha| = \\sqrt{\\alpha \\cdot \\alpha}$. 矩阵的转置: $A^T$ (原文用 $A’$). 1.2 三维向量的叉积 (Cross Product in $R^3$) 对于 $\\alpha = (x_1, x_2, x_3)^T$, $\\beta = (y_1, y_2, y_3)^T \\in R^3$:$\\alpha \\times \\beta = (x_2y_3 - x_3y_2, x_3y_1 - x_1y_3, x_1y_2 - x_2y_1)^T$. 几何意义: $|\\alpha \\times \\beta| = |\\alpha| |\\beta| \\sin\\theta$, 是由 $\\alpha, \\beta$ 张成的平行四边形的面积。 $\\alpha \\times \\beta$ 垂直于 $\\alpha$ 和 $\\beta$ 张成的平面。 混合积 (标量三重积): 对于 $\\alpha, \\beta, \\gamma \\in R^3$:$(\\alpha \\times \\beta) \\cdot \\gamma = \\begin{vmatrix} x_1 &amp; y_1 &amp; z_1 \\\\ x_2 &amp; y_2 &amp; z_2 \\\\ x_3 &amp; y_3 &amp; z_3 \\end{vmatrix}$其绝对值是由 $\\alpha, \\beta, \\gamma$ 张成的平行六面体的体积。 1.3 (酉)内积空间 / 欧几里得空间 (Unitary/Inner Product Space / Euclidean Space) 定义: 一个复（或实）向量空间 $V$ 称为一个内积空间，如果存在一个映射 $\\phi: V \\times V \\to C$ (或 $R$)，记作 $(\\alpha, \\beta)$，满足以下性质 (以复内积空间为例): (Hermitian对称性/共轭对称性): $(\\alpha, \\beta) = \\overline{(\\beta, \\alpha)}$ (对于实内积空间，即对称性 $(\\alpha, \\beta) = (\\beta, \\alpha)$). (对第一变元的线性性): $(k\\alpha + \\beta, \\gamma) = k(\\alpha, \\gamma) + (\\beta, \\gamma)$.(由1,2可知对第二变元共轭线性: $(\\alpha, k\\beta + \\gamma) = \\overline{(k\\beta + \\gamma, \\alpha)} = \\overline{k(\\beta, \\alpha) + (\\gamma, \\alpha)} = \\bar{k}\\overline{(\\beta, \\alpha)} + \\overline{(\\gamma, \\alpha)} = \\bar{k}(\\alpha, \\beta) + (\\alpha, \\gamma)$). (正定性): $(\\alpha, \\alpha) \\ge 0$，且 $(\\alpha, \\alpha) = 0$ 当且仅当 $\\alpha = 0$. 欧几里得空间: 实内积空间。 酉空间: 复内积空间。 $C^n$ 中的标准内积: 对于 $\\alpha = (x_1, \\dots, x_n)^T, \\beta = (y_1, \\dots, y_n)^T \\in C^n$:$(\\alpha, \\beta) = \\beta^ \\alpha = x_1\\bar{y}_1 + x_2\\bar{y}_2 + \\dots + x_n\\bar{y}_n$. (注意原文 $\\alpha \\cdot \\beta = \\beta^T \\alpha$ 可能是针对实数，此处为复数标准内积，$\\beta^$ 是 $\\beta$ 的共轭转置)。 矩阵的共轭转置: $A^*$ (原文用 $A^T$ 可能指实数情况，或需区分)。 2. 矩阵分解 (Matrix Factorization)2.1 Hermite标准形 (Hermite Normal Form / Row Echelon Form) 定义: 通过初等行变换可以将任何矩阵化为Hermite标准形（行最简形矩阵）。 特点: 若有零行，则零行在非零行的下方。 非零行的第一个非零元素（称为主元）为1。 主元所在列的其他元素均为0。 各非零行的主元所在列的列标随行标的增加而严格递增。 应用: 求解线性方程组 $AX=\\beta$ (如高斯消元法)。 2.2 满秩分解 (Full Rank Factorization) 定理: 若 $m \\times n$ 矩阵 $A$ 的秩为 $r$ ($r &gt; 0$)，则存在 $m \\times r$ 的列满秩矩阵 $L$ 和 $r \\times n$ 的行满秩矩阵 $R$，使得 $A = LR$。这个分解称为矩阵 $A$ 的一个满秩分解。 构造一种满秩分解:设 $A$ 通过初等行变换化为 Hermite 标准形 $H = \\begin{pmatrix} H_r \\\\ O \\end{pmatrix}$，其中 $H_r$ 是 $r \\times n$ 的行满秩矩阵，其主元分别在第 $j_1, j_2, \\dots, j_r$ 列。令 $L = (A_{j_1}, A_{j_2}, \\dots, A_{j_r})$ (即 $A$ 中对应 $H$ 主元列的那些列向量组成的矩阵)。令 $R = H_r$ (即 $H$ 的非零行组成的矩阵)。则 $A = LR$ 是一个满秩分解。 证明思路:初等行变换不改变列向量之间的线性关系。$H$ 的第 $k$ 列 $H_k$ 可以由 $H$ 的主元列 $H_{j_1}, \\dots, H_{j_r}$ 线性表示，即 $H_k = \\sum_{s=1}^r c_s H_{j_s}$。那么 $A$ 的第 $k$ 列 $A_k$ 也满足 $A_k = \\sum_{s=1}^r c_s A_{j_s}$。这表明 $A_k = L (H_r)_k$，其中 $(H_r)_k$ 是 $H_r$ 的第 $k$ 列。因此 $A=LR$。$L$ 的列向量是 $A$ 的 $r$ 个线性无关的列向量（因为它们对应 Hermite 形的主元列），所以 $L$ 列满秩。$H_r$ 是 Hermite 形的非零部分，显然行满秩。 应用: 求解线性方程组，理论分析。 示例:$A = \\begin{pmatrix} 3 &amp; -1 &amp; 3 &amp; 2 &amp; 5 \\\\ 5 &amp; -3 &amp; 2 &amp; 3 &amp; 4 \\\\ 1 &amp; -3 &amp; -5 &amp; 0 &amp; -7 \\\\ 2 &amp; -2 &amp; -1 &amp; 1 &amp; -1 \\end{pmatrix}$ 求一个满秩分解。(需要通过行变换得到Hermite标准形，然后找出主元列来构造 $L$ 和 $R$) 2.3 LU 分解 (LU Decomposition) 定义: 若方阵 $A$ 可以分解为一个下三角矩阵 $L$ (通常主对角线元素为1) 和一个上三角矩阵 $U$ 的乘积，即 $A=LU$，则称其为 $A$ 的 LU 分解。 存在性: 如果方阵 $A$ 的所有顺序主子式均非零，则 $A$ 有唯一的 $LDU$ 分解，其中 $L$ 是单位下三角阵， $D$ 是对角阵， $U$ 是单位上三角阵。若 $A$ 可逆，则 $A$ 有 $LU$ 分解（$L$为单位下三角阵）。 应用: 求解线性方程组 $AX=\\beta$ 可转化为 $LY=\\beta$ 和 $UX=Y$。计算行列式 $|A|=|L||U|=|U|$ (当 $L$ 为单位下三角时)。 高斯消元法与LU分解: 高斯消元的过程（不进行行交换时）可以看作是在构造LU分解。 带行交换的LU分解 (PA=LU): 对于任意可逆方阵 $A$，都存在一个置换矩阵 $P$，使得 $PA$ 有 $LU$ 分解。 LDU分解: $A=LDU$，其中 $L$ 是单位下三角矩阵， $D$ 是对角矩阵， $U$ 是单位上三角矩阵。 2.4 Cholesky 分解 (Cholesky Decomposition) 定义: 若 $A$ 是一个对称正定矩阵 (Hermitian 正定矩阵)，则 $A$ 可以唯一地分解为一个下三角矩阵 $G$ 和其共轭转置 $G^$ (对于实对称矩阵是 $G^T$) 的乘积，即 $A=GG^$ (或 $A=GG^T$ 对于实数情况)。$G$ 通常要求对角线元素为正。 与LU分解的关系: Cholesky 分解可以看作是对称正定矩阵的一种特殊的 LU 分解 ($L=G, U=G^T$)。 优点: 计算效率高，数值稳定性好。 2.5 置换矩阵 (Permutation Matrix) 定义: 置换矩阵是由单位矩阵经过行（或列）的若干次交换得到的方阵。 性质: 每一行和每一列只有一个元素为1，其余元素为0。 $P^{-1} = P^T$ (置换矩阵是正交矩阵)。 任意置换矩阵可以表示为若干个对换矩阵（交换两行的初等矩阵）的乘积。 应用: 在LU分解中处理行交换 (PA=LU)。 3. 线性方程组 (Systems of Linear Equations)3.1 单个线性方程 方程 $a_1x_1 + a_2x_2 + \\dots + a_nx_n = b$ 可以表示为 $\\alpha^T X = b$，其中 $\\alpha=(a_1, \\dots, a_n)^T, X=(x_1, \\dots, x_n)^T$。 几何意义 ($n=2,3$): $n=2$: 平面上一条直线。 $n=3$: 空间中一个平面。 解的结构: 若 $b=0$ (齐次方程)，解集是与向量 $\\alpha$ 正交的向量组成的子空间（超平面）。 若 $b \\ne 0$ (非齐次方程)，解集是一个仿射子空间（平移的超平面）。 3.2 线性方程组 $AX=\\beta$ 可以写成向量形式: $x_1\\alpha_1 + x_2\\alpha_2 + \\dots + x_n\\alpha_n = \\beta$，其中 $\\alpha_j$ 是 $A$ 的列向量。 有解的充要条件: $\\beta \\in \\text{span}\\{\\alpha_1, \\dots, \\alpha_n\\}$，即 $\\beta$ 可以由 $A$ 的列向量线性表示。等价于 $\\text{rank}(A) = \\text{rank}(A|\\beta)$ (Rouché-Capelli 定理)。 列空间 (Column Space): $C(A) = \\text{span}\\{\\alpha_1, \\dots, \\alpha_n\\}$。方程组有解当且仅当 $\\beta \\in C(A)$。 行空间 (Row Space): $R(A) = C(A^T)$。 零空间 (Null Space): $N(A) = \\{X | AX=0\\}$。$N(A)$ 是 $R^n$ 的一个子空间。 左零空间 (Left Null Space): $N(A^T) = \\{Y | A^TY=0 \\text{ or } Y^TA=0\\}$。 3.3 四个基本子空间 (Four Fundamental Subspaces)对于 $m \\times n$ 矩阵 $A$: 列空间 (Column Space) $C(A) \\subseteq R^m$ (由 $A$ 的列向量张成) 零空间 (Null Space) $N(A) \\subseteq R^n$ (齐次方程 $AX=0$ 的解空间) 行空间 (Row Space) $R(A) = C(A^T) \\subseteq R^n$ (由 $A$ 的行向量张成) 左零空间 (Left Null Space) $N(A^T) \\subseteq R^m$ (齐次方程 $A^TY=0$ 的解空间) 基本子空间的正交性: 行空间 $R(A)$ 与零空间 $N(A)$ 正交 ($R(A) \\perp N(A)$)。 证明: 若 $y \\in R(A)$, $x \\in N(A)$。则 $y = A^T z$ 对于某个 $z$，且 $Ax=0$。$(y, x) = y^T x = (A^T z)^T x = z^T A x = z^T (Ax) = z^T 0 = 0$. 列空间 $C(A)$ 与左零空间 $N(A^T)$ 正交 ($C(A) \\perp N(A^T)$)。(应用上一条到 $A^T$ 即可) 维数定理 (Rank-Nullity Theorem): $\\text{dim } R(A) + \\text{dim } N(A) = n$ (列数) $\\text{dim } C(A) + \\text{dim } N(A^T) = m$ (行数) 且有 $\\text{dim } R(A) = \\text{dim } C(A) = \\text{rank}(A)$. 直和分解: $R^n = R(A) \\oplus N(A)$ $R^m = C(A) \\oplus N(A^T)$ 4. 子空间 (Subspaces)4.1 子空间的并集与和证明是子空间需要：零向量，加法封闭，数乘封闭。 子空间的并集: 若干个子空间的并集通常不是子空间，除非其中一个包含其他所有子空间。 引理: 若 $U_1, U_2, \\dots, U_m$ 是向量空间 $V$ 的真子空间，则 $\\bigcup_{i=1}^m U_i \\neq V$ (如果域 $F$ 是无限域，或者 $V$ 的维数大于1且 $m &lt; |F|+1$)。 证明 (m=2, $U_1 \\not\\subseteq U_2, U_2 \\not\\subseteq U_1$):取 $\\alpha \\in U_1 \\setminus U_2$, $\\beta \\in U_2 \\setminus U_1$。考虑向量 $\\alpha + \\beta$。若 $\\alpha + \\beta \\in U_1$，则 $\\beta = (\\alpha+\\beta) - \\alpha \\in U_1$，与假设矛盾。若 $\\alpha + \\beta \\in U_2$，则 $\\alpha = (\\alpha+\\beta) - \\beta \\in U_2$，与假设矛盾。因此 $\\alpha + \\beta \\notin U_1 \\cup U_2$。所以 $U_1 \\cup U_2$ 不是子空间，且 $U_1 \\cup U_2 \\neq V$ (如果V中还有其他元素)。PPT中的证明思路更一般，用了反证法和构造特定线性组合。 子空间的和: 若 $W, U$ 是 $V$ 的子空间，则它们的和 $W+U = \\{w+u | w \\in W, u \\in U\\}$ 也是 $V$ 的一个子空间。 证明: 零向量: $0 \\in W, 0 \\in U \\implies 0+0=0 \\in W+U$. 加法封闭: 取 $\\alpha_1 = w_1+u_1, \\alpha_2 = w_2+u_2 \\in W+U$.$\\alpha_1 + \\alpha_2 = (w_1+w_2) + (u_1+u_2)$. 因为 $w_1+w_2 \\in W, u_1+u_2 \\in U$，所以 $\\alpha_1+\\alpha_2 \\in W+U$. 数乘封闭: 取 $\\alpha = w+u \\in W+U$, $k \\in F$.$k\\alpha = k(w+u) = kw + ku$. 因为 $kw \\in W, ku \\in U$，所以 $k\\alpha \\in W+U$. 子空间和的维数公式:$\\text{dim}(W+U) = \\text{dim } W + \\text{dim } U - \\text{dim}(W \\cap U)$. 证明思路:取 $W \\cap U$ 的一组基 $\\{ \\gamma_1, \\dots, \\gamma_k \\}$.将其扩充为 $W$ 的一组基 $\\{ \\gamma_1, \\dots, \\gamma_k, w_1, \\dots, w_s \\}$.将其扩充为 $U$ 的一组基 $\\{ \\gamma_1, \\dots, \\gamma_k, u_1, \\dots, u_t \\}$.可以证明 $\\{ \\gamma_1, \\dots, \\gamma_k, w_1, \\dots, w_s, u_1, \\dots, u_t \\}$ 是 $W+U$ 的一组基。因此 $\\text{dim}(W+U) = k+s+t$.$\\text{dim } W = k+s$, $\\text{dim } U = k+t$, $\\text{dim}(W \\cap U) = k$.所以 $\\text{dim } W + \\text{dim } U - \\text{dim}(W \\cap U) = (k+s) + (k+t) - k = k+s+t = \\text{dim}(W+U)$. 4.2 直和 (Direct Sum) 定义: 若 $W, U$ 是 $V$ 的子空间，且 $W \\cap U = \\{0\\}$，则称 $W+U$ 为 $W$ 和 $U$ 的直和，记作 $W \\oplus U$. 等价条件: $W+U$ 是直和。 $\\forall \\alpha \\in W+U$，$\\alpha$ 可以唯一地表示为 $w+u$，其中 $w \\in W, u \\in U$. 证明 (1 $\\implies$ 2): 假设 $\\alpha = w_1+u_1 = w_2+u_2$. 则 $w_1-w_2 = u_2-u_1$.因为 $w_1-w_2 \\in W$ 且 $u_2-u_1 \\in U$, 所以 $w_1-w_2 \\in W \\cap U = \\{0\\}$.故 $w_1-w_2=0 \\implies w_1=w_2$. 类似地 $u_1=u_2$. 唯一性得证。 证明 (2 $\\implies$ 1): 若 $W \\cap U \\neq \\{0\\}$, 取非零向量 $x \\in W \\cap U$.则 $0 = x + (-x)$, 其中 $x \\in W, -x \\in U$.也有 $0 = 0 + 0$, 其中 $0 \\in W, 0 \\in U$.这样零向量的表示不唯一，与条件2矛盾。所以 $W \\cap U = \\{0\\}$. 若 $0 = w+u$ 且 $w \\in W, u \\in U$，则 $w=0, u=0$. (这是条件2的特例) $\\text{dim}(W+U) = \\text{dim } W + \\text{dim } U$. (由维数公式和 $W \\cap U = \\{0\\}$ 直接得到) 4.3 补子空间 (Complementary Subspace) 定义: 若 $U$ 是 $V$ 的子空间，如果存在子空间 $W$ 使得 $V = W \\oplus U$，则称 $W$ 是 $U$ 的一个补子空间。 存在性: 对于 $V$ 的任意子空间 $U$，其补子空间总是存在的（但不唯一，除非 $U=\\{0\\}$ 或 $U=V$）。 正交补 (Orthogonal Complement): 在内积空间中，对于子空间 $U$，其正交补 $U^\\perp = \\{v \\in V | (v,u)=0, \\forall u \\in U\\}$ 是 $U$ 的一个补子空间，即 $V = U \\oplus U^\\perp$. 正交补是唯一的。 5. 投影 (Projection)5.1 向量到向量的投影 (Projection of a vector onto another vector) 向量 $\\beta$ 在向量 $\\alpha$ 方向上的投影向量 $\\gamma$ (或 $\\text{proj}_\\alpha \\beta$):$\\gamma = \\frac{(\\beta, \\alpha)}{(\\alpha, \\alpha)} \\alpha = \\frac{\\alpha^T \\beta}{\\alpha^T \\alpha} \\alpha$ (对于实向量空间标准内积)。 $\\eta = \\beta - \\gamma$ 是与 $\\alpha$ 正交的向量。 投影矩阵 (onto a line spanned by $\\alpha$): $P = \\frac{\\alpha \\alpha^T}{\\alpha^T \\alpha}$.则 $\\gamma = P \\beta$. 性质: $P^2=P$ (幂等性)，$P^T=P$ (对称性，因为 $\\alpha \\alpha^T$ 是对称的)。 $E-P$ 是投影到 $\\alpha$ 的正交补空间上的投影矩阵。 5.2 向量到子空间的投影 (Projection of a vector onto a subspace) 设 $W$ 是一个子空间，其一组基为 $\\{\\alpha_1, \\dots, \\alpha_m\\}$. 令 $A = (\\alpha_1, \\dots, \\alpha_m)$ (列向量构成矩阵 $A$)。 向量 $\\beta$ 在子空间 $W = C(A)$ 上的投影 $\\gamma = \\text{proj}_W \\beta$ 是 $W$ 中离 $\\beta$ 最近的向量。 $\\beta - \\gamma$ 必须与 $W$ 中的所有向量正交，即 $(\\beta-\\gamma, w)=0, \\forall w \\in W$.特别地，$(\\beta-\\gamma, \\alpha_i)=0$ 对所有基向量 $\\alpha_i$ 成立。 $\\gamma$ 可以表示为 $Ax$ 的形式，其中 $x=(x_1, \\dots, x_m)^T$ 是坐标。$\\alpha_i^T (\\beta - Ax) = 0 \\implies A^T(\\beta - Ax) = 0 \\implies A^T Ax = A^T \\beta$.这个方程称为正规方程 (Normal Equations)。 如果 $A$ 的列向量线性无关 (即 $\\{\\alpha_i\\}$ 确实是基)，则 $A^T A$ 是可逆的 $m \\times m$ 矩阵。$x = (A^T A)^{-1} A^T \\beta$.$\\gamma = Ax = A(A^T A)^{-1} A^T \\beta$. 投影矩阵 (onto subspace $W=C(A)$): $P_W = A(A^T A)^{-1} A^T$. 性质: $P_W^2 = P_W$ (幂等性)，$P_W^T = P_W$ (对称性)。 $E-P_W$ 是投影到 $W$ 的正交补空间 $W^\\perp$ 上的投影矩阵。 5.3 最小二乘法 (Least Squares) 对于无解的线性方程组 $AX=\\beta$ (即 $\\beta \\notin C(A)$)，我们寻找一个“最佳”近似解 $\\hat{X}$，使得 $|AX - \\beta|$ 最小。 这个最小化问题等价于找到 $\\beta$ 在 $A$ 的列空间 $C(A)$ 上的投影 $A\\hat{X}$。 因此，最小二乘解 $\\hat{X}$ 满足正规方程: $A^T A \\hat{X} = A^T \\beta$. 如果 $A$ 列满秩，则 $\\hat{X} = (A^T A)^{-1} A^T \\beta$ 是唯一的最小二乘解。 如果 $A$ 不是列满秩，则 $A^T A$ 不可逆，最小二乘解有无穷多个。通常取其中的范数最小解（使用伪逆）。 6. Gram-Schmidt 正交化过程 (Gram-Schmidt Orthogonalization) 目的: 将一组线性无关的向量 $\\{\\alpha_1, \\dots, \\alpha_m\\}$ 转化为一组正交向量 $\\{\\beta_1, \\dots, \\beta_m\\}$，使得 $\\text{span}\\{\\alpha_1, \\dots, \\alpha_k\\} = \\text{span}\\{\\beta_1, \\dots, \\beta_k\\}$ 对所有 $k=1, \\dots, m$ 成立。进一步单位化可得到标准正交基。 过程: $\\beta_1 = \\alpha_1$. $\\beta_2 = \\alpha_2 - \\text{proj}_{\\beta_1} \\alpha_2 = \\alpha_2 - \\frac{(\\alpha_2, \\beta_1)}{(\\beta_1, \\beta_1)} \\beta_1$. $\\beta_3 = \\alpha_3 - \\text{proj}_{\\beta_1} \\alpha_3 - \\text{proj}_{\\beta_2} \\alpha_3 = \\alpha_3 - \\frac{(\\alpha_3, \\beta_1)}{(\\beta_1, \\beta_1)} \\beta_1 - \\frac{(\\alpha_3, \\beta_2)}{(\\beta_2, \\beta_2)} \\beta_2$. … $\\beta_k = \\alpha_k - \\sum_{j=1}^{k-1} \\text{proj}_{\\beta_j} \\alpha_k = \\alpha_k - \\sum_{j=1}^{k-1} \\frac{(\\alpha_k, \\beta_j)}{(\\beta_j, \\beta_j)} \\beta_j$. 单位化: $\\varepsilon_k = \\frac{\\beta_k}{|\\beta_k|}$. 则 $\\{\\varepsilon_1, \\dots, \\varepsilon_m\\}$ 是一组标准正交向量。 6.1 QR 分解 (QR Decomposition) 定理: 任何 $m \\times n$ ($m \\ge n$) 的列满秩矩阵 $A$ (即 $A$ 的列向量线性无关) 可以分解为一个 $m \\times n$ 的正交列矩阵 $Q$ (其列向量构成标准正交组，即 $Q^T Q = I_n$) 和一个 $n \\times n$ 的可逆上三角矩阵 $R$ (通常要求对角线元素为正) 的乘积，即 $A=QR$. 与Gram-Schmidt的关系: Gram-Schmidt 正交化过程是QR分解的一种构造方法。设 $A = (\\alpha_1, \\dots, \\alpha_n)$，$Q = (\\varepsilon_1, \\dots, \\varepsilon_n)$。由Gram-Schmidt过程:$\\alpha_1 = r_{11}\\varepsilon_1$$\\alpha_2 = r_{12}\\varepsilon_1 + r_{22}\\varepsilon_2$…$\\alpha_k = \\sum_{j=1}^k r_{jk}\\varepsilon_j$其中 $r_{jj} = |\\beta_j|$ 且 $r_{ij} = (\\alpha_j, \\varepsilon_i)$ for $i &lt; j$.写成矩阵形式即 $A=QR$.$R = Q^T A$. 7. 等价关系与商空间 (Equivalence Relations and Quotient Spaces)7.1 等价关系 (Equivalence Relation) 定义: 集合 $S$ 上的一个二元关系 $\\sim$ 称为等价关系，如果它满足: 自反性 (Reflexivity): $\\forall a \\in S, a \\sim a$. 对称性 (Symmetry): $\\forall a, b \\in S$, if $a \\sim b$, then $b \\sim a$. 传递性 (Transitivity): $\\forall a, b, c \\in S$, if $a \\sim b$ and $b \\sim c$, then $a \\sim c$. 等价类 (Equivalence Class): 对于 $a \\in S$，其等价类 $\\bar{a}$ (或 $[a]$) 定义为 $\\bar{a} = \\{x \\in S | x \\sim a\\}$. 性质: $\\forall a \\in S, a \\in \\bar{a}$. $\\bar{a} = \\bar{b} \\iff a \\sim b$. 任意两个等价类要么相等，要么不相交。 划分 (Partition): 等价关系将集合 $S$ 划分为互不相交的等价类的并集。 商集 (Quotient Set): $S/\\sim = \\{\\bar{a} | a \\in S\\}$，即所有等价类的集合。 7.2 商空间 (Quotient Space) 设 $V$ 是域 $F$ 上的向量空间，$W$ 是 $V$ 的一个子空间。 在 $V$ 上定义关系 $\\sim$: $\\alpha \\sim \\beta \\iff \\alpha - \\beta \\in W$. 证明 (这是一个等价关系): 自反性: $\\alpha - \\alpha = 0 \\in W \\implies \\alpha \\sim \\alpha$. 对称性: 若 $\\alpha \\sim \\beta \\implies \\alpha - \\beta \\in W$. 则 $\\beta - \\alpha = -(\\alpha - \\beta) \\in W$ (因为 $W$ 是子空间). 所以 $\\beta \\sim \\alpha$. 传递性: 若 $\\alpha \\sim \\beta, \\beta \\sim \\gamma \\implies \\alpha - \\beta \\in W, \\beta - \\gamma \\in W$.则 $\\alpha - \\gamma = (\\alpha - \\beta) + (\\beta - \\gamma) \\in W$. 所以 $\\alpha \\sim \\gamma$. 向量 $\\alpha$ 所在的等价类记为 $\\alpha+W = \\{\\alpha+w | w \\in W\\}$，称为 $W$ 的一个 陪集 (coset)。 商空间 $V/W$: 所有陪集构成的集合 $V/W = \\{\\alpha+W | \\alpha \\in V\\}$. $V/W$ 上的向量空间结构: 加法: $(\\alpha+W) + (\\beta+W) = (\\alpha+\\beta)+W$. 数乘: $k(\\alpha+W) = (k\\alpha)+W$. (需要验证这些运算是良定义的，即不依赖于陪集代表元的选取)。 证明 (加法良定义): 设 $\\alpha+W = \\alpha’+W$ 且 $\\beta+W = \\beta’+W$.则 $\\alpha-\\alpha’ \\in W$ 且 $\\beta-\\beta’ \\in W$.我们想证明 $(\\alpha+\\beta)+W = (\\alpha’+\\beta’)+W$, 即 $(\\alpha+\\beta) - (\\alpha’+\\beta’) \\in W$.$(\\alpha+\\beta) - (\\alpha’+\\beta’) = (\\alpha-\\alpha’) + (\\beta-\\beta’)$.因为 $\\alpha-\\alpha’ \\in W$ 且 $\\beta-\\beta’ \\in W$，且 $W$ 是子空间，所以它们的和也在 $W$ 中。故加法良定义。数乘类似可证。 零元素: $0+W = W$. 负元素: $-(\\alpha+W) = (-\\alpha)+W$. 商空间的维数: 定理: 若 $V$ 是有限维向量空间，$W$ 是其子空间，则 $\\text{dim}(V/W) = \\text{dim } V - \\text{dim } W$. 证明思路:设 $\\text{dim } W = s$, $\\text{dim } V = n$.取 $W$ 的一组基 $\\{\\alpha_1, \\dots, \\alpha_s\\}$.将其扩充为 $V$ 的一组基 $\\{\\alpha_1, \\dots, \\alpha_s, \\alpha_{s+1}, \\dots, \\alpha_n\\}$.可以证明 $\\{\\alpha_{s+1}+W, \\dots, \\alpha_n+W\\}$ 构成 $V/W$ 的一组基。 张成性: 对任意 $\\beta+W \\in V/W$, $\\beta = \\sum_{i=1}^n b_i \\alpha_i$.$\\beta+W = (\\sum_{i=1}^s b_i \\alpha_i + \\sum_{i=s+1}^n b_i \\alpha_i) + W = (\\sum_{i=s+1}^n b_i \\alpha_i) + W = \\sum_{i=s+1}^n b_i (\\alpha_i+W)$. 线性无关性: 设 $\\sum_{i=s+1}^n k_i (\\alpha_i+W) = 0+W = W$.则 $(\\sum_{i=s+1}^n k_i \\alpha_i) + W = W \\implies \\sum_{i=s+1}^n k_i \\alpha_i \\in W$.所以 $\\sum_{i=s+1}^n k_i \\alpha_i = \\sum_{j=1}^s c_j \\alpha_j$ 对某些 $c_j$ 成立。即 $\\sum_{j=1}^s (-c_j) \\alpha_j + \\sum_{i=s+1}^n k_i \\alpha_i = 0$.因为 $\\{\\alpha_1, \\dots, \\alpha_n\\}$ 线性无关，所以所有系数 $k_i=0$ (以及 $c_j=0$)。因此 $V/W$ 的维数为 $n-s$. 余维数 (Codimension): $\\text{codim}_V W = \\text{dim}(V/W)$. 7.3 商空间与直和的同构 定理 (第一同构定理的某种形式或推论): 若 $V$ 是向量空间，$W$ 是其子空间。如果 $U$ 是 $W$ 在 $V$ 中的一个补子空间 (即 $V = W \\oplus U$)，则 $U \\cong V/W$ ( $U$ 与 $V/W$同构)。 证明思路:定义映射 $\\phi: U \\to V/W$ 为 $\\phi(u) = u+W$. $\\phi$ 是线性映射:$\\phi(k u_1 + u_2) = (k u_1 + u_2) + W = k(u_1+W) + (u_2+W) = k\\phi(u_1) + \\phi(u_2)$. $\\phi$ 是单射 (ker $\\phi = \\{0\\}$):若 $\\phi(u) = 0+W = W$, 则 $u+W = W \\implies u \\in W$.因为 $u \\in U$ 且 $V=W \\oplus U \\implies W \\cap U = \\{0\\}$, 所以 $u=0$. $\\phi$ 是满射:对任意 $\\alpha+W \\in V/W$. 因为 $V=W+U$, 所以 $\\alpha = w+u$ 对某个 $w \\in W, u \\in U$.则 $\\alpha+W = (w+u)+W = u+W = \\phi(u)$.因此 $\\phi$ 是同构映射。","link":"/MATH1408/Chapter2-MATH1408.html"},{"title":"容器定理应用进阶","text":"$K_{s,t}$-free 和 $C_{2k}$-free 图的计数这一部分的目标是计算不含特定二部图（如 $K_{s,t}$ 或偶环 $C_{2k}$）的图的数量。这类问题的解决依赖于一个强大的组合：平衡超饱和定理 + 超图容器引理。 1. 主要结论Theorem 46. (Balogh, Samotij, 2011) 对于任意固定的 $s, t$，不含 $K_{s,t}$ 的 $n$ 顶点图的数量至多为 $2^{O(n^{2 - 1/s})}$。 Theorem 47. (Morris, Saxton, 2016) 对于任意固定的 $k$，不含 $C_{2k}$ 的 $n$ 顶点图的数量至多为 $2^{O(n^{1 + 1/k})}$。 为了避免过于复杂的技术细节，本节以最简单但具有代表性的情况——$C_4$-free 图——为例来展示其证明方法。 2. $C_4$ 的平衡超饱和定理 (Theorem 48)对于容器方法而言，仅仅知道一个图里有很多 $C_4$ 是不够的。我们还需要保证这些 $C_4$ 是“良好分布”的，即没有某条边或某对边参与了过多的 $C_4$。这正是“平衡超饱和”的含义。 Theorem 48. 存在常数 $\\delta &gt; 0$ 和 $k_0 \\in \\mathbb{N}$。对于任意 $k &gt; k_0$ 和 $n \\in \\mathbb{N}$，给定一个有 $n$ 个顶点和 $kn^{3/2}$ 条边的图 $G$，存在一个 $G$ 中 $C_4$ 的集合 $\\mathcal{H}$，满足： (a) $|\\mathcal{H}| &gt; \\delta^3 k^4 n^2$ (集合足够大) (b) 每条边最多被用于 $\\delta^2 k^3 \\sqrt{n}$ 个 $\\mathcal{H}$ 中的成员。(边的度数有上界) (c) 每对边最多被用于 $\\delta k \\sqrt{n}$ 个 $\\mathcal{H}$ 中的成员。(边的共度有上界) 证明思路: 这个证明采用迭代构造的方法。我们从一个空的集合 $\\mathcal{H}$ 开始，在每一步都向其中添加一个“好”的 $C_4$，直到满足所有条件。 识别瓶颈: 直接证明(c)很困难。其难点在于控制相邻边对（即长度为2的路径）参与形成的 $C_4$ 数量。 强化条件 (引入(c’)): 为了解决这个瓶颈，作者引入了一个更强、更具体的技术条件(c’)。我们将 $C_4$（即 $K_{2,2}$）的四个顶点分成不相邻的两对，一对染成“红色”，另一对染成“蓝色”。 (c’) 每条以 $v$ 为中心的长度为2的路径，最多被用于: $\\delta k^2$ 个 $v$ 是红色的 $\\mathcal{H}$ 成员中。 $\\delta k \\sqrt{n}$ 个 $v$ 是蓝色的 $\\mathcal{H}$ 成员中。这个条件(c’)比(c)更强，只要满足了(c’)，原条件(c)也自然满足。 核心论证 (Proof of Claim): 假设我们当前的集合 $\\mathcal{H}$ 已经满足(b)和(c’)，但不满足(a)（即不够大）。我们要证明，此时一定能在图中找到一个“好”的 $C_4$ 添加到 $\\mathcal{H}$ 中，使得新集合仍然满足(b)和(c’)。这个论证分两步计数： 第一步：寻找大量“非饱和”的红心路径。我们先从 $G$ 中移除所有在 $\\mathcal{H}$ 中已经“饱和”的边（即度数达到(b)中上界的边），得到新图 $G’$。由于 $\\mathcal{H}$ 不够大，被移除的边数不多，$G’$ 仍然很稠密。在 $G’$ 中，我们寻找以某个顶点 $v$ 为中心（我们将 $v$ 染成红色）的长度为2的路径。一条路径如果已经参与了太多红心 $C_4$（达到(c’)中上界），就称之为“饱和路径”。通过简单的计数可以发现，饱和路径的总数并不多。因此，利用凸性 (convexity)（即詹森不等式），我们可以证明 $G’$ 中存在大量的非饱和红心路径：$ \\frac{1}{2} \\sum_{v \\in V(G’)} d(v)(d(v) - 3\\delta k \\sqrt{n}) &gt; \\frac{1}{3n} \\left(\\sum_{v \\in V(G’)} d(v)\\right)^2 &gt; \\frac{k^2 n^2}{4} $这里 $3\\delta k \\sqrt{n}$ 是从饱和路径计数中得到的损失项。这保证了我们有至少 $\\frac{k^4 n^2}{4}$ 这么多非饱和的红心路径。 第二步：将这些路径配对成“好”的 $C_4$。现在，我们固定两个顶点 $u, w$（我们将它们染成蓝色），并考虑所有连接它们的非饱和红心路径。假设我们已经有了一条路径 $u-v-w$。我们要找另一条路径 $u-v’-w$ 与之配对形成一个 $C_4$。什么样的 $v’$ 是“坏”的？如果 $v’$ 导致新生成的 $C_4$ 违反了条件(c’)（即路径 $v-u-v’$ 或 $v-w-v’$ 是蓝心饱和的），那么它就是坏的。通过(b)中的上界，我们可以估算出“坏”的 $v’$ 的数量最多不超过 $3\\delta k^2$ 个。我们有大约 $\\frac{k^4}{4}$ 个路径连接任意一对 $\\{u, w\\}$。从中选择两个不同的路径来构成 $C_4$。同样，利用凸性，我们可以得到“好”的 $C_4$ 的数量下界：$ \\frac{1}{2} \\binom{n}{2} \\binom{\\text{平均公共邻居数}}{\\text{2}} \\approx \\frac{1}{2} \\binom{n}{2} \\frac{k^4}{4} \\left(\\frac{k^4}{4} - 3\\delta k^2\\right) &gt; 2\\delta k^4 n^2 $这个数量远大于已经在 $\\mathcal{H}$ 中的 $C_4$ 数量，所以一定存在新的、“好”的 $C_4$ 可以被添加。 这样，通过迭代添加，最终可以得到满足所有条件的集合 $\\mathcal{H}$。 3. $C_4$-free图的容器定理 (Theorem 49)这是利用Theorem 48作为工具，通过容器方法得到的最终结构性定理。 Theorem 49. 存在常数 $k_0 &gt; 0, C &gt; 0$。对于所有 $n \\in \\mathbb{N}$ 和满足 $k_0 \\le k \\le n^{1/6}/\\log n$ 的 $k$，存在一个图的集合 $\\mathcal{G}(n,k)$，它有如下性质： $|\\mathcal{G}(n,k)| \\le \\exp\\left( C \\frac{\\log k}{k} \\cdot n^{3/2} \\right)$ (集合很小) 对每个 $G \\in \\mathcal{G}(n,k)$，有 $e(G) \\le k n^{3/2}$ (个体有界) 每个不含 $C_4$ 的图都是某个 $G \\in \\mathcal{G}(n,k)$ 的子图 (容器性) 证明思路 (Proof of Theorem 49): 这个证明是本文中迭代思想的集中体现。 迭代框架: 我们从 $\\mathcal{C}_0 = \\{K_n\\}$ 开始。 在第 $t$ 步，我们检查 $\\mathcal{C}_t$ 中的每个图 $G$。如果 $e(G)$ 仍然很大（大于某个阈值），我们就对它进行“打碎”操作。 这个阈值是动态变化的，由 $k(i) = \\max\\{(1-\\varepsilon)^i\\sqrt{n}, k_0\\}$ 决定。 “打碎”一个容器 $G$ 的过程: 假设 $G$ 有 $k’n^{3/2}$ 条边。我们应用 Theorem 48，在 $G$ 中找到一个平衡的 $C_4$ 超图 $\\mathcal{H}$。 然后对这个性质良好的 $\\mathcal{H}$ 应用超图容器引理。关键是选择合适的参数 $\\tau$。如上一问的分析，我们选择 $\\tau = \\max\\{\\frac{\\delta}{(k’)^2}, \\frac{1}{k’n^{1/6}}\\}$ 来满足引理的度数条件。 容器引理会生成一族新的、更小的容器（它们是 $G$ 的子图）。这一步生成的容器数量（即成本）由 $\\tau$ 和 $v(\\mathcal{H})$ 决定，其数量的对数（即指数部分）大致为：$ \\text{成本(指数)} \\approx n^{3/2} \\varepsilon \\cdot \\max\\left\\{ \\frac{\\log k’}{k’}, \\frac{\\log n}{n^{1/6}} \\right\\} $ 累加总成本: 总容器数是每一步“打碎”操作产生的容器数的乘积，所以总成本是所有指数的和。 总指数 $\\approx \\sum_{i=1}^{m} \\left( n^{3/2} \\varepsilon \\cdot \\max\\left\\{ \\frac{\\log k(i)}{k(i)}, \\frac{\\log n}{n^{1/6}} \\right\\} \\right)$ 由于 $k(i)$ 指数级下降，而函数 $\\frac{\\log x}{x}$ 在 $x$ 较大时递减，所以求和项 $\\frac{\\log k(i)}{k(i)}$ 是递增的。整个和由最后几项（当 $k(i)$ 接近常数 $k_0$ 时）主导。 定理中 $k \\le n^{1/6}/\\log n$ 的条件保证了 $max$ 函数中总是第一项 $\\frac{\\log k(i)}{k(i)}$ 更大。 经过仔细的计算，这个和的量级最终由 $O(\\frac{\\log k}{k})$ 决定，所以总容器数上界为 $\\exp\\left( C \\frac{\\log k}{k} n^{3/2} \\right)$。 4. 应用利用 Theorem 49，我们可以推导出关于随机图中 $C_4$-free 子图的结论。 Theorem 50. 对于 $p \\ge n^{-1/3}(\\log n)^3$，随机图 $G(n,p)$ 中最大的无 $C_4$ 子图的边数 $ex(G(n,p), C_4)$ 以高概率不超过 $p^{1/2} n^{3/2} \\log n$。 证明思路: 假设 $G(n,p)$ 中存在一个很大的无 $C_4$ 子图 $H$，其边数为 $m$。 根据 Theorem 49，这个 $H$ 必然被包含在我们的某个容器 $G \\in \\mathcal{G}(n,k)$ 中。 我们使用Union Bound：$P(\\text{存在这样的} H) \\le \\sum_{G \\in \\mathcal{G}(n,k)} P(G \\text{ 包含 } H \\text{ 这样大的子图})$。 对于单个容器 $G$，它在 $G(n,p)$ 中包含 $m$ 条边的概率极小，可以用 Chernoff 界得到，这个概率约为 $\\exp(-\\Omega(pn^2))$。 将这个极小的概率乘以容器的总数 $|\\mathcal{G}(n,k)|$。通过精细地选择参数 $k$ (使其满足 $\\frac{\\log k}{k^2} \\approx p$)，可以证明最终的乘积依然趋向于0。 Theorem 51. 这个定理给出了 $ex(G(n,p), C_4)$ 在不同 $p$ 的取值范围下更精确的量级。其证明更为复杂，讲义中省略了。 Conjecture 52.最后，作者提出了一个关于一般二部图 $H$ 的“平衡超饱和猜想”，并指出如果该猜想成立，将能证明不含 $H$ 的图的数量为 $2^{O(ex(n,H))}$。这为未来的研究指明了方向。","link":"/Com-Pro/hc3.html"},{"title":"Com_Pro","text":"","link":"/Com-Pro/index.html"},{"title":"Chapter1_MATH1408","text":"多项式的基本概念1.1 多项式的定义 设 $F$ 是一个数域 (Field) (例如实数域 $\\mathbb{R}$ 或复数域 $\\mathbb{C}$)。 一个以 $x$ 为不定元 (indeterminate) 的 $F$ 上的多项式 (polynomial) 是形如：$f(x) = a_n x^n + a_{n-1} x^{n-1} + \\dots + a_1 x + a_0$的表达式，其中 $0 \\le n &lt; \\infty$ 是一个非负整数，$a_i \\in F$ 称为多项式的系数 (coefficients)。 如果 $a_n \\ne 0$，则称 $n$ 为多项式 $f(x)$ 的次数 (degree)，记作 $\\deg f(x) = n$。 $a_n$ 称为首项系数 (leading coefficient)。 $a_n x^n$ 称为首项 (leading term)。 $a_0$ 称为常数项 (constant term)。 零多项式 (Zero Polynomial): 所有系数都为 0 的多项式，记作 $0$。其次数通常定义为 $-\\infty$ 或不定义。 常数多项式 (Constant Polynomial): $f(x) = a_0$，其中 $a_0 \\ne 0$。其次数为 0。 $F$ 上所有以 $x$ 为不定元的多项式的集合记作 $F[x]$。 1.2 环 (Ring) 的概念 (PPT中提及的代数结构) 一个集合 $R$ 配备两个二元运算 “+” (加法) 和 “•” (乘法)，称为一个环 (Ring)，如果满足： $(R, +)$ 是一个阿贝尔群 (Abelian group)： 加法结合律: $(a+b)+c = a+(b+c)$ 加法交换律: $a+b = b+a$ 存在加法单位元 (零元) $0_R$: $a+0_R = a$ 对任意 $a \\in R$，存在加法逆元 $-a$: $a+(-a) = 0_R$ $(R, \\cdot)$ 满足乘法结合律: $(a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)$ 乘法对加法分配律成立： $a \\cdot (b+c) = a \\cdot b + a \\cdot c$ $(b+c) \\cdot a = b \\cdot a + c \\cdot a$ 如果乘法还满足交换律 ($a \\cdot b = b \\cdot a$)，则称 $R$ 是交换环 (Commutative Ring)。 如果存在乘法单位元 $1_R$ ($a \\cdot 1_R = 1_R \\cdot a = a$)，则称 $R$ 是含幺环 (Ring with Unity) 或单位环 (Unit Ring)。 一个非零交换含幺环 $R$，如果其每个非零元都有乘法逆元，则称 $R$ 是一个域 (Field)。 $F[x]$ (系数在域 $F$ 上的多项式集合) 关于通常的多项式加法和乘法构成一个交换含幺环，称为多项式环 (Polynomial Ring)。 1.3 多项式的运算设 $f(x) = a_n x^n + \\dots + a_0$ 和 $g(x) = b_m x^m + \\dots + b_0$ 是 $F[x]$ 中的两个多项式。不妨设 $n \\ge m$，并令 $b_k = 0$ 当 $k &gt; m$ 时。 加法 (Addition):$f(x) + g(x) = (a_n+b_n)x^n + (a_{n-1}+b_{n-1})x^{n-1} + \\dots + (a_0+b_0)$ 减法 (Subtraction):$f(x) - g(x) = (a_n-b_n)x^n + (a_{n-1}-b_{n-1})x^{n-1} + \\dots + (a_0-b_0)$ 标量乘法 (Scalar Multiplication): 若 $k \\in F$，$k f(x) = (ka_n)x^n + (ka_{n-1})x^{n-1} + \\dots + (ka_0)$ $F[x]$ 关于多项式加法和标量乘法构成数域 $F$ 上的一个向量空间 (Vector Space)。 乘法 (Multiplication):$f(x) \\cdot g(x) = c_{m+n} x^{m+n} + c_{m+n-1} x^{m+n-1} + \\dots + c_1 x + c_0$其中 $c_k = \\sum_{i+j=k} a_i b_j$ (柯西乘积)。 乘法满足结合律和交换律。 乘法对加法满足分配律。 $1$ (常数多项式) 是乘法单位元。 1.4 多项式的次数性质设 $f(x), g(x) \\in F[x]$ 且它们都不是零多项式。 $\\deg(f(x) g(x)) = \\deg f(x) + \\deg g(x)$ 证明: 设 $\\deg f(x) = n$ (首项 $a_n x^n, a_n \\ne 0$)，$\\deg g(x) = m$ (首项 $b_m x^m, b_m \\ne 0$)。则 $f(x)g(x)$ 的首项是 $(a_n b_m) x^{n+m}$。因为 $F$ 是一个域，所以 $a_n b_m \\ne 0$。因此 $\\deg(f(x)g(x)) = n+m = \\deg f(x) + \\deg g(x)$。□ 若 $k \\in F, k \\ne 0$，则 $\\deg(k f(x)) = \\deg f(x)$。 $\\deg(f(x) \\pm g(x)) \\le \\max\\{\\deg f(x), \\deg g(x)\\}$。 等号成立的条件是 $\\deg f(x) \\ne \\deg g(x)$，或者 $\\deg f(x) = \\deg g(x)$ 但它们的最高次项系数之和（或差）不为零。 推论 (整环性): 如果 $F$ 是一个域，则 $F[x]$ 是一个整环 (Integral Domain)，即 $F[x]$ 中没有零因子 (若 $f(x)g(x)=0$，则 $f(x)=0$ 或 $g(x)=0$)。 证明: 若 $f(x) \\ne 0, g(x) \\ne 0$，则 $\\deg(f(x)g(x)) = \\deg f(x) + \\deg g(x) \\ge 0$。而零多项式的次数是 $-\\infty$。所以 $f(x)g(x) \\ne 0$。□ 1.5 多项式分式域 (Field of Rational Functions) (PPT中提及) 类似于从整数环 $\\mathbb{Z}$ 构造有理数域 $\\mathbb{Q}$，我们可以从多项式环 $F[x]$ 构造其分式域 (Field of Fractions)，称为有理函数域 (Field of Rational Functions)，记作 $F(x)$。 $F(x)$ 中的元素形如 $\\frac{f(x)}{g(x)}$，其中 $f(x), g(x) \\in F[x]$ 且 $g(x) \\ne 0$。 等价关系定义为：$\\frac{f_1(x)}{g_1(x)} \\sim \\frac{f_2(x)}{g_2(x)}$ 当且仅当 $f_1(x)g_2(x) = f_2(x)g_1(x)$。 $F(x)$ 关于通常的分式加法和乘法构成一个域。 2. 整除性 (Divisibility)2.1 定义 设 $f(x), g(x) \\in F[x]$。如果存在 $h(x) \\in F[x]$ 使得 $f(x) = g(x)h(x)$，则称 $g(x)$ 整除 (divides) $f(x)$，或者说 $g(x)$ 是 $f(x)$ 的一个因式 (factor/divisor)，或者说 $f(x)$ 是 $g(x)$ 的一个倍式 (multiple)。 记作 $g(x) | f(x)$。 如果 $g(x)$ 不整除 $f(x)$，记作 $g(x) \\nmid f(x)$。 平凡因式: 任何非零常数多项式和 $f(x)$ 的伴随式 (形如 $cf(x)$, $c \\in F, c \\ne 0$) 都是 $f(x)$ 的平凡因式。 真因式 (Proper Divisor): 如果 $g(x) | f(x)$ 且 $g(x)$ 不是 $f(x)$ 的伴随式也不是单位 (非零常数)，则称 $g(x)$ 是 $f(x)$ 的真因式。 2.2 整除性的基本性质设 $f(x), g(x), h(x) \\in F[x]$。 若 $f(x)|g(x)$，则对任意非零常数 $k \\in F$，$kf(x)|g(x)$ 且 $f(x)|kg(x)$。 $f(x)|f(x)$ (自反性)。 若 $f(x)|g(x)$ 且 $g(x)|h(x)$，则 $f(x)|h(x)$ (传递性)。 若 $f(x)|g(x)$ 且 $f(x)|h(x)$，则对任意 $u(x), v(x) \\in F[x]$，$f(x) | (u(x)g(x) + v(x)h(x))$。 证明: $g(x)=f(x)s(x)$, $h(x)=f(x)t(x)$。$u(x)g(x)+v(x)h(x) = u(x)f(x)s(x)+v(x)f(x)t(x) = f(x)(u(x)s(x)+v(x)t(x))$。所以 $f(x)|(u(x)g(x)+v(x)h(x))$。□ 若 $f(x)|g(x)$ 且 $g(x)|f(x)$，则存在非零常数 $c \\in F$ 使得 $f(x) = cg(x)$。称 $f(x)$ 和 $g(x)$ 是相伴的 (associated)。 证明: $f(x)=g(x)h_1(x)$，$g(x)=f(x)h_2(x)$。$f(x) = f(x)h_2(x)h_1(x)$。若 $f(x) \\ne 0$，则 $1 = h_1(x)h_2(x)$。这意味着 $h_1(x)$ 和 $h_2(x)$ 都是非零常数。□ 3. 带余除法 (Division Algorithm)3.1 定理 (带余除法 / Euclidean Division) 设 $f(x), g(x) \\in F[x]$ 且 $g(x) \\ne 0$。则存在唯一的多项式 $q(x)$ (商式, quotient) 和 $r(x)$ (余式, remainder) 使得：$f(x) = q(x)g(x) + r(x)$并且 $\\deg r(x) &lt; \\deg g(x)$ (或者 $r(x)=0$，此时 $\\deg r(x) = -\\infty &lt; \\deg g(x)$ 只要 $\\deg g(x) \\ge 0$)。 3.2 存在性证明 (构造性/归纳法) 情况1: 若 $\\deg f(x) &lt; \\deg g(x)$ (或 $f(x)=0$)。取 $q(x)=0, r(x)=f(x)$。显然 $\\deg r(x) &lt; \\deg g(x)$。 情况2: 若 $\\deg f(x) \\ge \\deg g(x)$。设 $f(x) = a_n x^n + \\dots + a_0$ ($a_n \\ne 0$)，$g(x) = b_m x^m + \\dots + b_0$ ($b_m \\ne 0$)，且 $n \\ge m$。构造 $f_1(x) = f(x) - \\frac{a_n}{b_m} x^{n-m} g(x)$。则 $f_1(x)$ 的 $x^n$ 项的系数为 $a_n - \\frac{a_n}{b_m} b_m = 0$。所以 $\\deg f_1(x) &lt; n$。对 $f_1(x)$ 和 $g(x)$ 应用归纳假设 (或重复此过程)。如果 $\\deg f_1(x) &lt; \\deg g(x)$，则令 $q_1(x) = \\frac{a_n}{b_m} x^{n-m}$，$r(x)=f_1(x)$。$f(x) = q_1(x)g(x) + r(x)$。如果 $\\deg f_1(x) \\ge \\deg g(x)$，根据归纳假设，存在 $q_2(x), r(x)$ 使得$f_1(x) = q_2(x)g(x) + r(x)$ 且 $\\deg r(x) &lt; \\deg g(x)$。则 $f(x) = \\frac{a_n}{b_m} x^{n-m} g(x) + q_2(x)g(x) + r(x) = (\\frac{a_n}{b_m} x^{n-m} + q_2(x))g(x) + r(x)$。令 $q(x) = \\frac{a_n}{b_m} x^{n-m} + q_2(x)$。这个过程最终会停止，因为每次 $f_i(x)$ 的次数都严格降低。□(PPT中给出了一个例子：$3x^4 - 4x^3 + 5x - 1 = (3x^2 - x - 4)(x^2 - x + 1) + (2x + 3)$) 3.3 唯一性证明 假设存在两组这样的商和余式：$f(x) = q_1(x)g(x) + r_1(x)$，其中 $\\deg r_1(x) &lt; \\deg g(x)$$f(x) = q_2(x)g(x) + r_2(x)$，其中 $\\deg r_2(x) &lt; \\deg g(x)$ 则 $(q_1(x) - q_2(x))g(x) = r_2(x) - r_1(x)$。 如果 $q_1(x) \\ne q_2(x)$，则 $q_1(x) - q_2(x) \\ne 0$。那么 $\\deg((q_1(x) - q_2(x))g(x)) = \\deg(q_1(x) - q_2(x)) + \\deg g(x) \\ge \\deg g(x)$。而 $\\deg(r_2(x) - r_1(x)) \\le \\max\\{\\deg r_2(x), \\deg r_1(x)\\} &lt; \\deg g(x)$。 这就产生了矛盾 ($\\ge \\deg g(x)$ 和 $&lt; \\deg g(x)$ 不能同时成立)。 因此，必然有 $q_1(x) - q_2(x) = 0$，即 $q_1(x) = q_2(x)$。 从而 $r_2(x) - r_1(x) = 0$，即 $r_1(x) = r_2(x)$。□ 4. 最大公因式 (Greatest Common Divisor, GCD)4.1 定义 设 $f(x), g(x) \\in F[x]$。如果 $F[x]$ 中的多项式 $h(x)$ 满足： $h(x) | f(x)$ 且 $h(x) | g(x)$ (即 $h(x)$ 是 $f(x)$ 和 $g(x)$ 的一个公因式 (common divisor))。 称 $f(x)$ 和 $g(x)$ 的公因式中次数最高的首一多项式 (monic polynomial) 为 $f(x)$ 和 $g(x)$ 的最大公因式 (greatest common divisor, GCD)，记作 $d(x) = (f(x), g(x))$ 或 $\\text{gcd}(f(x), g(x))$。 首一多项式是指首项系数为 1 的多项式。 如果 $f(x)=g(x)=0$，则 GCD 定义为 0。如果只有一个是0，比如 $f(x) \\ne 0, g(x)=0$，则 GCD 是 $f(x)$ 的首一伴随式。 等价定义: $d(x)$ 是 $f(x), g(x)$ 的 GCD，如果 $d(x)$ 是首一的。 $d(x)|f(x)$ 且 $d(x)|g(x)$。 若任意 $h(x)|f(x)$ 且 $h(x)|g(x)$，则 $h(x)|d(x)$。 4.2 欧几里得算法 (Euclidean Algorithm) 用于计算两个多项式 $f(x)$ 和 $g(x)$ (不妨设 $g(x) \\ne 0$) 的 GCD。 基于性质：$(f(x), g(x)) = (g(x), r(x))$，其中 $f(x) = q(x)g(x) + r(x)$。 证明:设 $d_1(x) = (f(x),g(x))$，$d_2(x) = (g(x),r(x))$。因为 $d_1|f$ 且 $d_1|g$，所以 $d_1|(f-qg)$，即 $d_1|r$。因此 $d_1$ 是 $g,r$ 的公因式，所以 $d_1|d_2$。因为 $d_2|g$ 且 $d_2|r$，所以 $d_2|(qg+r)$，即 $d_2|f$。因此 $d_2$ 是 $f,g$ 的公因式，所以 $d_2|d_1$。由于 $d_1,d_2$ 都是首一的且互相整除，所以 $d_1=d_2$。□ 算法步骤: $f(x) = q_1(x)g(x) + r_1(x)$, $\\deg r_1(x) &lt; \\deg g(x)$ $g(x) = q_2(x)r_1(x) + r_2(x)$, $\\deg r_2(x) &lt; \\deg r_1(x)$ $r_1(x) = q_3(x)r_2(x) + r_3(x)$, $\\deg r_3(x) &lt; \\deg r_2(x)$ … $r_{s-1}(x) = q_{s+1}(x)r_s(x) + r_{s+1}(x)$, $\\deg r_{s+1}(x) &lt; \\deg r_s(x)$ $r_s(x) = q_{s+2}(x)r_{s+1}(x) + 0$ (余式为0) 则最后一个非零余式 $r_{s+1}(x)$ 的首一伴随式就是 $(f(x), g(x))$。即 $c r_{s+1}(x)$ 是 GCD，其中 $c$ 是使 $cr_{s+1}(x)$ 首一的常数。$(f,g) = (g,r_1) = (r_1,r_2) = \\dots = (r_s, r_{s+1}) = (r_{s+1}, 0)$。$r_{s+1}(x)$ 的首一伴随式就是 $(f(x),g(x))$。 4.3 扩展欧几里得算法 (Extended Euclidean Algorithm) 与 Bézout 恒等式 定理 (Bézout’s Identity for Polynomials): 对任意 $f(x), g(x) \\in F[x]$，存在 $u(x), v(x) \\in F[x]$ 使得：$d(x) = (f(x), g(x)) = u(x)f(x) + v(x)g(x)$ $u(x), v(x)$ 可以通过欧几里得算法反向代换得到。 推论: 任意 $f(x), g(x)$ 的公因式都是其最大公因式 $d(x)$ 的因式。 证明: 若 $h(x)|f(x)$ 且 $h(x)|g(x)$，则 $h(x)|(u(x)f(x)+v(x)g(x))$，即 $h(x)|d(x)$。□ (PPT中例子: $(x^4 - x^3 - x^2 + 2x - 1, x^3 - 2x + 1) = x - 1$。且 $u(x) = x+1, v(x) = -x^2$ 使得 $(x+1)(x^4 - x^3 - x^2 + 2x - 1) + (-x^2)(x^3 - 2x + 1) = x-1$。) 4.4 GCD 的另一种刻画 定理: 对任意非零多项式 $f(x), g(x) \\in F[x]$，其最大公因式 $d(x)$ 是集合$S = \\{ h(x) \\mid h(x) = u(x)f(x) + v(x)g(x), u(x),v(x) \\in F[x], \\deg h(x) \\ge 0 \\}$中次数最低的首一多项式。 证明思路: 设 $h_0(x)$ 是 $S$ 中次数最低的非零多项式 (可以将其化为首一)。 证明 $h_0(x)|f(x)$ 和 $h_0(x)|g(x)$。用带余除法：$f(x) = q(x)h_0(x) + r(x)$。$r(x) = f(x) - q(x)h_0(x) = f(x) - q(x)(u_0(x)f(x)+v_0(x)g(x)) = (1-q(x)u_0(x))f(x) - q(x)v_0(x)g(x)$。所以 $r(x) \\in S$。由于 $\\deg r(x) &lt; \\deg h_0(x)$ 且 $h_0(x)$ 是 $S$ 中次数最低的非零多项式，所以 $r(x)=0$。因此 $h_0(x)|f(x)$。同理 $h_0(x)|g(x)$。 证明 $h_0(x)$ 是次数最高的。若 $d’(x)$ 是任意公因式，则 $d’(x)|(u(x)f(x)+v(x)g(x))$，所以 $d’(x)|h_0(x)$。因此 $\\deg d’(x) \\le \\deg h_0(x)$。□ 多个多项式的 GCD 可以递归定义：$(f_1(x), f_2(x), \\dots, f_k(x)) = ((f_1(x), f_2(x)), \\dots, f_k(x)) = (f_1(x), (f_2(x), \\dots, f_k(x)))$。 5. 互素多项式 (Relatively Prime Polynomials)5.1 定义 如果 $(f(x), g(x)) = 1$ (常数多项式1)，则称 $f(x)$ 和 $g(x)$ 互素 (relatively prime / coprime)。 定理: $f(x), g(x)$ 互素当且仅当存在 $u(x), v(x) \\in F[x]$ 使得 $u(x)f(x) + v(x)g(x) = 1$。(这是 Bézout 恒等式的直接推论) 5.2 互素的性质 欧几里得引理的推广: 若 $f_1(x)|g(x)$，$f_2(x)|g(x)$ 且 $(f_1(x), f_2(x))=1$，则 $f_1(x)f_2(x)|g(x)$。 证明: $u(x)f_1(x) + v(x)f_2(x) = 1$。$g(x) = g(x)(u(x)f_1(x) + v(x)f_2(x)) = g(x)u(x)f_1(x) + g(x)v(x)f_2(x)$。因为 $f_2(x)|g(x)$，所以 $g(x)=s(x)f_2(x)$。代入第一项：$s(x)f_2(x)u(x)f_1(x)$，可被 $f_1(x)f_2(x)$ 整除。因为 $f_1(x)|g(x)$，所以 $g(x)=t(x)f_1(x)$。代入第二项：$t(x)f_1(x)v(x)f_2(x)$，可被 $f_1(x)f_2(x)$ 整除。所以 $f_1(x)f_2(x) | g(x)$。□ 若 $(f(x), g(x))=1$ 且 $f(x)|g(x)h(x)$，则 $f(x)|h(x)$。 证明: $u(x)f(x) + v(x)g(x) = 1$。$h(x) = h(x)(u(x)f(x) + v(x)g(x)) = h(x)u(x)f(x) + g(x)h(x)v(x)$。第一项显然被 $f(x)$ 整除。第二项 $g(x)h(x)v(x)$ 因为 $f(x)|g(x)h(x)$，所以也被 $f(x)$ 整除。因此 $f(x)|h(x)$。□ 若 $(f(x), g(x)) = d(x)$，令 $f(x) = f_1(x)d(x)$，$g(x) = g_1(x)d(x)$，则 $(f_1(x), g_1(x)) = 1$。 证明: $u(x)f(x) + v(x)g(x) = d(x)$。$u(x)f_1(x)d(x) + v(x)g_1(x)d(x) = d(x)$。若 $d(x) \\ne 0$，两边除以 $d(x)$，得 $u(x)f_1(x) + v(x)g_1(x) = 1$。所以 $(f_1(x), g_1(x))=1$。□ 若 $(f(x), g(x)) = d(x)$，则对任意非零多项式 $h(x)$，$(h(x)f(x), h(x)g(x)) = c \\cdot h(x)d(x)$ (其中 $c$ 是使结果首一的常数)。通常写为 $(h(x)f(x), h(x)g(x)) = \\text{monic}(h(x)d(x))$。 证明思路:$u(x)f(x) + v(x)g(x) = d(x)$。两边乘以 $h(x)$: $u(x)(h(x)f(x)) + v(x)(h(x)g(x)) = h(x)d(x)$。这表明 $h(x)d(x)$ 是 $h(x)f(x)$ 和 $h(x)g(x)$ 的一个线性组合，因此任何 $(h(x)f(x), h(x)g(x))$ 的公因式都必须整除 $h(x)d(x)$。另一方面，$h(x)d(x)$ 显然是 $h(x)f(x)$ 和 $h(x)g(x)$ 的公因式。所以 $h(x)d(x)$ 的首一伴随式就是 GCD。□ 若 $(f_1(x), g(x))=1$ 且 $(f_2(x), g(x))=1$，则 $(f_1(x)f_2(x), g(x))=1$。 证明:$u_1(x)f_1(x) + v_1(x)g(x) = 1$ (1)$u_2(x)f_2(x) + v_2(x)g(x) = 1$ (2)将 (1) 和 (2) 相乘：$(u_1(x)f_1(x) + v_1(x)g(x))(u_2(x)f_2(x) + v_2(x)g(x)) = 1 \\cdot 1 = 1$展开左边：$u_1(x)u_2(x)f_1(x)f_2(x) + u_1(x)f_1(x)v_2(x)g(x) + v_1(x)g(x)u_2(x)f_2(x) + v_1(x)v_2(x)g(x)g(x) = 1$$u_1(x)u_2(x)(f_1(x)f_2(x)) + (u_1(x)f_1(x)v_2(x) + v_1(x)u_2(x)f_2(x) + v_1(x)v_2(x)g(x))g(x) = 1$这是 $f_1(x)f_2(x)$ 和 $g(x)$ 的一个等于1的线性组合，所以它们互素。□ 6. 不可约多项式与唯一因子分解6.1 不可约多项式 (Irreducible Polynomial) 设 $f(x)$ 是数域 $F$ 上的一个非常数多项式 (即 $\\deg f(x) \\ge 1$)。 如果 $f(x)$ 不能表示为 $F$ 上两个次数都低于 $\\deg f(x)$ 的多项式的乘积，则称 $f(x)$ 在数域 $F$ 上是不可约的 (irreducible over F) 或称其为 $F$ 上的不可约多项式。 否则，称 $f(x)$ 在 $F$ 上是可约的 (reducible over F)。 注意: 不可约性依赖于系数所在的数域 $F$。 例如，$x^2-2$ 在 $\\mathbb{Q}$ (有理数域) 上不可约，但在 $\\mathbb{R}$ (实数域) 上可约，因为 $x^2-2 = (x-\\sqrt{2})(x+\\sqrt{2})$。 $x^2+1$ 在 $\\mathbb{R}$ 上不可约，但在 $\\mathbb{C}$ (复数域) 上可约，因为 $x^2+1 = (x-i)(x+i)$。 如果一个多项式是不可约的，它的因式只有常数和它自身的伴随式。 6.2 不可约多项式的性质 (类比素数) 引理 (Euclid’s Lemma for Polynomials): 设 $p(x)$ 是 $F[x]$ 中的不可约多项式。若 $p(x) | f(x)g(x)$，其中 $f(x), g(x) \\in F[x]$，则 $p(x)|f(x)$ 或 $p(x)|g(x)$。 证明:假设 $p(x) \\nmid f(x)$。因为 $p(x)$ 不可约，所以 $(p(x), f(x))$ 只能是 $1$ (首一常数) 或 $p(x)$ 的首一伴随式。由于 $p(x) \\nmid f(x)$，所以 $(p(x), f(x)) = 1$。根据互素的性质 (若 $(a,b)=1$ 且 $a|bc$，则 $a|c$)，因为 $p(x)|f(x)g(x)$ 且 $(p(x),f(x))=1$，所以 $p(x)|g(x)$。□ 推广: 若 $p(x)$ 是 $F[x]$ 中的不可约多项式，且 $p(x) | f_1(x)f_2(x)\\dots f_s(x)$，则存在某个 $i$ 使得 $p(x)|f_i(x)$。 注意: 次数为0 (非零常数) 或次数为1的多项式通常不被认为是不可约的 (定义要求非常数)。但有时为了叙述方便，1次多项式在某些上下文中可以视为不可约的，因为它不能再分解为次数更低的多项式乘积。严格定义依赖于教科书。通常，不可约多项式的次数 $\\ge 1$。 6.3 唯一因子分解定理 (Unique Factorization Theorem for Polynomials) 定理: 数域 $F$ 上的任一个非常数多项式 $f(x) \\in F[x]$ 都可以唯一地 (不计因式顺序和常数因子的不同) 分解为 $F$ 上有限个首一不可约多项式的乘积。即，对任意首项系数为 $a_n$ 的 $f(x)$，存在唯一的分解：$f(x) = a_n p_1(x) p_2(x) \\dots p_s(x)$其中 $p_i(x)$ 是 $F$ 上的首一不可约多项式 (它们之间可能相同)。 如果将相同的不可约因式合并，则可以写成标准分解式：$f(x) = a_n [p_1(x)]^{\\alpha_1} [p_2(x)]^{\\alpha_2} \\dots [p_k(x)]^{\\alpha_k}$其中 $p_i(x)$ 是互不相伴的首一不可约多项式，$\\alpha_i \\ge 1$ 是整数。 存在性证明 (对次数归纳): $\\deg f(x) = 1$: $f(x) = ax+b = a(x+b/a)$。$x+b/a$ 是首一不可约的。 假设对所有次数 $&lt;\\deg f(x)$ 的多项式结论成立。 若 $f(x)$ 不可约，将其化为首一形式 $a_n p(x)$ 即可。 若 $f(x)$ 可约，则 $f(x) = g(x)h(x)$，其中 $\\deg g(x) &lt; \\deg f(x)$ 且 $\\deg h(x) &lt; \\deg f(x)$。根据归纳假设，$g(x)$ 和 $h(x)$ 都可以分解为首一不可约多项式的乘积。将这些乘积合并即可得到 $f(x)$ 的分解。 唯一性证明 (利用Euclid引理): 假设 $f(x) = a_n p_1 \\dots p_s = a_n q_1 \\dots q_t$ 是两种分解。 $p_1 | (q_1 \\dots q_t)$。由于 $p_1$ 不可约，根据Euclid引理的推广，$p_1$ 必须整除某个 $q_j$。 由于 $q_j$ 也是不可约且首一的，所以 $p_1 = q_j$ (不计顺序，可重排 $q_k$ 使 $q_1=p_1$)。 两边消去 $p_1$ (和 $q_1$)，得到 $p_2 \\dots p_s = q_2 \\dots q_t$。 重复此过程，直到所有因式都被匹配。 重因式 (Multiple Factor): 在标准分解式中，如果某个不可约因式 $p_i(x)$ 的指数 $\\alpha_i &gt; 1$，则称 $p_i(x)$ 是 $f(x)$ 的一个$\\alpha_i$重因式 (factor of multiplicity $\\alpha_i$)。 7. 多项式的导数 (Derivative of a Polynomial)7.1 定义 设 $f(x) = a_n x^n + a_{n-1} x^{n-1} + \\dots + a_1 x + a_0 \\in F[x]$。 $f(x)$ 的导数 (derivative) 定义为：$f’(x) = \\frac{df}{dx} = n a_n x^{n-1} + (n-1)a_{n-1} x^{n-2} + \\dots + 2a_2 x + a_1$(常数项 $a_0$ 的导数为0)。 这是一个形式上的定义，不依赖于微积分中的极限概念，在任何数域 $F$ 上都有定义。(注意：如果数域 $F$ 的特征 $p &gt; 0$，则 $p \\cdot a = 0$。例如，在 $F_2$ (特征为2的域) 中，$x^2$ 的导数是 $2x = 0$)。 7.2 导数的性质 $(f(x) + g(x))’ = f’(x) + g’(x)$ $(f(x)g(x))’ = f’(x)g(x) + f(x)g’(x)$ (乘法法则) $(cf(x))’ = cf’(x)$ (对 $c \\in F$) $(f(x)^m)’ = m f(x)^{m-1} f’(x)$ (链式法则的特例) 7.3 重因式与导数的关系 定理: 多项式 $f(x)$ 含有重因式当且仅当 $f(x)$ 与其导数 $f’(x)$ 不互素，即 $(f(x), f’(x)) \\ne 1$。 证明: ($\\Rightarrow$) 设 $f(x) = [p(x)]^m g(x)$，其中 $p(x)$ 是不可约因式，$m &gt; 1$，且 $p(x) \\nmid g(x)$。$f’(x) = m[p(x)]^{m-1}p’(x)g(x) + [p(x)]^m g’(x)$。由于 $m-1 \\ge 1$，所以 $[p(x)]^{m-1}$ 是 $f(x)$ 和 $f’(x)$ 的公因式。因此 $(f(x), f’(x))$ 至少是 $[p(x)]^{m-1}$ 的倍式，所以不为1。 ($\\Leftarrow$) 假设 $(f(x), f’(x)) = d(x) \\ne 1$。设 $p(x)$ 是 $d(x)$ 的一个不可约因式。则 $p(x)|f(x)$ 且 $p(x)|f’(x)$。由 $p(x)|f(x)$，可写 $f(x) = p(x)h(x)$。$f’(x) = p’(x)h(x) + p(x)h’(x)$。因为 $p(x)|f’(x)$，所以 $p(x)|(p’(x)h(x) + p(x)h’(x))$。由于 $p(x)|p(x)h’(x)$，所以必有 $p(x)|p’(x)h(x)$。如果 $F$ 的特征为0或不整除 $\\deg p(x)$，则 $p’(x) \\ne 0$ 且 $\\deg p’(x) &lt; \\deg p(x)$。由于 $p(x)$ 不可约，所以 $p(x) \\nmid p’(x)$。因此，根据Euclid引理，$p(x)|h(x)$。所以 $h(x) = p(x)k(x)$。代回 $f(x) = p(x)h(x) = [p(x)]^2 k(x)$。这意味着 $p(x)$ 是 $f(x)$ 的至少2重的因式，即 $f(x)$ 有重因式。(如果数域特征 $p&gt;0$ 且 $p|\\deg p(x)$，则 $p’(x)=0$，此时 $p(x)|f’(x)$ 自动成立，需要更细致的讨论，但结论通常仍然成立，除非 $f(x)$ 是 $x^p$ 的多项式。)□ 7.4 消除重因式 设 $d(x) = (f(x), f’(x))$。 则多项式 $f_0(x) = f(x)/d(x)$ 与 $f(x)$ 有相同的不可约因式，但 $f_0(x)$ 的所有不可约因式都是单重的 (即 $f_0(x)$ 无重因式)。 证明思路:设 $f(x) = a_n [p_1(x)]^{\\alpha_1} [p_2(x)]^{\\alpha_2} \\dots [p_s(x)]^{\\alpha_s}$ 是 $f(x)$ 的标准分解。则 $(f(x), f’(x)) = c \\cdot [p_1(x)]^{\\alpha_1-1} [p_2(x)]^{\\alpha_2-1} \\dots [p_s(x)]^{\\alpha_s-1}$ (首项系数为 $c$) (假设特征为0或与所有 $\\alpha_i$ 互素)。所以 $f(x) / (f(x),f’(x))$ (的伴随式) 就是 $a_n’ p_1(x) p_2(x) \\dots p_s(x)$，它没有重因式。这个过程可以用来获得一个与原多项式有相同根集但所有根都是单根的多项式。 8. 多项式的值与根 (Roots of Polynomials)8.1 多项式的值 (Value of a Polynomial) 设 $f(x) = a_n x^n + \\dots + a_0 \\in F[x]$。 对任意 $c \\in F$ (或 $F$ 的某个扩域中的元素)，$f(x)$ 在 $c$ 处的值 定义为：$f(c) = a_n c^n + a_{n-1} c^{n-1} + \\dots + a_1 c + a_0 \\in F$ (或扩域)。这相当于将不定元 $x$ 替换为具体的值 $c$。 8.2 根 (Root) / 零点 (Zero) 如果 $f(c) = 0$，则称 $c$ 是多项式 $f(x)$ 的一个根 (root) 或零点 (zero)。 余数定理 (Remainder Theorem): 设 $f(x) \\in F[x]$，$c \\in F$。则 $f(x)$ 除以 $(x-c)$ 的余式是 $f(c)$。即，存在 $q(x) \\in F[x]$ 使得 $f(x) = (x-c)q(x) + f(c)$。 证明: 根据带余除法，$f(x) = (x-c)q(x) + r(x)$，其中 $\\deg r(x) &lt; \\deg(x-c)=1$。所以 $r(x)$ 必须是一个常数，设为 $r_0$。$f(x) = (x-c)q(x) + r_0$。令 $x=c$，则 $f(c) = (c-c)q(c) + r_0 = 0 \\cdot q(c) + r_0 = r_0$。所以余式是 $f(c)$。□ 因式定理 (Factor Theorem): $c$ 是 $f(x)$ 的根当且仅当 $(x-c)$ 是 $f(x)$ 的因式。 证明:($\\Rightarrow$) 若 $c$ 是根，则 $f(c)=0$。由余数定理，$f(x)=(x-c)q(x)+0$，所以 $(x-c)|f(x)$。($\\Leftarrow$) 若 $(x-c)|f(x)$，则 $f(x)=(x-c)q(x)$。令 $x=c$，则 $f(c)=(c-c)q(c)=0$，所以 $c$ 是根。□ 8.3 根的个数与多项式次数 定理: 数域 $F$ 上的一个 $n$ 次多项式在 $F$ 中至多有 $n$ 个不同的根。 证明 (归纳法):若 $n=0$, $f(x)=a_0 \\ne 0$，没有根。若 $n=1$, $f(x)=ax+b$ ($a \\ne 0$)，有唯一的根 $x=-b/a$。假设对 $n-1$ 次多项式成立。若 $f(x)$ 没有根，则结论成立 ($0 \\le n$)。若 $f(x)$ 有一个根 $c_1$，则 $f(x)=(x-c_1)q(x)$，其中 $\\deg q(x) = n-1$。$f(x)$ 的任何其他根 $c \\ne c_1$ 必须是 $q(x)$ 的根 (因为 $f(c)=(c-c_1)q(c)=0 \\Rightarrow q(c)=0$)。根据归纳假设，$q(x)$ 至多有 $n-1$ 个根。所以 $f(x)$ 至多有 $1 + (n-1) = n$ 个根。□ 推论: 若 $f(x)$ 和 $g(x)$ 是数域 $F$ 上次数都不超过 $n$ 的多项式，且它们在 $F$ 中至少有 $n+1$ 个不同的点 $c_i$ 处取值相同 ($f(c_i)=g(c_i)$)，则 $f(x)=g(x)$ (作为多项式恒等)。 证明: 考虑 $h(x) = f(x)-g(x)$。$\\deg h(x) \\le n$。$h(c_i) = f(c_i)-g(c_i) = 0$ 对至少 $n+1$ 个 $c_i$ 成立。这意味着 $h(x)$ 有至少 $n+1$ 个根。但如果 $h(x)$ 非零，其次数 $\\le n$，则它至多有 $n$ 个根，矛盾。所以 $h(x)$ 必须是零多项式，即 $f(x)=g(x)$。□ 注意: 这个推论要求 $F$ 是一个域 (或至少是一个整环)。如果 $F$ 只是一个环且有零因子，结论不一定成立。例如，在 $Z_6[x]$ (系数在模6整数环上) 中，$x^2-5x$ 的根有 $0,1,2,3,4,5$ (6个根)，但次数是2。 8.4 重根 (Multiple Root) 如果 $(x-c)^k | f(x)$ 但 $(x-c)^{k+1} \\nmid f(x)$ (其中 $k \\ge 1$)，则称 $c$ 是 $f(x)$ 的一个 $k$ 重根 (root of multiplicity $k$)。 $k=1$ 时为单根 (simple root)。 $k&gt;1$ 时为重根。 定理: 数域 $F$ (特征为0或与所有重数互素) 上的 $n$ 次多项式在 $F$ (或其代数闭包) 中恰好有 $n$ 个根 (计重数)。 定理: $c$ 是 $f(x)$ 的 $k$ 重根 ($k \\ge 1$) 当且仅当$f(c) = f’(c) = \\dots = f^{(k-1)}(c) = 0$ 且 $f^{(k)}(c) \\ne 0$。(其中 $f^{(j)}(c)$ 是 $f(x)$ 的 $j$ 阶导数在 $c$ 处的值)。 证明思路:($\\Rightarrow$) 若 $c$ 是 $k$ 重根，则 $f(x)=(x-c)^k g(x)$ 且 $g(c) \\ne 0$。对 $f(x)$ 求导 $k-1$ 次，每次都会留下 $(x-c)$ 因子。$f^{(j)}(x)$ 的形式为 $(x-c)^{k-j} h_j(x) + \\text{terms divisible by } (x-c)^{k-j+1}$，其中 $h_j(c) \\ne 0$ (除非 $k-j=0$)。所以 $f^{(j)}(c)=0$ for $j=0, \\dots, k-1$。$f^{(k)}(x) = k! g(x) + (x-c) (\\dots)$。所以 $f^{(k)}(c) = k! g(c) \\ne 0$ (假设特征为0或不整除 $k!$)。($\\Leftarrow$) 用泰勒展开式在 $c$ 点展开 $f(x)$ (形式上的)：$f(x) = f(c) + f’(c)(x-c) + \\frac{f’’(c)}{2!}(x-c)^2 + \\dots + \\frac{f^{(k)}(c)}{k!}(x-c)^k + \\dots$若 $f(c)=\\dots=f^{(k-1)}(c)=0$ 且 $f^{(k)}(c) \\ne 0$，则$f(x) = \\frac{f^{(k)}(c)}{k!}(x-c)^k + \\frac{f^{(k+1)}(c)}{(k+1)!}(x-c)^{k+1} + \\dots$$f(x) = (x-c)^k [\\frac{f^{(k)}(c)}{k!} + \\frac{f^{(k+1)}(c)}{(k+1)!}(x-c) + \\dots]$令 $g(x) = [\\dots]$。则 $g(c) = \\frac{f^{(k)}(c)}{k!} \\ne 0$。所以 $c$ 是 $k$ 重根。□ 9. 复数域与实数域上的多项式9.1 代数基本定理 (Fundamental Theorem of Algebra) 定理: 任何复系数 (或实系数，因为实数是复数的子集) 的非常数多项式在复数域 $\\mathbb{C}$ 中至少有一个根。 证明: 通常需要复分析的工具 (如Liouville定理或最大模原理的推论)，或拓扑学方法。PPT中给出了一个基于 $|f(z)|$ 在 $|z| \\to \\infty$ 时趋于 $\\infty$，以及 $|f(z)|$ 在某点 $z_0$ 达到最小值的分析思路，然后论证若 $|f(z_0)| \\ne 0$，则可以找到 $z_0+h$ 使得 $|f(z_0+h)| &lt; |f(z_0)|$，从而导出矛盾。这个证明的核心在于展示多项式函数模的最小值必为0。 思路: $|f(z)| \\to \\infty$ as $|z| \\to \\infty$。这意味着存在一个足够大的闭圆盘 $D_R = \\{z \\mid |z| \\le R\\}$，使得 $|f(z)|$ 在 $D_R$ 外部的值都大于 $|f(0)|$。 由于 $D_R$ 是紧集且 $|f(z)|$ 是连续函数，所以 $|f(z)|$ 在 $D_R$ 上必达到其最小值，设在 $z_0 \\in D_R$ 处达到。这个最小值也是 $|f(z)|$ 在整个复平面上的最小值。 假设 $f(z_0) \\ne 0$。考虑 $f(z_0+h)$ 在 $h$ 较小时的泰勒展开（或直接代数展开）：$f(z_0+h) = f(z_0) + b_k h^k + b_{k+1} h^{k+1} + \\dots + b_n h^n$，其中 $b_k = \\frac{f^{(k)}(z_0)}{k!} \\ne 0$ 是第一个非零系数 (因为 $f$ 不是常数)。$\\frac{f(z_0+h)}{f(z_0)} = 1 + \\frac{b_k}{f(z_0)} h^k + O(h^{k+1}) = 1 + c_k h^k + \\dots$可以选择 $h$ (一个小的复数，其幅角使得 $c_k h^k$ 为负实数，例如 $h = \\delta (-\\frac{1}{c_k})^{1/k}$，其中 $\\delta$ 是小的正实数)，使得 $|1+c_k h^k + \\dots| &lt; 1$。这意味着 $|\\frac{f(z_0+h)}{f(z_0)}| &lt; 1$，即 $|f(z_0+h)| &lt; |f(z_0)|$。这与 $z_0$ 是最小值点矛盾。 因此，假设 $f(z_0) \\ne 0$ 是错误的。所以 $f(z_0)=0$。□ 9.2 推论 推论 1 (Vieta 定理的前提): 复系数 $n$ 次多项式在复数域 $\\mathbb{C}$ 中恰好有 $n$ 个根 (计重数)。 推论 2: 复数域 $\\mathbb{C}$ 上的不可约多项式都是一次多项式。 推论 3: 复数域 $\\mathbb{C}$ 上的任一多项式都可以唯一地分解为一次因式的乘积。 9.3 韦达定理 (Vieta’s Formulas) 设首一多项式 $f(x) = x^n + a_{n-1}x^{n-1} + \\dots + a_1 x + a_0 \\in F[x]$ 在 $F$ (或其扩域) 中有 $n$ 个根 $x_1, x_2, \\dots, x_n$ (计重数)。 则根与系数之间有如下关系： $\\sum_{1 \\le i_1 &lt; \\dots &lt; i_k \\le n} x_{i_1} x_{i_2} \\dots x_{i_k} = (-1)^k a_{n-k}$ (对 $k=1, \\dots, n$) 特别地： $x_1 + x_2 + \\dots + x_n = -a_{n-1}$ (所有根之和) $\\sum_{i&lt;j} x_i x_j = a_{n-2}$ (所有根两两乘积之和) … $x_1 x_2 \\dots x_n = (-1)^n a_0$ (所有根之积) 9.4 三次方程求根公式 (Cardano’s Formula) (PPT中提及) 对于一般三次方程 $x^3 + ax^2 + bx + c = 0$。 通过代换 $x = y - \\frac{a}{3}$，可以化为标准形式 (没有二次项)：$y^3 + py + q = 0$其中 $p = b - \\frac{a^2}{3}$，$q = c - \\frac{ab}{3} + \\frac{2a^3}{27}$。 解法思路: 设 $y = u+v$。代入方程：$(u+v)^3 + p(u+v) + q = 0$$u^3+v^3+3uv(u+v) + p(u+v) + q = 0$$u^3+v^3 + (3uv+p)(u+v) + q = 0$ 令 $3uv+p=0$，即 $uv = -\\frac{p}{3}$。则方程变为 $u^3+v^3+q=0$，即 $u^3+v^3 = -q$。 我们有方程组：$\\begin{cases} u^3 v^3 = (-\\frac{p}{3})^3 = -\\frac{p^3}{27} \\\\ u^3 + v^3 = -q \\end{cases}$ $u^3$ 和 $v^3$ 可以看作是一个二次方程 $t^2 - (u^3+v^3)t + u^3v^3 = 0$ 的两个根。即 $t^2 - (-q)t + (-\\frac{p^3}{27}) = 0 \\Rightarrow t^2 + qt - \\frac{p^3}{27} = 0$。 解此二次方程得到 $t = \\frac{-q \\pm \\sqrt{q^2 + 4(p^3/27)}}{2} = -\\frac{q}{2} \\pm \\sqrt{(\\frac{q}{2})^2 + (\\frac{p}{3})^3}$。 令 $\\Delta = (\\frac{q}{2})^2 + (\\frac{p}{3})^3$ (判别式的一部分)。$u^3 = -\\frac{q}{2} + \\sqrt{\\Delta}$$v^3 = -\\frac{q}{2} - \\sqrt{\\Delta}$ 取 $u$ 为 $-\\frac{q}{2} + \\sqrt{\\Delta}$ 的一个立方根，则 $v = -\\frac{p}{3u}$。$y_1 = u+v$$y_2 = \\omega u + \\omega^2 v$$y_3 = \\omega^2 u + \\omega v$其中 $\\omega = -\\frac{1}{2} + i\\frac{\\sqrt{3}}{2}$ 是单位复根。 判别式 $\\Delta$ 的作用: $\\Delta &gt; 0$: 一个实根，两个共轭复根。 $\\Delta = 0$: 至少两个相等的实根 (可能是三重实根)。 $\\Delta &lt; 0$: 三个不相等的实根 (Casus irreducibilis，此时 $\\sqrt{\\Delta}$ 是纯虚数，但最终结果是实数)。 9.5 四次方程求根公式 (Ferrari’s Method) (PPT中提及) 对于一般四次方程，可以先通过线性代换消去三次项，得到 $x^4 + ax^2 + bx + c = 0$。 解法思路 (Ferrari): 移项并配方：$x^4 + ax^2 = -bx - c$$(x^2 + \\frac{a}{2})^2 = -bx - c + (\\frac{a}{2})^2 = (\\frac{a}{2}-a)x^2 -bx + (\\frac{a}{2})^2 - c = -\\frac{a}{2}x^2 -bx + (\\frac{a^2}{4}-c)$ (这里PPT的推导可能略有不同，核心是引入一个参数使其变成完全平方)PPT中的方法是：$x^4 = -ax^2 - bx - c$两边加 $ux^2 + \\frac{u^2}{4}$ (PPT中可能写成 $2ux^2+u^2$ 然后调整)，目标是使右边也变成完全平方。$(x^2 + \\frac{u}{2})^2 = (u-a)x^2 - bx + (\\frac{u^2}{4}-c)$ (PPT的 $(x^2+u)^2 = (2u-a)x^2 -bx + (u^2-c)$ 更常见) 选择参数 $u$ 使得右边 $(u-a)x^2 - bx + (\\frac{u^2}{4}-c)$ 是关于 $x$ 的完全平方式。这要求其判别式为0：$(-b)^2 - 4(u-a)(\\frac{u^2}{4}-c) = 0$$b^2 - (u-a)(u^2-4c) = 0$这是一个关于 $u$ 的三次方程，称为预解三次方程 (resolvent cubic)。 解出这个三次方程的一个根 $u_0$。 代回 $(x^2 + \\frac{u_0}{2})^2 = (u_0-a)x^2 - bx + (\\frac{u_0^2}{4}-c)$。右边现在是 $(Kx+L)^2$ 的形式。 于是 $x^2 + \\frac{u_0}{2} = \\pm (Kx+L)$。得到两个关于 $x$ 的二次方程，分别求解即可得到四个根。 五次及更高次的方程一般没有根式解 (Abel-Ruffini 定理，由Galois理论阐明)。 9.6 实系数多项式的根 定理: 若 $f(x)$ 是实系数多项式，且复数 $z$ 是 $f(x)$ 的一个根，则其共轭复数 $\\bar{z}$ 也是 $f(x)$ 的一个根。 证明: $f(x) = a_n x^n + \\dots + a_0$，其中 $a_i \\in \\mathbb{R}$。若 $f(z) = a_n z^n + \\dots + a_0 = 0$。取共轭：$\\overline{f(z)} = \\overline{a_n z^n + \\dots + a_0} = \\overline{0} = 0$。$\\overline{a_n} \\overline{z^n} + \\dots + \\overline{a_0} = 0$。因为 $a_i \\in \\mathbb{R}$，所以 $\\overline{a_i}=a_i$。且 $\\overline{z^k} = (\\bar{z})^k$。$a_n (\\bar{z})^n + a_{n-1} (\\bar{z})^{n-1} + \\dots + a_0 = 0$。即 $f(\\bar{z}) = 0$。所以 $\\bar{z}$ 也是根。□ 推论: 实系数多项式的虚根成对出现。 推论: 实数域 $\\mathbb{R}$ 上的不可约多项式只能是一次或二次的。 一次：$ax+b$ 二次：$ax^2+bx+c$ 且判别式 $b^2-4ac &lt; 0$ (否则它在实数域有根，可分解为一次因式)。 推论: 任何实系数多项式都可以分解为实系数的一次因式和二次不可约因式的乘积。 9.7 实根的界 (Bounds for Real Roots) (PPT中提及一种) 定理 (Lagrange’s Bound - 类似思想): 设 $f(x) = a_n x^n + \\dots + a_0$ 是实系数多项式，$a_n &gt; 0$。如果 $k$ 是使得 $a_{n-1}, a_{n-2}, \\dots, a_{n-k+1} \\ge 0$ 而 $a_{n-k} &lt; 0$ 的第一个负系数的下标 (即 $a_n, \\dots, a_{n-k+1}$ 是首项之后第一个非负系数序列，然后遇到第一个负系数 $a_{n-k}$)。令 $B$ 是 $f(x)$ 中所有负系数的绝对值的最大值。则 $f(x)$ 的所有正实根 $c$ 都满足 $c &lt; 1 + \\sqrt[k]{\\frac{B}{a_n}}$。(PPT中的公式是 $c &lt; 1 + \\sqrt[k]{\\frac{b}{a_n}}$，其中 $b$ 可能是负系数绝对值的最大值，需要核对PPT的具体定义) 证明思路 (PPT中给出了一种推导):假设 $d &gt; 1 + \\sqrt[k]{B/a_n}$。要证明 $f(d)&gt;0$。$(d-1)^k &gt; B/a_n \\Rightarrow a_n(d-1)^k &gt; B$。$f(d) = a_n d^n + \\dots + a_{n-k+1}d^{n-k+1} + a_{n-k}d^{n-k} + \\dots + a_0$$f(d) \\ge a_n d^n - B(d^{n-k} + d^{n-k-1} + \\dots + d + 1)$ (将所有负系数替换为 $-B$，非负系数保留或设为0)$f(d) &gt; a_n d^n - B \\frac{d^{n-k+1}-1}{d-1}$$f(d) &gt; a_n d^n - B \\frac{d^{n-k+1}}{d-1}$ (因为 $d&gt;1$)$f(d) &gt; \\frac{d^{n-k+1}}{d-1} [a_n d^{k-1}(d-1) - B]$考虑 $a_n d^{k-1}(d-1)^k (d-1)^{-(k-1)} (d-1) - B = a_n (d-1)^k d^{k-1} - B$。PPT中的推导是：$f(d) = d^{n-k+1} [a_n d^{k-1} + \\dots + a_{n-k+1}] + a_{n-k}d^{n-k} + \\dots + a_0$$f(d) &gt; a_n d^n - B \\sum_{j=0}^{n-k} d^j = a_n d^n - B \\frac{d^{n-k+1}-1}{d-1}$$f(d) &gt; \\frac{1}{d-1} [a_n d^n(d-1) - B d^{n-k+1} + B]$$f(d) &gt; \\frac{d^{n-k+1}}{d-1} [a_n d^{k-1}(d-1) - B] + \\frac{B}{d-1}$如果 $a_n (d-1)^k &gt; B$，则 $a_n d^{k-1}(d-1) &gt; B \\frac{(d-1)^{1-k}}{d^{k-1}}$。这个推导比较复杂，需要严格按照PPT的步骤。核心思想是当 $d$ 足够大时，首项 $a_n d^n$ 会主导其他项，使得 $f(d)&gt;0$。 应用: 可以通过 $f(-x)$ 来找负根的下界。 9.8 Sturm 定理 (Sturm’s Theorem) (用于确定实根个数) 用于确定实系数多项式 $f(x)$ 在给定区间 $(a,b)$ 内不同实根的个数。 Sturm 序列的构造:假设 $f(x)$ 没有重根 (若有，先用 $f(x)/(f(x),f’(x))$ 替换)。 $g_0(x) = f(x)$ $g_1(x) = f’(x)$ $g_0(x) = q_1(x)g_1(x) - g_2(x)$ (注意是减去余式) $g_1(x) = q_2(x)g_2(x) - g_3(x)$ … $g_{k-1}(x) = q_k(x)g_k(x) - g_{k+1}(x)$ … $g_{s-1}(x) = q_s(x)g_s(x)$ (最后一个非零余式 $g_s(x)$ 是一个常数，因为 $f(x)$ 和 $f’(x)$ 互素)。这个序列 $g_0(x), g_1(x), \\dots, g_s(x)$ 称为 $f(x)$ 的Sturm 序列 (Sturm sequence)。 变号数 (Number of Sign Changes):给定一个实数 $c$，计算序列 $g_0(c), g_1(c), \\dots, g_s(c)$ 中符号变化的次数 (忽略0)。记作 $V(c)$。 Sturm 定理:设 $f(x)$ 是实系数多项式且无重根。设 $a &lt; b$ 且 $f(a) \\ne 0, f(b) \\ne 0$。则 $f(x)$ 在开区间 $(a,b)$ 内不同实根的个数等于 $V(a) - V(b)$。 Sturm 序列的性质: $g_s(x)$ (最后一个多项式) 不变号 (因为它是一个非零常数)。 序列中相邻两项 $g_k(x), g_{k+1}(x)$ 不会同时为零。(若 $g_k(c)=g_{k+1}(c)=0$，则由构造 $g_{k-1}(c)=0, \\dots, g_0(c)=g_1(c)=0$，与无重根矛盾)。 若 $g_k(c)=0$ ($0 &lt; k &lt; s$)，则 $g_{k-1}(c)$ 和 $g_{k+1}(c)$ 异号。(因为 $g_{k-1}(x) = q_k(x)g_k(x) - g_{k+1}(x)$，在 $c$ 点 $g_{k-1}(c) = -g_{k+1}(c)$)。 当 $x$ 经过 $f(x)$ 的一个根 $c$ 时： 若 $f’(c) &gt; 0$ ($f$ 在 $c$ 递增)，则 $g_0(x)=f(x)$ 从负变到正，$g_1(x)=f’(x)$ 在 $c$ 附近为正。符号序列 $(g_0, g_1)$ 从 $(-,+)$ 变为 $(+,+)$，变号数减少1。 若 $f’(c) &lt; 0$ ($f$ 在 $c$ 递减)，则 $g_0(x)=f(x)$ 从正变到负，$g_1(x)=f’(x)$ 在 $c$ 附近为负。符号序列 $(g_0, g_1)$ 从 $(+,-)$ 变为 $(-,-)$，变号数减少1。在这两种情况下，$V(x)$ 都减少1。 证明概要:当 $x$ 从 $a$ 连续变化到 $b$ 时： 如果 $x$ 经过一个 $g_k(x)$ ($k&gt;0$) 的根 $c$，但不是 $f(x)$ 的根。根据性质3，$g_{k-1}(c)$ 和 $g_{k+1}(c)$ 异号。在 $c$ 的小邻域内，$g_{k-1}(x)$ 和 $g_{k+1}(x)$ 的符号不变。$g_k(x)$ 在 $c$ 点变号。序列片段 $(g_{k-1}, g_k, g_{k+1})$ 的符号可能是 $(+, \\pm, -)$ 或 $(-, \\pm, +)$。例如，从 $(+,+,-)$ 变为 $(+,-,-)$，或从 $(+,-,-)$ 变为 $(+,+,-)$。变号数在 $c$ 点局部不变 ($V(c-\\epsilon) = V(c+\\epsilon)$ 对这个片段而言)。 如果 $x$ 经过 $f(x)$ 的一个根 $c$ (即 $g_0(c)=0$)。根据性质4，序列 $g_0(x), g_1(x)$ 的符号变化导致 $V(x)$ 减少1。因此，总的变号数变化 $V(a)-V(b)$ 就等于 $(a,b)$ 内根的个数。 (PPT中例子: $f(x) = x^5 - 3x + 1$。Sturm序列是 $x^5-3x+1$, $5x^4-3$, $\\frac{12}{5}x-1$, $\\frac{59083}{20736}$ (这里应该是正的，因为是最后一个非零余式的相反数)。在 $x=0$: $(1, -3, -1, \\text{pos}) \\implies V(0)=2$ (例如 $(+,-,-,+)$ 变号2次)。在 $x=1$: $(-1, 2, \\frac{7}{5}, \\text{pos}) \\implies V(1)=1$ (例如 $(-,+,+,+)$ 变号1次)。$V(0)-V(1)=2-1=1$。所以 $(0,1)$ 内有1个根。) 10. 有理系数与整系数多项式10.1 有理根定理 (Rational Root Theorem) 设 $f(x) = a_n x^n + \\dots + a_1 x + a_0$ 是一个整系数多项式。 如果一个既约分数 $\\frac{p}{q}$ (即 $p,q \\in \\mathbb{Z}, q \\ne 0, \\text{gcd}(p,q)=1$) 是 $f(x)$ 的一个有理根，则必有： $p | a_0$ (分子整除常数项) $q | a_n$ (分母整除首项系数) 证明:将 $\\frac{p}{q}$ 代入 $f(x)=0$：$a_n (\\frac{p}{q})^n + a_{n-1}(\\frac{p}{q})^{n-1} + \\dots + a_1(\\frac{p}{q}) + a_0 = 0$两边乘以 $q^n$：$a_n p^n + a_{n-1} p^{n-1}q + \\dots + a_1 p q^{n-1} + a_0 q^n = 0$ 移项 $a_0 q^n = -(a_n p^n + \\dots + a_1 p q^{n-1}) = -p(a_n p^{n-1} + \\dots + a_1 q^{n-1})$。所以 $p | a_0 q^n$。因为 $\\text{gcd}(p,q)=1$，所以 $\\text{gcd}(p,q^n)=1$。因此 $p | a_0$。 移项 $a_n p^n = -(a_{n-1}p^{n-1}q + \\dots + a_0 q^n) = -q(a_{n-1}p^{n-1} + \\dots + a_0 q^{n-1})$。所以 $q | a_n p^n$。因为 $\\text{gcd}(p,q)=1$，所以 $\\text{gcd}(q,p^n)=1$。因此 $q | a_n$。□ 应用: 可以列出所有可能的有理根的候选者，然后逐一检验。 (PPT中例子: $2x^3+x^2-3x+1$。可能有理根 $\\pm 1, \\pm \\frac{1}{2}$。检验知 $\\frac{1}{2}$ 是根。) 10.2 本原多项式 (Primitive Polynomial) 一个整系数多项式 $f(x) = a_n x^n + \\dots + a_0$ 称为本原的 (primitive)，如果其所有系数 $a_0, a_1, \\dots, a_n$ 的最大公约数是1。(即 $\\text{gcd}(a_0, \\dots, a_n)=1$)。 任何非零整系数多项式 $f(x)$ 都可以表示为 $f(x) = c \\cdot f_0(x)$，其中 $c$ 是 $f(x)$ 系数的最大公约数 (称为 $f(x)$ 的容度 (content))，$f_0(x)$ 是本原多项式。这个表示在差一个符号的意义下是唯一的。 10.3 高斯引理 (Gauss’s Lemma for Polynomials) 引理: 两个本原多项式 (整系数) 的乘积仍然是本原多项式。 证明:设 $f(x) = a_n x^n + \\dots + a_0$ 和 $g(x) = b_m x^m + \\dots + b_0$ 都是本原的。令 $h(x) = f(x)g(x) = c_k x^k + \\dots + c_0$。假设 $h(x)$ 不是本原的，则存在一个素数 $p$ 整除 $h(x)$ 的所有系数 $c_j$。因为 $f(x)$ 本原，所以 $p$ 不整除 $f(x)$ 的所有系数。设 $a_i$ 是第一个不被 $p$ 整除的 $f(x)$ 的系数 (即 $p|a_0, \\dots, p|a_{i-1}$ 但 $p \\nmid a_i$)。同理，设 $b_j$ 是第一个不被 $p$ 整除的 $g(x)$ 的系数 (即 $p|b_0, \\dots, p|b_{j-1}$ 但 $p \\nmid b_j$)。考虑 $h(x)$ 的系数 $c_{i+j}$:$c_{i+j} = (a_0 b_{i+j} + \\dots + a_{i-1}b_{j+1}) + a_i b_j + (a_{i+1}b_{j-1} + \\dots + a_{i+j}b_0)$。括号中的每一项都含有 $a_0, \\dots, a_{i-1}$ 中的一个或 $b_0, \\dots, b_{j-1}$ 中的一个，所以它们都被 $p$ 整除。因此 $p | (c_{i+j} - a_i b_j)$。又因为我们假设 $p|c_{i+j}$，所以 $p | a_i b_j$。但 $p$ 是素数，且 $p \\nmid a_i$，$p \\nmid b_j$，这与 $p|a_i b_j$ 矛盾 (根据素数的性质)。所以假设 $h(x)$ 不是本原的是错误的。因此 $h(x)$ 必须是本原的。□ 10.4 有理数域上的可约性与整系数环上的可约性 定理: 如果一个整系数多项式 $f(x)$ 在有理数域 $\\mathbb{Q}$ 上可约，那么它在整数环 $\\mathbb{Z}$ 上也一定可以分解为两个次数更低的整系数多项式的乘积。(换句话说，如果一个整系数多项式在 $\\mathbb{Z}[x]$ 中不可约，那么它在 $\\mathbb{Q}[x]$ 中也一定不可约，除非它能分解出一个非单位常数因子。)更准确地说：如果 $f(x) \\in \\mathbb{Z}[x]$ 且 $f(x)$ 在 $\\mathbb{Q}[x]$ 中可约，即 $f(x) = g(x)h(x)$ 其中 $g(x),h(x) \\in \\mathbb{Q}[x]$ 且 $\\deg g, \\deg h &lt; \\deg f$，则存在 $g_0(x), h_0(x) \\in \\mathbb{Z}[x]$ 使得 $f(x)=g_0(x)h_0(x)$ 且 $\\deg g_0 = \\deg g, \\deg h_0 = \\deg h$。 证明思路: 设 $f(x) = g(x)h(x)$，其中 $f(x) \\in \\mathbb{Z}[x]$，$g(x),h(x) \\in \\mathbb{Q}[x]$。 可以将 $g(x)$ 和 $h(x)$ 的系数通分，写成 $g(x) = \\frac{a}{b} g_1(x)$ 和 $h(x) = \\frac{c}{d} h_1(x)$，其中 $a,b,c,d \\in \\mathbb{Z}$，$g_1(x), h_1(x)$ 是本原的整系数多项式。 $f(x) = \\frac{ac}{bd} g_1(x)h_1(x)$。 令 $\\frac{ac}{bd} = \\frac{P}{Q}$ (既约分数)。则 $Q f(x) = P g_1(x)h_1(x)$。 $g_1(x)h_1(x)$ 根据高斯引理是本原的。 $Q f(x)$ 的容度是 $Q \\cdot \\text{content}(f)$。$P g_1(x)h_1(x)$ 的容度是 $P$ (因为 $g_1h_1$ 本原)。 所以 $Q \\cdot \\text{content}(f) = \\pm P$。由于 $\\text{gcd}(P,Q)=1$，所以 $Q$ 必须整除 $\\text{content}(f)$ 的符号，且 $P$ 必须整除 $\\text{content}(f)$ 的符号 (这里不严谨，应该是 $Q|\\text{content}(f)$ 且 $\\text{content}(f) = \\pm P/Q \\cdot 1 = \\pm P/Q$)。更简单地说：$Q f(x)$ 和 $P g_1(x)h_1(x)$ 是相伴的整系数多项式。由于 $g_1h_1$ 本原，所以 $P$ 是 $P g_1h_1$ 的容度。$Q f(x)$ 的容度是 $|Q| \\cdot \\text{content}(f)$。所以 $|Q| \\cdot \\text{content}(f) = |P|$。由于 $\\text{gcd}(P,Q)=1$，所以 $Q$ 必须是 $\\pm 1$。 因此 $\\frac{ac}{bd}$ 是一个整数或一个整数除以容度。实际上，$f(x) = (\\frac{ac}{bd} \\cdot \\text{content}(g_1 h_1)) \\cdot (\\text{primitive part of } g_1 h_1)$可以证明 $\\frac{ac}{bd}$ 必须是一个整数乘以 $f(x)$ 的容度的倒数。结论是 $f(x)$ 可以分解为两个整系数多项式的乘积，其次数与 $g(x), h(x)$ 相同。一个更清晰的论证是：$f(x) = r \\cdot G(x)H(x)$ 其中 $r \\in \\mathbb{Q}$, $G,H \\in \\mathbb{Z}[x]$ 且本原。$f(x)$ 的容度 $c(f)$ 必须等于 $r$ 的容度 $c(r)$ 乘以 $G(x)H(x)$ 的容度 (为1)。所以 $r$ 必须是 $c(f)$。因此 $f(x) = c(f) G(x)H(x)$。如果 $f(x)$ 本身是本原的，则 $r=\\pm 1$。 10.5 艾森斯坦判别法 (Eisenstein’s Criterion) 定理: 设 $f(x) = a_n x^n + a_{n-1} x^{n-1} + \\dots + a_1 x + a_0$ 是一个整系数多项式。如果存在一个素数 $p$ 使得： $p | a_0, p | a_1, \\dots, p | a_{n-1}$ (p 整除除首项系数外的所有系数) $p \\nmid a_n$ (p 不整除首项系数) $p^2 \\nmid a_0$ (p的平方不整除常数项)则 $f(x)$ 在有理数域 $\\mathbb{Q}$ 上是不可约的 (因此，如果它是本原的，在 $\\mathbb{Z}$ 上也除了平凡因子外不可约)。 证明 (反证法):假设 $f(x)$ 在 $\\mathbb{Q}$ 上可约。根据上面的定理，它可以分解为两个次数更低的整系数多项式的乘积：$f(x) = g(x)h(x) = (b_r x^r + \\dots + b_0)(c_s x^s + \\dots + c_0)$其中 $r,s \\ge 1$，$r+s=n$，$b_i, c_j \\in \\mathbb{Z}$。$a_n = b_r c_s$$a_0 = b_0 c_0$由于 $p|a_0$ 且 $p^2 \\nmid a_0$，所以 $p$ 必然恰好整除 $b_0, c_0$ 中的一个。不妨设 $p|b_0$ 且 $p \\nmid c_0$。由于 $p \\nmid a_n$，所以 $p \\nmid b_r$ 且 $p \\nmid c_s$。考虑 $b_i$ 的系数。因为 $p|b_0$ 且 $p \\nmid b_r$ (因为 $r \\ge 1$)，所以存在一个最小的下标 $k$ ($0 \\le k \\le r$) 使得 $p \\nmid b_k$ (且 $p|b_0, \\dots, p|b_{k-1}$)。因为 $p|b_0$，所以这个 $k \\ge 1$。现在看 $a_k = b_k c_0 + b_{k-1}c_1 + \\dots + b_0 c_k$。 $b_k c_0$: 因为 $p \\nmid b_k$ 且 $p \\nmid c_0$，所以 $p \\nmid b_k c_0$。 $b_{k-1}c_1 + \\dots + b_0 c_k$: 每一项都含有 $b_0, \\dots, b_{k-1}$ 中的一个，所以每一项都被 $p$ 整除。因此 $a_k \\equiv b_k c_0 \\pmod p$。由于 $p \\nmid b_k c_0$，所以 $p \\nmid a_k$。但是，根据艾森斯坦判别法的条件，如果 $k &lt; n$ (即 $a_k$ 不是首项系数 $a_n$)，则 $p|a_k$。这就产生了矛盾，除非 $k=n$。如果 $k=n$，则意味着 $p|b_0, p|b_1, \\dots, p|b_{n-1}$ 且 $p \\nmid b_n$ (这里 $r=n, s=0$，与 $s \\ge 1$ 矛盾，除非 $g(x)=f(x)$ 是平凡分解)。更准确的论证：$a_k = \\sum_{i=0}^k b_i c_{k-i}$。我们有 $p|b_0, \\dots, p|b_{k-1}$ 但 $p \\nmid b_k$。$a_k = (b_0 c_k + \\dots + b_{k-1}c_1) + b_k c_0$。第一部分被 $p$ 整除。因为 $p \\nmid b_k$ 且 $p \\nmid c_0$，所以 $p \\nmid b_k c_0$。因此 $p \\nmid a_k$。根据判别法条件，这要求 $k=n$ (即 $a_k=a_n$)。如果 $k=n$，那么 $r=n, s=0$。这意味着 $h(x)=c_0$ 是一个常数。由于 $f(x)$ 在 $\\mathbb{Q}$ 上可约，我们假设了 $r,s \\ge 1$，所以这里导致矛盾。因此，原假设 $f(x)$ 可约是错误的。□ 应用: 对任意 $n \\ge 1$，$x^n-2$ 在 $\\mathbb{Q}$ 上不可约 (取 $p=2$)。 对任意素数 $p$，$f(x) = x^{p-1} + x^{p-2} + \\dots + x + 1$ (分圆多项式 $\\Phi_p(x)$) 在 $\\mathbb{Q}$ 上不可约。证明: 做代换 $x = y+1$。$f(x) = \\frac{x^p-1}{x-1}$。$g(y) = f(y+1) = \\frac{(y+1)^p-1}{(y+1)-1} = \\frac{y^p + \\binom{p}{1}y^{p-1} + \\dots + \\binom{p}{p-1}y + 1 - 1}{y}$$g(y) = y^{p-1} + \\binom{p}{1}y^{p-2} + \\dots + \\binom{p}{p-2}y + \\binom{p}{p-1}$。首项系数是1 (不被 $p$ 整除)。常数项是 $\\binom{p}{p-1} = p$ (被 $p$ 整除，但不被 $p^2$ 整除)。中间项的系数 $\\binom{p}{k}$ ($1 \\le k \\le p-1$) 都可以被 $p$ 整除 (因为 $p$ 是素数)。所以 $g(y)$ 满足艾森斯坦判别法条件 (对素数 $p$)，因此 $g(y)$ 在 $\\mathbb{Q}$ 上不可约。如果 $f(x)$ 可约 $f(x)=A(x)B(x)$，则 $g(y)=f(y+1)=A(y+1)B(y+1)$ 也可约，矛盾。所以 $f(x)$ 在 $\\mathbb{Q}$ 上不可约。□ 11. 多元多项式 (Polynomials in Multiple Variables)11.1 定义 设 $F$ 是一个数域，$x_1, x_2, \\dots, x_n$ 是 $n$ 个独立的不定元。 一个单项式 (monomial) 形如 $a x_1^{k_1} x_2^{k_2} \\dots x_n^{k_n}$，其中 $a \\in F$ 是系数，$k_j \\ge 0$ 是整数。 如果 $a \\ne 0$，该单项式的次数 (degree) 是 $k = k_1 + k_2 + \\dots + k_n$。 一个多元多项式 (polynomial in $n$ variables) 是有限个单项式的和：$f(x_1, x_2, \\dots, x_n) = \\sum a_{j_1 j_2 \\dots j_n} x_1^{j_1} x_2^{j_2} \\dots x_n^{j_n}$ 齐次多项式 (Homogeneous Polynomial): 如果一个多项式中所有非零单项式的次数都相同，则称其为齐次多项式。这个共同的次数称为齐次多项式的次数。 11.2 多元多项式的运算 加法、减法、乘法定义类似于一元多项式，通过合并同类项 (相同 $x_1^{k_1}\\dots x_n^{k_n}$ 部分的项)。 $F[x_1, \\dots, x_n]$ (系数在 $F$ 上的所有 $n$ 元多项式的集合) 关于这些运算构成一个交换含幺环。 $F[x_1, \\dots, x_n]$ 也是一个整环。 11.3 字典序 (Lexicographical Order) 用于比较单项式，从而可以定义多元多项式的首项。 单项式 $x_1^{k_1} \\dots x_n^{k_n}$ 大于 $x_1^{j_1} \\dots x_n^{j_n}$ (记作 $&gt;_{lex}$)，如果从左到右比较指数序列 $(k_1, \\dots, k_n)$ 和 $(j_1, \\dots, j_n)$，第一个不同的指数是前者较大。即，存在 $s$ 使得 $k_1=j_1, \\dots, k_{s-1}=j_{s-1}$ 且 $k_s &gt; j_s$。 例如，$x_1^2 x_2 &gt;_{lex} x_1 x_2^3$ 因为第一个指数 $2&gt;1$。 一个多元多项式的首项 (leading term) 是其在字典序下最大的非零单项式。 11.4 多元多项式环的性质 定理 (Hilbert’s Basis Theorem - 的推论): 如果 $F$ 是一个域，则 $F[x_1, \\dots, x_n]$ 是一个唯一因子分解整环 (Unique Factorization Domain, UFD)。这意味着每个非常数多项式都可以唯一地 (不计顺序和单位元因子) 分解为不可约多项式的乘积。 定理: 若 $f(x_1, \\dots, x_n)$ 是数域 $F$ 上的一个非零多元多项式，则存在 $c_1, \\dots, c_n \\in F$ (如果 $F$ 是无限域) 使得 $f(c_1, \\dots, c_n) \\ne 0$。 证明 (对变量个数 $n$ 归纳):$n=1$: 非零一元多项式至多有有限个根，所以总能找到使其不为0的值 (如果 $F$ 无限)。假设对 $n-1$ 个变量成立。将 $f(x_1, \\dots, x_n)$ 看作是关于 $x_n$ 的多项式，其系数是 $x_1, \\dots, x_{n-1}$ 的多项式：$f(x_1, \\dots, x_n) = A_k(x_1, \\dots, x_{n-1}) x_n^k + \\dots + A_0(x_1, \\dots, x_{n-1})$。由于 $f$ 非零，所以至少有一个系数 $A_j(x_1, \\dots, x_{n-1})$ 是非零的。根据归纳假设 (如果 $F$ 无限)，存在 $c_1, \\dots, c_{n-1} \\in F$ 使得 $A_j(c_1, \\dots, c_{n-1}) \\ne 0$ (特别是对于最高次项系数 $A_k$ 如果它非零)。代入这些值，得到一个关于 $x_n$ 的非零一元多项式 $f(c_1, \\dots, c_{n-1}, x_n)$。这个一元多项式至多有有限个根，所以 (如果 $F$ 无限) 存在 $c_n \\in F$ 使得 $f(c_1, \\dots, c_{n-1}, c_n) \\ne 0$。□ 推论 (多项式恒等): 如果数域 $F$ 是无限域，两个多元多项式 $f(x_1, \\dots, x_n)$ 和 $g(x_1, \\dots, x_n)$ 相等 (作为函数，即对所有 $(c_1, \\dots, c_n) \\in F^n$ 都有 $f(c_1, \\dots, c_n) = g(c_1, \\dots, c_n)$) 当且仅当它们作为形式多项式是相等的 (即对应项系数都相同)。 证明: 考虑 $h=f-g$。如果 $h$ 作为函数为0，则 $h$ 在所有点取值为0。如果 $h$ 不是零多项式，根据上一定理，它不可能在所有点为0 (矛盾)。所以 $h$ 必须是零多项式。□ 11.5 对称多项式 (Symmetric Polynomials) 一个多元多项式 $f(x_1, \\dots, x_n)$ 称为对称的 (symmetric)，如果对任意的下标置换 $\\sigma \\in S_n$ (作用于不定元)，都有：$f(x_{\\sigma(1)}, x_{\\sigma(2)}, \\dots, x_{\\sigma(n)}) = f(x_1, x_2, \\dots, x_n)$。(即，任意交换两个变量的位置，多项式不变)。 初等对称多项式 (Elementary Symmetric Polynomials):$\\sigma_1 = x_1 + x_2 + \\dots + x_n$$\\sigma_2 = \\sum_{1 \\le i &lt; j \\le n} x_i x_j$…$\\sigma_k = \\sum_{1 \\le i_1 &lt; \\dots &lt; i_k \\le n} x_{i_1} x_{i_2} \\dots x_{i_k}$…$\\sigma_n = x_1 x_2 \\dots x_n$ 对称多项式基本定理 (Fundamental Theorem of Symmetric Polynomials):任何数域 $F$ 上的对称多项式 $f(x_1, \\dots, x_n)$ 都可以唯一地表示为初等对称多项式 $\\sigma_1, \\dots, \\sigma_n$ 的一个多项式。即，存在唯一的 $g(y_1, \\dots, y_n) \\in F[y_1, \\dots, y_n]$ 使得$f(x_1, \\dots, x_n) = g(\\sigma_1(x_1,\\dots,x_n), \\dots, \\sigma_n(x_1,\\dots,x_n))$。 证明思路 (构造性，基于字典序): 取 $f(x_1, \\dots, x_n)$ 在字典序下的首项 $L(f) = a x_1^{k_1} x_2^{k_2} \\dots x_n^{k_n}$。由于 $f$ 对称，其首项的指数必然满足 $k_1 \\ge k_2 \\ge \\dots \\ge k_n \\ge 0$。(否则，若 $k_s &lt; k_{s+1}$，交换 $x_s, x_{s+1}$ 会得到一个字典序更大的项 $a x_1^{k_1} \\dots x_s^{k_{s+1}} x_{s+1}^{k_s} \\dots$，与 $L(f)$ 是首项矛盾)。 构造一个初等对称多项式的乘积 $h = a \\sigma_1^{k_1-k_2} \\sigma_2^{k_2-k_3} \\dots \\sigma_{n-1}^{k_{n-1}-k_n} \\sigma_n^{k_n}$。 $h$ 也是一个对称多项式。其字典序下的首项 $L(h)$ 恰好等于 $L(f)$。 $L(\\sigma_1) = x_1$ $L(\\sigma_2) = x_1 x_2$ … $L(\\sigma_j) = x_1 x_2 \\dots x_j$ $L(h) = a (L(\\sigma_1))^{k_1-k_2} \\dots (L(\\sigma_n))^{k_n}$$= a (x_1)^{k_1-k_2} (x_1x_2)^{k_2-k_3} \\dots (x_1\\dots x_n)^{k_n}$$= a x_1^{(k_1-k_2)+(k_2-k_3)+\\dots+k_n} x_2^{(k_2-k_3)+\\dots+k_n} \\dots x_n^{k_n}$$= a x_1^{k_1} x_2^{k_2} \\dots x_n^{k_n} = L(f)$。 考虑 $f_1 = f - h$。$f_1$ 仍然是对称多项式，且其字典序首项 $L(f_1)$ 严格小于 $L(f)$。 对 $f_1$ 重复此过程。由于字典序良序，这个过程必然在有限步内终止 (当 $f_k=0$ 时)。最终得到 $f = h_1 + h_2 + \\dots + h_m$，其中每个 $h_j$ 都是初等对称多项式的多项式。 唯一性证明:假设 $g(\\sigma_1, \\dots, \\sigma_n) = h(\\sigma_1, \\dots, \\sigma_n)$ 且 $g \\ne h$。令 $\\phi = g-h \\ne 0$。则 $\\phi(\\sigma_1, \\dots, \\sigma_n) = 0$ (作为 $x_i$ 的多项式恒为0)。取 $\\phi(y_1, \\dots, y_n)$ 中字典序最高的一个非零单项式 $c y_1^{k_1} \\dots y_n^{k_n}$。将其代入 $\\sigma_i$ (即 $y_i \\leftarrow \\sigma_i$)，得到关于 $x_i$ 的多项式。$\\phi(\\sigma_1, \\dots, \\sigma_n)$ 的字典序首项是由 $c \\sigma_1^{k_1} \\dots \\sigma_n^{k_n}$ 的首项决定的，即 $c (x_1)^{k_1} (x_1x_2)^{k_2} \\dots (x_1\\dots x_n)^{k_n}$ 的首项，这个首项不为0，与 $\\phi(\\sigma_1, \\dots, \\sigma_n)=0$ 矛盾。所以 $\\phi$ 必须是零多项式，即 $g=h$。□ (PPT中例子: $f(x_1,x_2,x_3) = x_1^2 x_2 + x_1^2 x_3 + x_1 x_2^2 + x_1 x_2 x_3 + x_2^2 x_3 + x_2 x_3^2$ (这似乎不是对称的，PPT例子可能是 $x_1^2x_2 + x_1x_2^2 + x_1^2x_3 + x_1x_3^2 + x_2^2x_3 + x_2x_3^2$)如果 $f(x_1,x_2,x_3) = x_1^2x_2 + x_1x_2^2 + x_1^2x_3 + x_1x_3^2 + x_2^2x_3 + x_2x_3^2$首项是 $x_1^2 x_2$ (按字典序 $x_1 &gt; x_2 &gt; x_3$)$k_1=2, k_2=1, k_3=0$。$h_1 = \\sigma_1^{2-1} \\sigma_2^{1-0} \\sigma_3^0 = \\sigma_1 \\sigma_2 = (x_1+x_2+x_3)(x_1x_2+x_1x_3+x_2x_3)$$= (x_1^2x_2 + x_1^2x_3 + x_1x_2^2 + x_1x_3^2 + x_2^2x_3 + x_2x_3^2) + 3x_1x_2x_3$$f_1 = f - h_1 = -3x_1x_2x_3 = -3\\sigma_3$。所以 $f = \\sigma_1\\sigma_2 - 3\\sigma_3$。) 12. 结式与判别式 (Resultant and Discriminant)12.1 两个多项式有公共根的条件 设 $f(x), g(x) \\in F[x]$。$f(x)$ 和 $g(x)$ 有公共根 $d(x) \\ne 1$ (即 $(f(x), g(x))$ 的次数 $\\ge 1$) 当且仅当存在非零多项式 $u(x), v(x) \\in F[x]$ 使得： $\\deg u(x) &lt; \\deg g(x)$ $\\deg v(x) &lt; \\deg f(x)$ $f(x)u(x) = g(x)v(x)$ (或者 $f(x)u(x) + g(x)v(x) = 0$ 如果 $v(x)$ 取相反数) 证明:($\\Rightarrow$) 若 $(f,g)=d(x)$ 且 $\\deg d(x) \\ge 1$。则 $f(x) = d(x)f_1(x)$，$g(x) = d(x)g_1(x)$。令 $u(x) = g_1(x)$，$v(x) = f_1(x)$。则 $\\deg u(x) = \\deg g(x) - \\deg d(x) &lt; \\deg g(x)$。$\\deg v(x) = \\deg f(x) - \\deg d(x) &lt; \\deg f(x)$。$f(x)u(x) = d(x)f_1(x)g_1(x)$$g(x)v(x) = d(x)g_1(x)f_1(x)$。所以 $f(x)u(x) = g(x)v(x)$。($\\Leftarrow$) 若 $f(x)u(x) = g(x)v(x)$ 且 $u,v$ 非零且次数受限。$f(x)u(x) - g(x)v(x) = 0$。假设 $(f,g)=1$ (互素)。则由 $f|gv$ 可得 $f|v$ (因为 $(f,g)=1$)。但 $\\deg v &lt; \\deg f$，所以这只有当 $v(x)=0$ 时才可能。如果 $v(x)=0$，则 $f(x)u(x)=0$。由于 $f(x)$ (如果非零) 不是零因子，则 $u(x)=0$。这与 $u,v$ 非零矛盾。所以 $(f,g) \\ne 1$，即它们有公共因式 (因此有公共根，如果在代数闭包中)。□ 12.2 Sylvester 矩阵与结式 (Resultant) 设 $f(x) = a_n x^n + \\dots + a_0$ ($a_n \\ne 0$)$g(x) = b_m x^m + \\dots + b_0$ ($b_m \\ne 0$) 我们寻找 $u(x) = t_{m-1}x^{m-1} + \\dots + t_0$ 和 $v(x) = s_{n-1}x^{n-1} + \\dots + s_0$ (PPT中 $v(x)$ 的系数是 $t_{m+n-1}, \\dots, t_m$) 使得 $f(x)u(x) + g(x)v(x) = 0$ (这里 $v(x)$ 带负号)。将 $f(x)u(x) = -g(x)v(x)$ (或 $f(x)u(x) - g(x)v_0(x) = 0$ 如果 $v_0 = -v$) 展开，比较 $x^{m+n-1}, \\dots, x^1, x^0$ 的系数，得到一个关于 $t_i$ 和 $s_j$ (或PPT中的 $t_i$) 的 $m+n$ 个线性方程的齐次方程组。这个方程组有非零解 (即 $u,v$ 不全为0) 当且仅当其系数矩阵的行列式为0。 这个 $(m+n) \\times (m+n)$ 的系数矩阵称为 $f(x)$ 和 $g(x)$ 的Sylvester 矩阵 (Sylvester Matrix)。它的行列式称为 $f(x)$ 和 $g(x)$ 的结式 (Resultant)，记作 $R(f,g)$ 或 $\\text{Res}(f,g)$。 $R(f,g) = \\det \\begin{pmatrix}a_n &amp; a_{n-1} &amp; \\dots &amp; a_1 &amp; a_0 &amp; 0 &amp; \\dots &amp; 0 \\\\0 &amp; a_n &amp; a_{n-1} &amp; \\dots &amp; a_1 &amp; a_0 &amp; \\dots &amp; 0 \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; &amp; &amp; &amp; \\ddots &amp; \\vdots \\\\0 &amp; \\dots &amp; 0 &amp; a_n &amp; a_{n-1} &amp; \\dots &amp; a_1 &amp; a_0 \\\\b_m &amp; b_{m-1} &amp; \\dots &amp; b_1 &amp; b_0 &amp; 0 &amp; \\dots &amp; 0 \\\\0 &amp; b_m &amp; b_{m-1} &amp; \\dots &amp; b_1 &amp; b_0 &amp; \\dots &amp; 0 \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; &amp; &amp; &amp; \\ddots &amp; \\vdots \\\\0 &amp; \\dots &amp; 0 &amp; b_m &amp; b_{m-1} &amp; \\dots &amp; b_1 &amp; b_0\\end{pmatrix}$(上面 $m$ 行 $a_i$，下面 $n$ 行 $b_j$) 定理: $f(x)$ 和 $g(x)$ (在某个代数闭包中) 有公共根当且仅当 $R(f,g)=0$。(假设 $a_n \\ne 0, b_m \\ne 0$) 12.3 结式与根的关系 定理: 若 $f(x) = a_n \\prod_{i=1}^n (x-x_i)$ 且 $g(x) = b_m \\prod_{j=1}^m (x-y_j)$，则$R(f,g) = a_n^m b_m^n \\prod_{i=1}^n \\prod_{j=1}^m (x_i - y_j)$或者，更常用的形式是：$R(f,g) = a_n^m \\prod_{i=1}^n g(x_i)$$R(f,g) = (-1)^{nm} b_m^n \\prod_{j=1}^m f(y_j)$ 证明思路:将 $R(f,g)$ 看作是 $a_i, b_j$ 的多项式。如果某个 $x_i = y_j$，则 $f,g$ 有公共根，$R(f,g)=0$。所以 $(x_i-y_j)$ 必须是 $R(f,g)$ 的因式 (作为根的多项式)。$R(f,g)$ 关于 $a_i$ 是 $m$ 次齐次的，关于 $b_j$ 是 $n$ 次齐次的。$g(x_i) = b_m \\prod (x_i-y_j)$。$a_n^m \\prod g(x_i) = a_n^m \\prod_{i=1}^n (b_m \\prod_{j=1}^m (x_i-y_j)) = a_n^m b_m^n \\prod_i \\prod_j (x_i-y_j)$。比较次数和首项系数 (例如当 $g(x)=x^m$ 时) 可以确定常数因子。当 $g(x)=x^m$ ($b_m=1$, 其他 $b_j=0$)，$g(x_i)=x_i^m$。$R(f,x^m)$ 通过Laplace展开 Sylvester 矩阵可以得到 $a_0^m$ (或 $(-1)^{nm}a_0^m$)。$a_0 = a_n \\prod (-x_i)$。所以 $a_n^m \\prod g(x_i) = a_n^m \\prod x_i^m = a_n^m ((-1)^n a_0/a_n)^m = a_n^{m-m} (-1)^{nm} a_0^m = (-1)^{nm} a_0^m$。这与直接计算 $R(f,x^m)$ 相符。□ 12.4 判别式 (Discriminant) 多项式 $f(x) = a_n x^n + \\dots + a_0$ ($a_n \\ne 0$) 有重根当且仅当 $f(x)$ 与其导数 $f’(x)$ 有公共根。 $f(x)$ 的判别式 (Discriminant) 定义为：$\\Delta(f) = \\frac{(-1)^{n(n-1)/2}}{a_n} R(f, f’)$(分母 $a_n$ 是为了使判别式成为系数的整系数多项式，并且与经典定义匹配)。 定理: $f(x)$ 有重根当且仅当 $\\Delta(f)=0$。 (PPT中例子: 二次 $ax^2+bx+c$，判别式是 $b^2-4ac$。三次 $x^3+px+q$，判别式是 $-4p^3-27q^2$。) 12.5 判别式与根的关系 若 $f(x) = a_n \\prod_{i=1}^n (x-x_i)$，则$\\Delta(f) = a_n^{2n-2} \\prod_{1 \\le i &lt; j \\le n} (x_i - x_j)^2$ 证明思路:$R(f,f’) = a_n^{n-1} \\prod_{i=1}^n f’(x_i)$。$f’(x) = a_n \\sum_{k=1}^n \\prod_{j \\ne k} (x-x_j)$。$f’(x_i) = a_n \\prod_{j \\ne i} (x_i-x_j)$。$R(f,f’) = a_n^{n-1} \\prod_{i=1}^n [a_n \\prod_{j \\ne i} (x_i-x_j)]$$= a_n^{n-1} a_n^n \\prod_{i=1}^n \\prod_{j \\ne i} (x_i-x_j)$$= a_n^{2n-1} \\prod_{i \\ne j} (x_i-x_j)$$= a_n^{2n-1} \\prod_{i&lt;j} (x_i-x_j) \\prod_{j&lt;i} (x_j-x_i)$$= a_n^{2n-1} \\prod_{i&lt;j} (x_i-x_j) \\prod_{i&lt;j} (-(x_i-x_j))$$= a_n^{2n-1} (-1)^{n(n-1)/2} \\prod_{i&lt;j} (x_i-x_j)^2$。代入 $\\Delta(f) = \\frac{(-1)^{n(n-1)/2}}{a_n} R(f, f’)$ 即可得到公式。□ 12.6 应用 (消元法) 结式可以用于求解多元多项式方程组，通过消去变量。 例如，求解方程组 $\\{f(x,y)=0, g(x,y)=0\\}$。将 $f,g$ 看作是关于 $x$ 的多项式，其系数是 $y$ 的多项式：$f(x,y) = A_n(y)x^n + \\dots + A_0(y)$$g(x,y) = B_m(y)x^m + \\dots + B_0(y)$计算关于 $x$ 的结式 $R_x(f,g)$。这将得到一个只含 $y$ 的多项式 (或多项式方程 $R_x(f,g)(y)=0$)。这个方程的根 $y_k$ 是使得原方程组有公共解 $(x^, y_k)$ 的 $y$ 值。然后可以将 $y_k$ 代回原方程组求对应的 $x^$。 这是一种将多元问题降维的方法。","link":"/MATH1408/Chapter1-MATH1408.html"},{"title":"Chapter3_MATH1408","text":"行列式与矩阵理论进阶1. 行列式 (Determinant)1.1 行列式的定义$n$ 阶行列式，记作 $|A|$ 或 $\\det(A)$，其中 $A = (a_{ij})$ 是一个 $n \\times n$ 的方阵，其值为： $$ |A| = \\left| \\begin{array}{cccc} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\cdots & a_{nn} \\end{array} \\right| = \\sum_{j_1j_2\\cdots j_n} (-1)^{\\tau(j_1j_2\\cdots j_n)} a_{1j_1}a_{2j_2}\\cdots a_{nj_n} $$ 其中： 求和遍及所有 $n$ 元排列 $j_1j_2\\cdots j_n$ (共 $n!$ 项)。 $\\tau(j_1j_2\\cdots j_n)$ 是排列 $j_1j_2\\cdots j_n$ 的逆序数。 每一项是取自不同行、不同列的 $n$ 个元素的乘积，并根据列下标排列的奇偶性赋予正负号。 若 $\\tau(j_1j_2\\cdots j_n)$ 为偶数，则该项取正号。 若 $\\tau(j_1j_2\\cdots j_n)$ 为奇数，则该项取负号。 历史: 行列式的概念早于矩阵，最初用于判断线性方程组是否有唯一解。 1.2 行列式的性质 (Properties of Determinants)PPT中列举了7条性质，其中G. Strang强调了性质(2)到(5)是定义行列式的基本公理： 转置不变性: $|A^T| = |A|$。 证明思路: 行列式定义中 $a_{ij}$ 可以按行展开也可以按列展开，其本质是对称的。 线性性 (对行/列): 若某一行 (或列) 的元素是两数之和，则行列式等于对应的两个行列式之和。$| \\dots, \\mathbf{r}_i + \\mathbf{r’}_i, \\dots | = | \\dots, \\mathbf{r}_i, \\dots | + | \\dots, \\mathbf{r’}_i, \\dots |$ 若某一行 (或列) 有公因子 $k$，则 $k$ 可以提到行列式符号外面。$| \\dots, k\\mathbf{r}_i, \\dots | = k | \\dots, \\mathbf{r}_i, \\dots |$ 推论: 若某一行 (或列) 元素全为0，则行列式为0。 两行 (或列) 互换，行列式变号: 证明思路: 交换两行对应于排列的逆序数改变奇偶性。 有两行 (或列) 相同，行列式为0: 证明思路: 交换相同的两行，行列式变号，但行列式本身不变，所以 $D = -D \\implies D=0$。 (要求域的特征不为2)或者，可以将一行减去另一相同的行，得到一行全为0，故行列式为0。 将某一行 (或列) 的 $k$ 倍加到另一行 (或列)，行列式不变: 证明思路: $| \\dots, \\mathbf{r}_i + k\\mathbf{r}_j, \\dots, \\mathbf{r}_j, \\dots | = | \\dots, \\mathbf{r}_i, \\dots, \\mathbf{r}_j, \\dots | + | \\dots, k\\mathbf{r}_j, \\dots, \\mathbf{r}_j, \\dots |$第二项因为有两行成比例 (或利用性质4)，所以为0。 行列式乘法定理: $|AB| = |A||B|$ (对同阶方阵 $A, B$)。 证明思路: 若 $|A|=0$ 或 $|B|=0$，则 $\\text{rank}(A) &lt; n$ 或 $\\text{rank}(B) &lt; n$。那么 $\\text{rank}(AB) \\le \\min(\\text{rank}(A), \\text{rank}(B)) &lt; n$，所以 $|AB|=0$。公式成立。 若 $|A| \\neq 0$ 且 $|B| \\neq 0$。可以将 $A$ 和 $B$ 表示为初等矩阵的乘积。对初等矩阵验证该性质，然后推广到一般矩阵。另一种方法是构造分块矩阵 $\\begin{pmatrix} A &amp; 0 \\\\ -I &amp; B \\end{pmatrix}$，其行列式为 $|A||B|$。通过行变换（将 $A$ 的行乘以 $B$ 的对应列元素加到 $B$ 的行上）可以将其变为 $\\begin{pmatrix} A &amp; AB \\\\ -I &amp; 0 \\end{pmatrix}$，其行列式为 $(-1)^{n^2} |-I| |AB| = |AB|$ (需要仔细处理符号)。更常见的证明是构造 $\\begin{pmatrix} A &amp; 0 \\\\ -I &amp; B \\end{pmatrix}$ 通过行变换变为 $\\begin{pmatrix} A &amp; AB \\\\ -I &amp; 0 \\end{pmatrix}$，或者构造 $\\begin{pmatrix} A &amp; -AB \\\\ 0 &amp; B \\end{pmatrix}$。 Laplace展开定理 (按行/列展开):行列式等于其任一行 (或列) 的各元素与其对应的代数余子式乘积之和。按第 $i$ 行展开: $D = \\sum_{j=1}^n a_{ij}A_{ij}$按第 $j$ 列展开: $D = \\sum_{i=1}^n a_{ij}A_{ij}$其中 $A_{ij} = (-1)^{i+j}M_{ij}$ 是元素 $a_{ij}$ 的代数余子式，$M_{ij}$ 是去掉第 $i$ 行和第 $j$ 列后得到的 $n-1$ 阶子行列式。 重要推论: 某一行 (或列) 的元素与另一行 (或列) 对应元素的代数余子式乘积之和为0。$\\sum_{k=1}^n a_{ik}A_{jk} = \\delta_{ij} D$其中 $\\delta_{ij}$ 是 Kronecker delta: $\\delta_{ij} = \\begin{cases} 1, &amp; i=j \\\\ 0, &amp; i \\neq j \\end{cases}$ 证明 ($i \\neq j$): $\\sum_{k=1}^n a_{ik}A_{jk}$ 可以看作是将矩阵的第 $j$ 行替换为第 $i$ 行后得到的新矩阵 $A’$ 按第 $j$ 行展开。由于 $A’$ 有两行相同 (都是原来的第 $i$ 行)，所以 $|A’|=0$。 1.3 行列式的计算方法 利用定义: 仅适用于低阶 (2阶、3阶对角线法则)。高阶计算量巨大 ($n!$)。 利用性质化为上/下三角行列式: 上/下三角行列式的值等于主对角线元素的乘积。 通过初等行变换（性质3, 5）将行列式化为三角形式。 降阶法 (Laplace展开): 选择含有较多0的行或列进行展开。 利用行列式性质进行化简和运算: 例1:$D = \\left| \\begin{array}{cccc}1+x &amp; 1 &amp; 1 &amp; 1 \\\\1 &amp; 1-x &amp; 1 &amp; 1 \\\\1 &amp; 1 &amp; 1+y &amp; 1 \\\\1 &amp; 1 &amp; 1 &amp; 1-y\\end{array} \\right|$方法1 (行变换):$r_1 \\leftarrow r_1 - r_2$, $r_3 \\leftarrow r_3 - r_4$$D = \\left| \\begin{array}{cccc}x &amp; x &amp; 0 &amp; 0 \\\\1 &amp; 1-x &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; y &amp; y \\\\1 &amp; 1 &amp; 1 &amp; 1-y\\end{array} \\right| = xy \\left| \\begin{array}{cccc}1 &amp; 1 &amp; 0 &amp; 0 \\\\1 &amp; 1-x &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; 1 &amp; 1 \\\\1 &amp; 1 &amp; 1 &amp; 1-y\\end{array} \\right|$$r_2 \\leftarrow r_2 - r_1$, $r_4 \\leftarrow r_4 - r_3$ (对内部2x2块操作，或直接对4x4操作)$ = xy \\left| \\begin{array}{cccc}1 &amp; 1 &amp; 0 &amp; 0 \\\\0 &amp; -x &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; 0 &amp; -y\\end{array} \\right| = xy \\left| \\begin{array}{cccc}1 &amp; 1 &amp; 0 &amp; 0 \\\\0 &amp; -x &amp; 0 &amp; 0 \\\\ % r_2 &lt;- r_2 - r_30 &amp; 0 &amp; 1 &amp; 1 \\\\0 &amp; 0 &amp; 0 &amp; -y\\end{array} \\right| = xy(1)(-x)(1)(-y) = x^2y^2$. 方法2 (升阶/镶边法，原PPT中似乎是这种思路，但行列式操作有误):原PPT的第二种解法似乎是想构造一个 $(n+1) \\times (n+1)$ 的行列式，然后通过变换得到原行列式与某个因子的乘积。例如：$D = \\left| \\begin{array}{cc} A &amp; B \\\\ C &amp; D_0 \\end{array} \\right|$。但PPT中的操作 $D = \\left| \\begin{array}{ccccc} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1+x &amp; 1 &amp; 1 &amp; 1 \\\\ \\vdots \\end{array} \\right|$ 后续的 $-1$ 倍加到各行，然后得到对角阵 $x, -x, y, -y$ 的思路是正确的，但初始构造的镶边行列式与原行列式的关系需要明确。一个更规范的镶边技巧是针对 $D = |A|$，可以构造 $\\begin{vmatrix} 1 &amp; \\mathbf{b}^T \\\\ \\mathbf{c} &amp; A \\end{vmatrix}$ 等。PPT中的第二种解法更像是直接从 $r_2 \\leftarrow r_2-r_1, r_3 \\leftarrow r_3-r_1, r_4 \\leftarrow r_4-r_1$ 开始，然后提取公因子或者继续化简。 方法3 (多项式零点法/待定系数法):将 $D(x,y)$ 看作是关于 $x$ 的多项式。$D(x,y) = \\left| \\begin{array}{cccc}1+x &amp; 1 &amp; 1 &amp; 1 \\\\1 &amp; 1-x &amp; 1 &amp; 1 \\\\1 &amp; 1 &amp; 1+y &amp; 1 \\\\1 &amp; 1 &amp; 1 &amp; 1-y\\end{array} \\right|$当 $x=0$ 时，$D(0,y) = \\left| \\begin{array}{cccc}1 &amp; 1 &amp; 1 &amp; 1 \\\\1 &amp; 1 &amp; 1 &amp; 1 \\\\\\dots \\end{array} \\right| = 0$。所以 $x$ 是一个因子。由于 $D(x,y)$ 中 $x$ 的最高次数是2 (来自 $(1+x)(1-x)$)，且 $D(x,y) = D(-x,y)$ (将 $x$ 替换为 $-x$，然后交换前两列再交换前两行，行列式不变号两次)，所以 $D(x,y)$ 是关于 $x$ 的偶函数，且 $x=0$ 是二重根，即 $x^2$ 是因子。同理可证 $y^2$ 是因子。所以 $D(x,y) = C x^2 y^2$。比较 $x^2y^2$ 的系数 (例如，来自对角线元素 $(1+x)(1-x)(1+y)(1-y)$ 展开后 $x^2y^2$ 的系数是1)，得到 $C=1$.因此 $D(x,y) = x^2y^2$. 例2 (轮换行列式/循环行列式):$D = \\left| \\begin{array}{cccc}x &amp; y &amp; z &amp; w \\\\y &amp; x &amp; w &amp; z \\\\z &amp; w &amp; x &amp; y \\\\w &amp; z &amp; y &amp; x\\end{array} \\right|$(这是一个循环矩阵的行列式，但元素排列方式与标准循环矩阵不同，更像某些对称结构)PPT中的解法：将所有列加到第一列：第一列元素都变成 $x+y+z+w$，提取公因子。$c_1 \\leftarrow c_1+c_2-c_3-c_4$ (或类似组合) 尝试凑出 $x+y-z-w$ 等因子。对于 $n$ 阶循环行列式 $D = \\text{circ}(a_1, a_2, \\dots, a_n)$，其值为 $\\prod_{j=0}^{n-1} f(\\omega_j)$，其中 $f(x) = a_1 + a_2x + \\dots + a_nx^{n-1}$，$x^n=1$ 的根为 $\\omega_j = e^{i \\frac{2\\pi j}{n}}$.本题的行列式是 $D = (x+y+z+w)(x+y-z-w)(x+z-y-w)(x+w-y-z)$.(这里PPT用了“范德蒙行列式”来辅助，可能是指将其与某个多项式在单位根处取值联系起来)设 $P(t) = x+yt+zt^2+wt^3$。$D = \\prod_{k=0}^3 P(\\epsilon_k)$ 其中 $\\epsilon_k$ 是 $t^4=1$ 的某个特定排列的根（可能不是标准的单位根，而是 $1, -1, i, -i$的组合应用到列变换上）。更直接的方法：$c_1 \\leftarrow c_1+c_2+c_3+c_4 \\implies (x+y+z+w)$ 是因子。$c_1 \\leftarrow c_1+c_2-c_3-c_4 \\implies (x+y-z-w)$ 是因子。$c_1 \\leftarrow c_1-c_2+c_3-c_4 \\implies (x-y+z-w)$ 是因子。$c_1 \\leftarrow c_1-c_2-c_3+c_4 \\implies (x-y-z+w)$ 是因子。(需要验证这些操作确实能分离出因子) 例3 (友矩阵的特征多项式相关的行列式):$F_n(\\lambda) = \\left| \\begin{array}{cccccc}\\lambda &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; a_n \\\\-1 &amp; \\lambda &amp; 0 &amp; \\cdots &amp; 0 &amp; a_{n-1} \\\\0 &amp; -1 &amp; \\lambda &amp; \\cdots &amp; 0 &amp; a_{n-2} \\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\lambda &amp; a_2 \\\\0 &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; \\lambda+a_1\\end{array} \\right|$按第一行展开: $F_n = \\lambda M_{11} + (-1)^{1+n} a_n M_{1n}$.$M_{11}$ 是一个类似的 $n-1$ 阶行列式。$M_{1n}$ 是一个下三角行列式，对角线元素都是 $-1$，所以 $M_{1n} = (-1)^{n-1}$.递推关系: $F_n = \\lambda F_{n-1}’ + (-1)^{n+1} a_n (-1)^{n-1}$ (这里 $F_{n-1}’$ 结构与 $F_{n-1}$略有不同，需要仔细看)。PPT给出的递推关系: $F_n = \\lambda F_{n-1} + (-1)^{n+1}(-1)^{n-1}a_n = \\lambda F_{n-1} + a_n$. (这似乎是 $a_n$ 符号定义不同的情况)正确的递推（按第一列展开更简单）：$F_n(\\lambda) = \\lambda \\cdot \\text{det}(\\text{submatrix for } a_{11}) - (-1) \\cdot \\text{det}(\\text{submatrix for } a_{21})$如果按第一行展开 $F_n(\\lambda) = \\lambda \\cdot \\text{det} \\begin{pmatrix} \\lambda &amp; \\dots &amp; a_{n-1} \\\\ -1 &amp; \\dots &amp; a_{n-2} \\\\ \\dots \\\\ -1 &amp; \\lambda+a_1 \\end{pmatrix}_{\\text{右上角为}(n-1)\\times(n-1)} + (-1)^{n+1} a_n \\cdot \\text{det} \\begin{pmatrix} -1 &amp; \\lambda &amp; \\dots \\\\ 0 &amp; -1 &amp; \\dots \\\\ \\dots \\\\ 0 &amp; \\dots &amp; -1 \\end{pmatrix}_{(n-1)\\times(n-1)}$$F_n(\\lambda) = \\lambda \\cdot (\\lambda^{n-1} + a_1 \\lambda^{n-2} + \\dots + a_{n-1}) + (-1)^{n+1}a_n (-1)^{n-1}$$F_n(\\lambda) = \\lambda^n + a_1 \\lambda^{n-1} + \\dots + a_{n-1}\\lambda + a_n$.这是多项式 $p(t) = t^n + a_1 t^{n-1} + \\dots + a_n$ 的友矩阵的特征行列式 $|\\lambda I - C_p|$. 1.4 Laplace 展开定理的推广 (子式与代数余子式) $k$ 阶子式: 从 $n$ 阶行列式 $D$ 中任意取定 $k$ 行 (行标 $i_1 &lt; i_2 &lt; \\dots &lt; i_k$) 和 $k$ 列 (列标 $j_1 &lt; j_2 &lt; \\dots &lt; j_k$)，位于这些行和列交叉处的 $k^2$ 个元素按原来的相对位置组成的 $k$ 阶行列式，称为 $D$ 的一个 $k$ 阶子式。记作 $D\\left(\\begin{smallmatrix} i_1 &amp; i_2 &amp; \\cdots &amp; i_k \\\\ j_1 &amp; j_2 &amp; \\cdots &amp; j_k \\end{smallmatrix}\\right)$. 余子式: 在 $D$ 中划去上述 $k$ 行和 $k$ 列后，余下的 $(n-k)^2$ 个元素按原来的相对位置组成的 $(n-k)$ 阶行列式，称为上述 $k$ 阶子式的余子式。记作 $M\\left(\\begin{smallmatrix} i_1 &amp; i_2 &amp; \\cdots &amp; i_k \\\\ j_1 &amp; j_2 &amp; \\cdots &amp; j_k \\end{smallmatrix}\\right)$. 代数余子式:$\\hat{D}\\left(\\begin{smallmatrix} i_1 &amp; i_2 &amp; \\cdots &amp; i_k \\\\ j_1 &amp; j_2 &amp; \\cdots &amp; j_k \\end{smallmatrix}\\right) = (-1)^{\\sum_{s=1}^k i_s + \\sum_{s=1}^k j_s} M\\left(\\begin{smallmatrix} i_1 &amp; i_2 &amp; \\cdots &amp; i_k \\\\ j_1 &amp; j_2 &amp; \\cdots &amp; j_k \\end{smallmatrix}\\right)$. Laplace 展开定理: 在 $n$ 阶行列式 $D$ 中，任意取定 $k$ 行 (或 $k$ 列)，由这 $k$ 行 (或 $k$ 列) 元素组成的所有 $k$ 阶子式与它们对应的代数余子式的乘积之和等于行列式 $D$ 的值。例如，按 $i_1, \\dots, i_k$ 行展开:$D = \\sum_{1 \\le j_1 &lt; j_2 &lt; \\dots &lt; j_k \\le n} D\\left(\\begin{smallmatrix} i_1 &amp; \\dots &amp; i_k \\\\ j_1 &amp; \\dots &amp; j_k \\end{smallmatrix}\\right) \\hat{D}\\left(\\begin{smallmatrix} i_1 &amp; \\dots &amp; i_k \\\\ j_1 &amp; \\dots &amp; j_k \\end{smallmatrix}\\right)$. 证明思路: 证明一个特殊情况：如果行列式是分块上三角或下三角形式 $D = \\left| \\begin{array}{cc} A_k &amp; B \\\\ O &amp; C_{n-k} \\end{array} \\right|$, 则 $D = |A_k||C_{n-k}|$.在这个特殊情况下，如果选前 $k$ 行，那么只有当 $j_1=1, \\dots, j_k=k$ 时子式 $D(\\dots)$ 非零 (如果 $B=0$)。此时 $D(\\begin{smallmatrix} 1 \\dots k \\\\ 1 \\dots k \\end{smallmatrix}) = |A_k|$ 且 $\\hat{D}(\\begin{smallmatrix} 1 \\dots k \\\\ 1 \\dots k \\end{smallmatrix}) = |C_{n-k}|$. 一般情况可以通过行变换和列变换将任意选定的 $k$ 行 $k$ 列移动到左上角，形成 $D\\left(\\begin{smallmatrix} i_1 &amp; \\dots &amp; i_k \\\\ j_1 &amp; \\dots &amp; j_k \\end{smallmatrix}\\right)$ 作为左上角块，其代数余子式对应右下角块的行列式（带符号）。 核心思想是将行列式的定义 (求和 $n!$ 项) 进行分组。每一组对应于一个特定的 $k$ 阶子式及其余子式的乘积。符号的匹配是关键。PPT中的证明思路：考虑行列式定义中 $n!$ 项的求和。公式 (2) 的每一项 $D(\\dots)\\hat{D}(\\dots)$ 展开后包含 $k!(n-k)!$ 项乘积。总共有 $\\binom{n}{k}$ 个这样的 $k$ 阶子式。所以总项数 $\\binom{n}{k} k!(n-k)! = n!$。每一项的符号需要匹配。 1.5 应用 Cramer 法则: 对于线性方程组 $AX=\\beta$，若系数行列式 $D=|A| \\neq 0$，则方程组有唯一解：$x_j = \\frac{D_j}{D}$, for $j=1, \\dots, n$.其中 $D_j$ 是将 $D$ 中第 $j$ 列替换为常数项向量 $\\beta$ 后得到的行列式。 证明: $A \\cdot \\text{adj}(A) = |A|I$.$AX=\\beta \\implies \\text{adj}(A)AX = \\text{adj}(A)\\beta \\implies |A|X = \\text{adj}(A)\\beta$.$|A|x_j = (\\text{adj}(A)\\beta)_j = \\sum_{k=1}^n (\\text{adj}(A))_{jk} \\beta_k = \\sum_{k=1}^n A_{kj} \\beta_k$.$\\sum_{k=1}^n \\beta_k A_{kj}$ 正是 $D_j$ 按第 $j$ 列展开的结果。 伴随矩阵: $A^ = \\text{adj}(A) = (A_{ji})$ (代数余子式 $A_{ji}$ 放在 $(i,j)$ 位置，即代数余子式矩阵的转置)。$AA^ = A^A = |A|I_n$.若 $|A| \\neq 0$, 则 $A^{-1} = \\frac{1}{|A|}A^$. 1.6 特殊行列式的计算 循环行列式 (Circulant Determinant):$D = \\left| \\begin{array}{cccc}a_1 &amp; a_2 &amp; \\cdots &amp; a_n \\\\a_n &amp; a_1 &amp; \\cdots &amp; a_{n-1} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\a_2 &amp; a_3 &amp; \\cdots &amp; a_1\\end{array} \\right|$令 $f(x) = a_1 + a_2x + \\dots + a_nx^{n-1}$.令 $\\varepsilon_k$ ($k=0, \\dots, n-1$) 是 $x^n=1$ 的 $n$ 个单位根。则 $D = \\prod_{k=0}^{n-1} f(\\varepsilon_k)$. 证明思路: 构造范德蒙矩阵 $V$ (列为 $(1, \\varepsilon_k, \\dots, \\varepsilon_k^{n-1})^T$)。考虑矩阵乘积 $DV$。$(DV)_{jk} = \\sum_{l=1}^n D_{jl} V_{lk} = \\sum_{l=1}^n a_{\\text{index}(j,l)} \\varepsilon_k^{l-1}$.可以证明 $(DV)_{jk} = f(\\varepsilon_k) \\varepsilon_k^{j-1}$.所以 $DV = \\text{diag}(f(\\varepsilon_0), \\dots, f(\\varepsilon_{n-1})) \\cdot V^T$ (或者类似形式，这里可能 $V$ 的行和列定义有关)。更标准的是 $CV = V \\text{diag}(f(\\varepsilon_0), \\dots, f(\\varepsilon_{n-1}))$，其中 $C$ 是循环矩阵。所以 $|C||V| = |V| \\prod f(\\varepsilon_k)$. 由于 $|V| \\neq 0$ (范德蒙行列式)，所以 $|C| = \\prod f(\\varepsilon_k)$. Cauchy-Binet 公式: (行列式的乘积)设 $A$ 是 $m \\times n$ 矩阵，$B$ 是 $n \\times m$ 矩阵。 若 $m &gt; n$: 则 $|AB| = 0$. 证明: $\\text{rank}(A) \\le n$, $\\text{rank}(B) \\le n$.$\\text{rank}(AB) \\le \\min(\\text{rank}(A), \\text{rank}(B)) \\le n &lt; m$.由于 $AB$ 是 $m \\times m$ 矩阵，其秩小于 $m$，所以 $|AB|=0$. 若 $m \\le n$: 则 $AB$ 是 $m \\times m$ 矩阵。$|AB| = \\sum_{1 \\le j_1 &lt; j_2 &lt; \\dots &lt; j_m \\le n} A\\left(\\begin{smallmatrix} 1 &amp; 2 &amp; \\cdots &amp; m \\\\ j_1 &amp; j_2 &amp; \\cdots &amp; j_m \\end{smallmatrix}\\right) B\\left(\\begin{smallmatrix} j_1 &amp; j_2 &amp; \\cdots &amp; j_m \\\\ 1 &amp; 2 &amp; \\cdots &amp; m \\end{smallmatrix}\\right)$.这里 $A(\\dots)$ 是 $A$ 取所有 $m$ 行和 $j_1, \\dots, j_m$ 列构成的 $m$ 阶子式。$B(\\dots)$ 是 $B$ 取 $j_1, \\dots, j_m$ 行和所有 $m$ 列构成的 $m$ 阶子式。 证明思路:构造分块矩阵 $C = \\begin{pmatrix} A &amp; 0 \\\\ -I_n &amp; B \\end{pmatrix}$ (这里 $I_n$ 可能是 $I_m$ 或 $I_n$ 取决于构造方式，PPT用 $I_n$)，这是一个 $(m+n) \\times (m+n)$ 阵。一方面 $|C| = |A| |B|$ (如果 $A,B$ 是方阵且这样分块)。PPT的构造是 $C = \\begin{pmatrix} A &amp; 0 \\\\ -I_n &amp; B \\end{pmatrix}$，但这里的 $A$ 是 $m \\times n$, $B$ 是 $n \\times m$。这个 $C$ 不是方阵，除非 $m=n$。更标准的构造：考虑 $M = \\begin{pmatrix} I_m &amp; A \\\\ 0 &amp; I_n \\end{pmatrix} \\begin{pmatrix} -AB &amp; 0 \\\\ B &amp; I_m \\end{pmatrix} = \\begin{pmatrix} 0 &amp; A \\\\ B &amp; I_m \\end{pmatrix}$ (这也不太对)。 正确的构造（一种）:考虑 $2m \\times 2m$ 矩阵 (假设 $n=m$ 先看 $|AB|=|A||B|$) :$\\begin{vmatrix} A &amp; 0 \\\\ -I &amp; B \\end{vmatrix} = |A||B|$.通过 $R_i \\leftarrow R_i + \\sum_k a_{ik} R_{m+k}$ (对 $i=1 \\dots m$) 可以变换为 $\\begin{vmatrix} A &amp; AB \\\\ -I &amp; 0 \\end{vmatrix} = (-1)^{m^2} |-I||AB| = |AB|$. 对于 Cauchy-Binet，PPT 的提示是构造 $C = \\begin{pmatrix} A &amp; 0 \\\\ -I_n &amp; B \\end{pmatrix}$，然后用 Laplace 展开。另一种思路是利用 Grassman 代数 (外代数)。一个更初等的证明涉及到将 $A$ 的列向量和 $B$ 的行向量展开，然后利用行列式的多线性性质。 子式的乘积公式 (推广的Cauchy-Binet):设 $A$ 是 $m \\times n$ 矩阵, $B$ 是 $n \\times p$ 矩阵。 $C=AB$ 是 $m \\times p$ 矩阵。取 $C$ 的任意 $r \\times r$ 子式 $C\\left(\\begin{smallmatrix} i_1 \\dots i_r \\\\ j_1 \\dots j_r \\end{smallmatrix}\\right)$.$C\\left(\\begin{smallmatrix} i_1 \\dots i_r \\\\ j_1 \\dots j_r \\end{smallmatrix}\\right) = \\sum_{1 \\le k_1 &lt; \\dots &lt; k_r \\le n} A\\left(\\begin{smallmatrix} i_1 \\dots i_r \\\\ k_1 \\dots k_r \\end{smallmatrix}\\right) B\\left(\\begin{smallmatrix} k_1 \\dots k_r \\\\ j_1 \\dots j_r \\end{smallmatrix}\\right)$.这个公式表明乘积矩阵的任一子式等于第一个矩阵的相应行构成的所有同阶子式与第二个矩阵的相应列构成的所有同阶子式的对应乘积之和。如果 $r &gt; n$，则右边没有 $k_1 &lt; \\dots &lt; k_r$ 的组合，和为0 (子式为0)。 Lagrange 恒等式:$(\\sum_{i=1}^n a_i^2)(\\sum_{i=1}^n b_i^2) - (\\sum_{i=1}^n a_ib_i)^2 = \\sum_{1 \\le i &lt; j \\le n} (a_ib_j - a_jb_i)^2$. 证明: 可以看作是 Cauchy-Binet 公式在 $m=2$ 时的特殊情况。令 $A = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\dots &amp; a_n \\\\ b_1 &amp; b_2 &amp; \\dots &amp; b_n \\end{pmatrix}$ (2xN矩阵)。则 $AA^T = \\begin{pmatrix} \\sum a_i^2 &amp; \\sum a_ib_i \\\\ \\sum a_ib_i &amp; \\sum b_i^2 \\end{pmatrix}$.$|AA^T| = (\\sum a_i^2)(\\sum b_i^2) - (\\sum a_ib_i)^2$.根据 Cauchy-Binet: $|AA^T| = \\sum_{1 \\le i &lt; j \\le n} A\\left(\\begin{smallmatrix} 1 &amp; 2 \\\\ i &amp; j \\end{smallmatrix}\\right) (A^T)\\left(\\begin{smallmatrix} i &amp; j \\\\ 1 &amp; 2 \\end{smallmatrix}\\right)$.$A\\left(\\begin{smallmatrix} 1 &amp; 2 \\\\ i &amp; j \\end{smallmatrix}\\right) = \\left| \\begin{array}{cc} a_i &amp; a_j \\\\ b_i &amp; b_j \\end{array} \\right| = a_ib_j - a_jb_i$.$(A^T)\\left(\\begin{smallmatrix} i &amp; j \\\\ 1 &amp; 2 \\end{smallmatrix}\\right) = \\left| \\begin{array}{cc} a_i &amp; b_i \\\\ a_j &amp; b_j \\end{array} \\right|$ (这是 $A^T$ 的子式，即取 $A^T$ 的第 $i,j$ 行和第 $1,2$ 列)$= a_ib_j - a_jb_i$.所以 $|AA^T| = \\sum_{1 \\le i &lt; j \\le n} (a_ib_j - a_jb_i)^2$. 此恒等式也与 Cauchy-Schwarz 不等式相关。 1.7 Jacobian 行列式 在多变量微积分中，用于变量替换时的体积/面积缩放因子。如果 $y_k = f_k(x_1, \\dots, x_n)$, 则 Jacobian 行列式为 $J = \\left| \\frac{\\partial y_i}{\\partial x_j} \\right|$. 2. 分块矩阵 (Block Matrices)2.1 分块矩阵的运算 加法: 对应分块相加 (要求分块方式一致)。 数乘: 数乘每一个子块。 乘法: 类似普通矩阵乘法规则，但块的乘积需要有意义 (列数=行数)，且块的顺序不能交换。$\\begin{pmatrix} A &amp; B \\\\ C &amp; D \\end{pmatrix} \\begin{pmatrix} X &amp; Y \\\\ Z &amp; W \\end{pmatrix} = \\begin{pmatrix} AX+BZ &amp; AY+BW \\\\ CX+DZ &amp; CY+DW \\end{pmatrix}$. 转置: $(A_{ij})^T = (A_{ji}^T)$. 对整个块矩阵转置，然后对每个子块转置。 2.2 分块对角矩阵 $A = \\text{diag}(A_1, A_2, \\dots, A_k) = \\begin{pmatrix} A_1 &amp; &amp; &amp; O \\\\ &amp; A_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ O &amp; &amp; &amp; A_k \\end{pmatrix}$. 行列式: $|A| = |A_1||A_2|\\cdots|A_k|$. 逆 (如果存在): $A^{-1} = \\text{diag}(A_1^{-1}, A_2^{-1}, \\dots, A_k^{-1})$. (要求每个 $A_i$ 都可逆)。 幂: $A^p = \\text{diag}(A_1^p, \\dots, A_k^p)$. 2.3 分块上/下三角矩阵 $A = \\begin{pmatrix} A_{11} &amp; A_{12} &amp; \\cdots &amp; A_{1k} \\\\ &amp; A_{22} &amp; \\cdots &amp; A_{2k} \\\\ &amp; &amp; \\ddots &amp; \\vdots \\\\ O &amp; &amp; &amp; A_{kk} \\end{pmatrix}$ (分块上三角)。 行列式: $|A| = |A_{11}||A_{22}|\\cdots|A_{kk}|$. (要求对角块 $A_{ii}$ 都是方阵)。 证明思路: 可以通过一系列的分块初等行变换（不改变行列式的值，或只改变符号）将非对角块消去，或者用数学归纳法。 逆 (如果 $A_{ii}$ 都可逆): 逆矩阵也是分块上/下三角形式。例如 $2 \\times 2$ 分块: $\\begin{pmatrix} A &amp; B \\\\ O &amp; D \\end{pmatrix}^{-1} = \\begin{pmatrix} A^{-1} &amp; -A^{-1}BD^{-1} \\\\ O &amp; D^{-1} \\end{pmatrix}$. (要求 $A,D$ 可逆)。 证明: $\\begin{pmatrix} A &amp; B \\\\ O &amp; D \\end{pmatrix} \\begin{pmatrix} X &amp; Y \\\\ Z &amp; W \\end{pmatrix} = \\begin{pmatrix} I &amp; O \\\\ O &amp; I \\end{pmatrix}$.$AZ=O \\implies Z=O$ (若 $A$ 可逆)。$AX=I \\implies X=A^{-1}$.$DW=I \\implies W=D^{-1}$.$AY+BW=O \\implies AY = -BW = -BD^{-1} \\implies Y = -A^{-1}BD^{-1}$. 2.4 行列式的分块公式 (Schur Complement)设 $A, B, C, D$ 分别是 $p \\times p, p \\times q, q \\times p, q \\times q$ 矩阵。 若 $A$ 可逆:$\\left| \\begin{array}{cc} A &amp; B \\\\ C &amp; D \\end{array} \\right| = |A| |D - CA^{-1}B|$.$D - CA^{-1}B$ 称为 $A$ 在原矩阵中的 Schur 补。 证明:$\\begin{pmatrix} I &amp; O \\\\ -CA^{-1} &amp; I \\end{pmatrix} \\begin{pmatrix} A &amp; B \\\\ C &amp; D \\end{pmatrix} = \\begin{pmatrix} A &amp; B \\\\ O &amp; D-CA^{-1}B \\end{pmatrix}$.左边第一个矩阵行列式为1。所以原行列式等于右边矩阵的行列式，即 $|A||D-CA^{-1}B|$. 若 $D$ 可逆:$\\left| \\begin{array}{cc} A &amp; B \\\\ C &amp; D \\end{array} \\right| = |D| |A - BD^{-1}C|$.$A - BD^{-1}C$ 称为 $D$ 在原矩阵中的 Schur 补。 证明: 类似地，用 $\\begin{pmatrix} I &amp; -BD^{-1} \\\\ O &amp; I \\end{pmatrix}$ 左乘。 重要恒等式 (若 $A,D$ 均可逆):$|A||D-CA^{-1}B| = |D||A-BD^{-1}C|$.这可以用于某些行列式计算技巧。例: 计算 $G = \\begin{pmatrix} a_1^2 &amp; a_1a_2+1 &amp; \\cdots \\\\ a_2a_1+1 &amp; a_2^2 &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{pmatrix}$. (PPT中 $a_i a_j+1$ 不在对角线)。若 $G = \\text{diag}(a_1^2, \\dots, a_n^2) - \\mathbf{1}\\mathbf{1}^T + (\\mathbf{a}\\mathbf{a}^T - \\text{diag}(a_i^2))$ (假设是 $a_ia_j + \\delta_{ij}$ 这种形式)。PPT中的例子 $G_{ij} = a_i a_j + \\delta_{ij}$ (这是 $I + \\mathbf{a}\\mathbf{a}^T$)。$|I + \\mathbf{a}\\mathbf{a}^T| = 1 + \\mathbf{a}^T\\mathbf{a} = 1 + \\sum a_i^2$.(使用公式 $|I_n + uv^T| = 1+v^Tu$). PPT中的 $G = \\begin{pmatrix} -I_n &amp; \\mathbf{a} \\\\ \\mathbf{a}^T &amp; -1 \\end{pmatrix}$ (这是错误的分解，PPT中是 $G_{ij} = a_i a_j$ 如果 $i \\ne j$ 且 $G_{ii} = a_i^2$)它给的例子 $G_{ij} = a_ia_j + \\delta_{ij}$。$|G| = |I_n + \\mathbf{a}\\mathbf{a}^T| = 1 + \\mathbf{a}^T I_n^{-1} \\mathbf{a} = 1 + \\sum a_i^2$. (使用Sylvester行列式定理的特例)PPT的推导似乎是想用一个更复杂的Schur补形式，或者对 $G = \\begin{pmatrix} -E_n &amp; X \\\\ Y &amp; E_m \\end{pmatrix}$ 的某种变换。它给的 $G = aa^T - I + J$ (其中 $J$ 是全1矩阵， $a=(a_1, \\dots, a_n)^T$)$G_{ij} = a_i a_j - \\delta_{ij} + 1$. (这与前面的 $a_ia_j+\\delta_{ij}$ 不一样)。PPT中的 $G$ 的具体形式及其计算需要仔细核对原题。最终的例子是 $A_{ij} = a_ia_j + \\delta_{ij}$，则 $|A| = 1 + \\sum a_k^2$. 2.5 矩阵秩的不等式 $r(AB) \\le \\min \\{r(A), r(B)\\}$. 证明: $AB$ 的列向量是 $A$ 的列向量的线性组合，所以 $C(AB) \\subseteq C(A) \\implies r(AB) \\le r(A)$.$AB$ 的行向量是 $B$ 的行向量的线性组合，所以 $R(AB) \\subseteq R(B) \\implies r(AB) \\le r(B)$. $r(A+B) \\le r(A) + r(B)$ (实际上 $r(A+B) \\le r((A|B))$ 进一步 $\\le r(A)+r(B)$)。 证明: $C(A+B) \\subseteq C(A) + C(B)$.$\\text{dim}(C(A)+C(B)) = \\text{dim}C(A) + \\text{dim}C(B) - \\text{dim}(C(A)\\cap C(B)) \\le r(A)+r(B)$. Frobenius 不等式: $r(AB) + r(BC) \\le r(B) + r(ABC)$ (不在此PPT范围，但相关)。 Sylvester 秩不等式: 若 $A$ 是 $m \\times n$ 矩阵, $B$ 是 $n \\times p$ 矩阵, 则$r(A) + r(B) - n \\le r(AB)$. 证明:考虑线性映射 $T_B: R^p \\to R^n, X \\mapsto BX$. $T_A: R^n \\to R^m, Y \\mapsto AY$.$r(AB) = \\text{dim Im}(T_A|_{Im(T_B)})$.$\\text{dim Im}(T_A|_{Im(T_B)}) = \\text{dim Im}(T_B) - \\text{dim}(\\text{Ker}(T_A) \\cap \\text{Im}(T_B))$.$r(AB) = r(B) - \\text{dim}(\\text{Ker}(T_A) \\cap \\text{Im}(T_B))$.因为 $\\text{Ker}(T_A) \\cap \\text{Im}(T_B) \\subseteq \\text{Ker}(T_A)$, 所以 $\\text{dim}(\\text{Ker}(T_A) \\cap \\text{Im}(T_B)) \\le \\text{dim Ker}(T_A) = n - r(A)$.$r(AB) \\ge r(B) - (n-r(A)) = r(A)+r(B)-n$. $r\\begin{pmatrix} A &amp; O \\\\ O &amp; B \\end{pmatrix} = r(A) + r(B)$. $r\\begin{pmatrix} A &amp; C \\\\ O &amp; B \\end{pmatrix} \\ge r(A) + r(B)$. (通常是等于，如果通过初等变换可以消掉 $C$)实际上是 $r\\begin{pmatrix} A &amp; C \\\\ O &amp; B \\end{pmatrix} \\ge r(A) + r(B)$. 等号成立当且仅当 $C$ 的列可以由 $A$ 的列和 $B$ 的行的某种方式生成（较复杂条件）。如果 $A,B$ 是方阵，则行列式是 $|A||B|$，秩是 $r(A)+r(B)$ 当且仅当 $A,B$ 的线性无关的行/列能组合成整个矩阵的秩。PPT中的证明 (Sylvester):$r(A) + r(B) \\le r\\begin{pmatrix} A &amp; 0 \\\\ -I &amp; B \\end{pmatrix}$ (通过变换得到右边矩阵的秩).$\\begin{pmatrix} A &amp; 0 \\\\ -I &amp; B \\end{pmatrix} \\xrightarrow{\\text{row ops}} \\begin{pmatrix} 0 &amp; AB \\\\ -I &amp; B \\end{pmatrix} \\xrightarrow{\\text{col ops}} \\begin{pmatrix} 0 &amp; AB \\\\ -I &amp; 0 \\end{pmatrix}$.后者的秩为 $r(AB)+n$.所以 $r(A)+r(B) \\le r(AB)+n$. (这基于对秩的变换性质) 2.6 伴随矩阵的秩设 $A$ 是 $n \\times n$ 矩阵, $A^*$ 是其伴随矩阵。 若 $r(A) = n$, 则 $|A| \\neq 0$, $A^{*}$可逆, $r(A^{*}) = n$. 若 $r(A) = n-1$, 则 $|A|=0$. 此时 $A$ 至少有一个 $(n-1)$ 阶子式非零，所以 $A^{*} \\neq O$, $r(A^{*}) \\ge 1$.又因为 $AA^{*} = |A|I = O$. 由 Sylvester 不等式 $r(A)+r(A^{*})-n \\le r(AA^{*}) = 0$.$(n-1) + r(A^{*}) - n \\le 0 \\implies r(A^{*}) - 1 \\le 0 \\implies r(A^{*}) \\le 1$.所以 $r(A^{*}) = 1$. 若 $r(A) &lt; n-1$, 则 $A$ 的所有 $(n-1)$ 阶子式均为0 (因为若有非零的，秩至少为 $n-1$)。所以 $A^{*}$ 的所有元素均为0, $A^{*}=O$, $r(A^{*})=0$. 2.7 幂等矩阵的秩若 $A^2=A$ ( $A$ 是幂等矩阵), 则 $r(A) + r(I-A) = n$. 证明: $r(I) = r(A + (I-A))$.$n = r(A + (I-A)) \\le r(A) + r(I-A)$ (子空间和的维数). $A(I-A) = A - A^2 = A - A = O$.由 Sylvester 不等式: $r(A) + r(I-A) - n \\le r(A(I-A)) = r(O) = 0$.$r(A) + r(I-A) \\le n$.结合1和2，得 $r(A) + r(I-A) = n$. 3. 广义逆矩阵 (Generalized Inverse Matrix) $A^-$3.1 定义对于 $m \\times n$ 矩阵 $A$，若存在 $n \\times m$ 矩阵 $G$ 使得 $AGA=A$，则称 $G$ 是 $A$ 的一个 广义逆 (或 减号逆、g-逆)，记作 $A^-$. 广义逆不唯一 (除非 $A$ 可逆，此时 $A^- = A^{-1}$ 是唯一的)。 3.2 存在性与构造 定理: 任何矩阵 $A$ 都存在广义逆。 证明 (基于满秩分解):设 $r(A)=r &gt; 0$. 则 $A$ 可以满秩分解为 $A=LR$，其中 $L$ 是 $m \\times r$ 列满秩矩阵，$R$ 是 $r \\times n$ 行满秩矩阵。由于 $L$ 列满秩，$L^TL$ 可逆。由于 $R$ 行满秩，$RR^T$ 可逆。存在 $L_L^{-1}$ (左逆) 使得 $L_L^{-1}L = I_r$，例如 $L_L^{-1} = (L^TL)^{-1}L^T$.存在 $R_R^{-1}$ (右逆) 使得 $RR_R^{-1} = I_r$，例如 $R_R^{-1} = R^T(RR^T)^{-1}$.令 $G = R_R^{-1} L_L^{-1} = R^T(RR^T)^{-1}(L^TL)^{-1}L^T$.则 $AGA = L R (R^T(RR^T)^{-1}(L^TL)^{-1}L^T) L R = L (RR^T(RR^T)^{-1}) ((L^TL)^{-1}L^TL) R = L I_r I_r R = LR = A$. 证明 (基于等价标准形): (PPT中的方法)存在可逆矩阵 $P, Q$ 使得 $A = P \\begin{pmatrix} I_r &amp; O \\\\ O &amp; O \\end{pmatrix} Q$.令 $G = Q^{-1} \\begin{pmatrix} I_r &amp; X \\\\ Y &amp; Z \\end{pmatrix} P^{-1}$，其中 $X, Y, Z$ 是任意适当大小的矩阵块。则 $AGA = P \\begin{pmatrix} I_r &amp; O \\\\ O &amp; O \\end{pmatrix} Q Q^{-1} \\begin{pmatrix} I_r &amp; X \\\\ Y &amp; Z \\end{pmatrix} P^{-1} P \\begin{pmatrix} I_r &amp; O \\\\ O &amp; O \\end{pmatrix} Q$$= P \\begin{pmatrix} I_r &amp; O \\\\ O &amp; O \\end{pmatrix} \\begin{pmatrix} I_r &amp; X \\\\ Y &amp; Z \\end{pmatrix} \\begin{pmatrix} I_r &amp; O \\\\ O &amp; O \\end{pmatrix} Q$$= P \\begin{pmatrix} I_r &amp; X \\\\ O &amp; O \\end{pmatrix} \\begin{pmatrix} I_r &amp; O \\\\ O &amp; O \\end{pmatrix} Q = P \\begin{pmatrix} I_r &amp; O \\\\ O &amp; O \\end{pmatrix} Q = A$.所以，形如 $G = Q^{-1} \\begin{pmatrix} I_r &amp; X \\\\ Y &amp; Z \\end{pmatrix} P^{-1}$ 的矩阵都是 $A$ 的广义逆。最简单的一个是取 $X,Y,Z$ 为零矩阵，得到 $A^- = Q^{-1} \\begin{pmatrix} I_r &amp; O \\\\ O &amp; O \\end{pmatrix} P^{-1}$. 3.3 广义逆与线性方程组 $AX=\\beta$ 定理 (相容性条件): 线性方程组 $AX=\\beta$ 有解 (相容) 的充要条件是 $AA^-\\beta = \\beta$ 对任意一个 $A^-$ 成立。(注意：更常用的条件是 $\\beta \\in C(A)$, 或者 $r(A)=r(A|\\beta)$)。这里的 $AA^-\\beta = \\beta$ 是指如果方程有解，那么这个等式成立。反过来，如果这个等式对某个 $A^-$ 成立，则方程有解。 证明:($\\Rightarrow$) 若 $AX_0=\\beta$ 有解。则 $AA^- \\beta = AA^- AX_0 = AX_0 = \\beta$.($\\Leftarrow$) 若 $AA^-\\beta = \\beta$. 令 $X_0 = A^-\\beta$. 则 $AX_0 = AA^-\\beta = \\beta$. 所以 $A^-\\beta$ 是一个解。 通解: 若 $AX=\\beta$ 相容，则其通解为 $X = A^-\\beta + (I - A^-A)Z$，其中 $A^-$ 是 $A$ 的任一广义逆，$Z$ 是任意的 $n \\times 1$ 向量。 证明: 验证 $A^-\\beta + (I - A^-A)Z$ 是解:$A(A^-\\beta + (I - A^-A)Z) = AA^-\\beta + A(I - A^-A)Z = \\beta + (A - AA^-A)Z = \\beta + (A-A)Z = \\beta$. 验证所有解都可以表为此形式:设 $X_s$ 是任一特解。则 $AX_s=\\beta$.$X_s = A^-\\beta + (X_s - A^-\\beta)$.我们希望 $X_s - A^-\\beta$ 能表示为 $(I-A^-A)Z$ 的形式。$A(X_s - A^-\\beta) = AX_s - AA^-\\beta = \\beta - \\beta = 0$.所以 $X_s - A^-\\beta$ 是齐次方程 $AY=0$ 的解。我们需要证明齐次方程的解空间是 $\\{ (I-A^-A)Z | Z \\in R^n \\}$.若 $AY_0=0$. 令 $Z=Y_0$. 则 $(I-A^-A)Y_0 = Y_0 - A^-AY_0 = Y_0 - A^-0 = Y_0$.所以任何齐次解 $Y_0$ 都可以写成 $(I-A^-A)Y_0$.因此 $X_s = A^-\\beta + (I-A^-A)(X_s-A^-\\beta_{any_particular})$ (这里需要更细致的论证， $I-A^-A$ 是投影到 $N(A)$ 上的算子)。实际上， $A(I-A^-A)Z = (A-AA^-A)Z = (A-A)Z = 0$. 所以 $(I-A^-A)Z$ 确实是齐次方程的解。并且 $N(A) = \\text{Im}(I-A^-A)$. 3.4 广义逆的一般形式 定理: 若 $A^-_0$ 是 $A$ 的一个广义逆，则 $A$ 的所有广义逆可以表示为：$A^- = A^-_0 + V - A^-_0AVAA^-_0$，其中 $V$ 是任意的 $n \\times m$ 矩阵。 证明:首先验证这种形式的 $A^-$ 满足 $A(A^-_0 + V - A^-_0AVAA^-_0)A = A$.$A(A^-_0 + V - A^-_0AVAA^-_0)A = AA^-_0A + AVA - AA^-_0AVAA^-_0A$$= A + AVA - (AA^-_0A)V(AA^-_0A) = A + AVA - AVA = A$.反过来，若 $G$ 是任一广义逆，即 $AGA=A$.令 $V = G - A^-_0$. 我们要说明 $G$ 可以写成上述形式。$A^-_0 + (G-A^-_0) - A^-_0A(G-A^-_0)AA^-_0 = G - A^-_0AGAA^-_0 + A^-_0AA^-_0AA^-_0$.这与PPT的形式 $A^- = A_0^- + V - A_0^- A V A A_0^-$ 不同，PPT的应该是 $A_0^- + V - A_0^- A V A A_0^-$.这里的 $A^-_0AVAA^-_0$ 应该是 $A_0^- A V A A_0^-$.PPT公式: $G = A_0^- + V - A_0^- A V A A_0^-$.如果 $V$ 是任意的，那么 $A_0^-AVAA_0^-$ 这项似乎可以被 $V$ 吸收。更常见的表达是: $A^- = A_0^- + (I-A_0^-A)U + W(I-AA_0^-)$ 其中 $U,W$ 任意。PPT的公式 $G = A^- + V - A^-AVAA^-$ (假设 $A^-$ 是一个已知的g-逆)。$AGA = A(A^- + V - A^-AVAA^-)A = AA^-A + AVA - A(A^-AVAA^-)A$$= A + AVA - (AA^-A)V(AA^-A) = A + AVA - AVA = A$.若 $G_1, G_2$ 都是 $A$ 的g-逆。则 $G_1 = G_2 + (G_1-G_2) - G_2A(G_1-G_2)AG_2$ (将 $V=G_1-G_2$ 代入)。这表明任何一个g-逆可以由另一个g-逆通过这种形式得到。 Moore-Penrose 伪逆 ($A^+$): 是一种特殊的广义逆，它唯一存在且满足四个Penrose条件: $AGA=A$ $GAG=G$ $(AG)^*=AG$ (AG是Hermitian的) $(GA)^*=GA$ (GA是Hermitian的) 这部分PPT主要集中在行列式的理论和计算，以及分块矩阵的运算和行列式性质，最后简要介绍了广义逆的概念和基本性质。","link":"/MATH1408/Chapter3-MATH1408.html"},{"title":"Chapter5-1 MATH1408","text":"矩阵多项式与Jordan标准型1. 矩阵多项式 (Matrix Polynomial) 与零化多项式 (Annihilating Polynomial)1.1 矩阵多项式 定义: 设 $f(x) = c_m x^m + c_{m-1} x^{m-1} + \\dots + c_1 x + c_0$ 是一个数域 $F$ 上的多项式。对于 $n$ 阶方阵 $A$，定义矩阵多项式为：$f(A) = c_m A^m + c_{m-1} A^{m-1} + \\dots + c_1 A + c_0 E$其中 $E$ 是 $n$ 阶单位矩阵。 性质: 若 $f(x) = g(x) + h(x)$，则 $f(A) = g(A) + h(A)$。 若 $f(x) = g(x)h(x)$，则 $f(A) = g(A)h(A)$。 由于矩阵乘法一般不可交换，所以 $g(A)h(A)$ 通常不等于 $h(A)g(A)$。但如果 $g(x)$ 和 $h(x)$ 是关于 $x$ 的多项式，则 $g(A)$ 和 $h(A)$ 是可交换的，因为它们都是 $A$ 的幂和单位矩阵的线性组合。 1.2 零化多项式 (Annihilating Polynomial / Nullifying Polynomial) 定义: 设 $A$ 是一个 $n$ 阶方阵， $f(x)$ 是一个数域 $F$ 上的多项式。如果 $f(A) = O$ (零矩阵)，则称 $f(x)$ 是矩阵 $A$ 的一个零化多项式。 存在性: 对于任意 $n$ 阶方阵 $A$，其零化多项式总是存在的。 考虑矩阵序列 $E, A, A^2, \\dots, A^{n^2}$。这些是 $n^2+1$ 个 $n \\times n$ 矩阵。 $n \\times n$ 矩阵构成的线性空间 $M_n(F)$ 的维数是 $n^2$。 因此，上述 $n^2+1$ 个矩阵在线性空间 $M_n(F)$ 中必定线性相关。 即存在不全为零的标量 $c_0, c_1, \\dots, c_{n^2}$ 使得 $c_0 E + c_1 A + \\dots + c_{n^2} A^{n^2} = O$。 令 $f(x) = c_{n^2} x^{n^2} + \\dots + c_1 x + c_0$，则 $f(x)$ 是 $A$ 的一个非零零化多项式。 (Cayley-Hamilton 定理给出了一个次数最高为 $n$ 的零化多项式)。 若 $B$ 是可逆矩阵，$f(x)$ 是 $A$ 的零化多项式，则 $f(x)$ 也是 $B^{-1}AB$ 的零化多项式。 证明: $f(B^{-1}AB) = B^{-1}f(A)B = B^{-1}OB = O$。 2. Cayley-Hamilton 定理 (Cayley-Hamilton Theorem) 定理叙述: 每个方阵 $A$ 都是其自身特征多项式的根。即，若 $f_A(\\lambda) = |\\lambda E - A| = \\lambda^n + a_{n-1}\\lambda^{n-1} + \\dots + a_1\\lambda + a_0$ 是 $A$ 的特征多项式，则 $f_A(A) = A^n + a_{n-1}A^{n-1} + \\dots + a_1A + a_0E = O$。 证明思路:令 $B(\\lambda) = \\text{adj}(\\lambda E - A)$ 是 $\\lambda E - A$ 的伴随矩阵。伴随矩阵的元素是 $\\lambda E - A$ 的代数余子式，它们是关于 $\\lambda$ 的次数不超过 $n-1$ 的多项式。因此， $B(\\lambda)$ 可以表示为：$B(\\lambda) = B_{n-1}\\lambda^{n-1} + B_{n-2}\\lambda^{n-2} + \\dots + B_1\\lambda + B_0$其中 $B_j$ 是 $n$ 阶常数矩阵 (其元素是 $A$ 中元素的某些组合)。根据伴随矩阵的性质：$(\\lambda E - A) \\text{adj}(\\lambda E - A) = |\\lambda E - A| E$。即 $(\\lambda E - A) B(\\lambda) = f_A(\\lambda) E$。$(\\lambda E - A)(B_{n-1}\\lambda^{n-1} + \\dots + B_0) = (\\lambda^n + a_{n-1}\\lambda^{n-1} + \\dots + a_0)E$展开左边：$\\lambda B_{n-1}\\lambda^{n-1} + \\lambda B_{n-2}\\lambda^{n-2} + \\dots + \\lambda B_0 - A B_{n-1}\\lambda^{n-1} - A B_{n-2}\\lambda^{n-2} - \\dots - A B_0$$= B_{n-1}\\lambda^n + (B_{n-2} - A B_{n-1})\\lambda^{n-1} + \\dots + (B_0 - A B_1)\\lambda - A B_0$比较两边 $\\lambda$ 的同次幂的系数矩阵： $\\lambda^n$: $B_{n-1} = E$ $\\lambda^{n-1}$: $B_{n-2} - A B_{n-1} = a_{n-1}E$ … $\\lambda^1$: $B_0 - A B_1 = a_1E$ $\\lambda^0$: $- A B_0 = a_0E$ 将上述等式依次左乘 $A^n, A^{n-1}, \\dots, A, E$： $A^n B_{n-1} = A^n E$ $A^{n-1}(B_{n-2} - A B_{n-1}) = A^{n-1} a_{n-1}E \\Rightarrow A^{n-1}B_{n-2} - A^n B_{n-1} = a_{n-1}A^{n-1}$ … $A(B_0 - A B_1) = A a_1E \\Rightarrow AB_0 - A^2 B_1 = a_1A$ $E(- A B_0) = E a_0E \\Rightarrow -A B_0 = a_0E$ 将这些等式全部相加，左边的项会成对抵消：$(A^n B_{n-1}) + (A^{n-1}B_{n-2} - A^n B_{n-1}) + \\dots + (AB_0 - A^2 B_1) + (-A B_0)$$= A^n E + a_{n-1}A^{n-1} + \\dots + a_1A + a_0E$左边化简后为 $0$。 （这里有个更简洁的看法：直接在 $B_{n-1} = E, B_{k-1} - AB_k = a_k E, -AB_0 = a_0E$ 中，将 $A$ “代入” $\\lambda$）更直接地，将 $B_{n-1}=E$ 代入第二个式子得 $B_{n-2} - A = a_{n-1}E$, 即 $B_{n-2} = A + a_{n-1}E$。代入 $B_{k-1} = A B_k + a_k E$ 的关系。将 $(\\lambda E - A) B(\\lambda) = f_A(\\lambda) E$ 这个关于多项式矩阵的恒等式，直接看作一个“值”为 $A$ 的替换 (虽然严格来说不能直接代入矩阵，但可以通过构造一个以矩阵为系数的多项式环来理解，或者通过元素来证明)。$f_A(A) = A^n + a_{n-1}A^{n-1} + \\dots + a_1A + a_0E = O$。□ 注意: 不能简单地在 $|\\lambda E - A|=0$ 中直接令 $\\lambda = A$ 得到 $|A \\cdot E - A| = |O| = 0$。行列式是一个数，而 $f_A(A)$ 是一个矩阵。 3. 最小多项式 (Minimal Polynomial) 定义: 设 $A$ 是一个 $n$ 阶方阵。在 $A$ 的所有非零零化多项式中，次数最低且首项系数为1的多项式称为 $A$ 的最小多项式，记为 $m_A(x)$ 或 $m(x)$。 性质: 唯一性: 对任意方阵 $A$，其最小多项式是唯一的。 证明: 假设 $m_1(x)$ 和 $m_2(x)$ 都是 $A$ 的最小多项式。则它们的次数相同，首项系数都为1。令 $d(x) = m_1(x) - m_2(x)$。则 $d(A) = m_1(A) - m_2(A) = O - O = O$。如果 $d(x) \\ne 0$，则 $\\text{deg}(d(x)) &lt; \\text{deg}(m_1(x))$ (因为首项抵消)。如果 $d(x) \\ne 0$，可以将 $d(x)$ 除以其首项系数得到一个首项为1的零化多项式，其次数严格小于最小多项式的次数，矛盾。因此 $d(x) = 0$，即 $m_1(x) = m_2(x)$。□ 整除性: 任何 $A$ 的零化多项式 $g(x)$ 都能被 $A$ 的最小多项式 $m_A(x)$ 整除。即，若 $g(A)=O$，则 $m_A(x) | g(x)$。 证明: 用 $g(x)$ 除以 $m_A(x)$，得 $g(x) = q(x)m_A(x) + r(x)$，其中 $r(x)=0$ 或 $\\text{deg}(r(x)) &lt; \\text{deg}(m_A(x))$。代入 $A$：$g(A) = q(A)m_A(A) + r(A)$。因为 $g(A)=O$ 且 $m_A(A)=O$，所以 $r(A)=O$。如果 $r(x) \\ne 0$，则 $r(x)$ 是一个次数低于 $m_A(x)$ 的零化多项式。将 $r(x)$ 首项系数化为1后，就得到了一个次数更低的零化多项式，与 $m_A(x)$ 的最小性矛盾。因此 $r(x) = 0$，即 $m_A(x)$ 整除 $g(x)$。□ 特征多项式与最小多项式关系: $A$ 的最小多项式 $m_A(x)$ 整除其特征多项式 $f_A(x)$。 证明: 由 Cayley-Hamilton 定理，$f_A(A)=O$，所以 $f_A(x)$ 是 $A$ 的一个零化多项式。根据性质2，$m_A(x) | f_A(x)$。□ 根的性质: $A$ 的最小多项式 $m_A(x)$ 与特征多项式 $f_A(x)$ 有相同的根 (即 $A$ 的所有特征值)，只是重数可能不同。 证明:设 $\\lambda_0$ 是 $A$ 的一个特征值，$m_A(x)$ 是最小多项式。$m_A(A)=O$。若 $\\alpha$ 是对应于 $\\lambda_0$ 的特征向量，则 $m_A(A)\\alpha = O\\alpha = 0$。又 $m_A(A)\\alpha = m_A(\\lambda_0)\\alpha$ (因为 $A^k \\alpha = \\lambda_0^k \\alpha$)。所以 $m_A(\\lambda_0)\\alpha = 0$。因为 $\\alpha \\ne 0$，所以 $m_A(\\lambda_0) = 0$。这表明 $A$ 的每个特征值都是 $m_A(x)$ 的根。反之，若 $\\mu$ 是 $m_A(x)$ 的根，即 $m_A(\\mu)=0$。则 $m_A(x) = (x-\\mu)q(x)$。由于 $m_A(x)|f_A(x)$，所以 $(x-\\mu)|f_A(x)$，即 $\\mu$ 也是特征多项式的根，从而是 $A$ 的特征值。□ 相似不变量: 相似矩阵有相同的最小多项式。 证明: 设 $A \\sim C$，即 $C = P^{-1}AP$。$m_A(C) = m_A(P^{-1}AP) = P^{-1}m_A(A)P = P^{-1}OP = O$。所以 $m_A(x)$ 是 $C$ 的一个零化多项式。因此 $m_C(x) | m_A(x)$。同理， $m_B(x)$ 是 $A$ 的一个零化多项式，所以 $m_A(x) | m_C(x)$。由于它们都是首1多项式，所以 $m_A(x) = m_C(x)$。□ 对角化条件: 矩阵 $A$ 可对角化的充要条件是其最小多项式 $m_A(x)$ 无重根。即 $m_A(x) = (x-\\lambda_1)(x-\\lambda_2)\\dots(x-\\lambda_s)$，其中 $\\lambda_i$ 互不相同。(这个定理非常重要，但证明较复杂，通常在更高级的线性代数中给出)。 计算最小多项式:由于 $m_A(x) | f_A(x)$ 且它们有相同的根，所以如果 $f_A(x) = \\prod (x-\\lambda_i)^{k_i}$，则 $m_A(x) = \\prod (x-\\lambda_i)^{r_i}$，其中 $1 \\le r_i \\le k_i$。可以通过尝试 $f_A(x)$ 的不同次数的因子来找到满足 $g(A)=O$ 且次数最低的那个。 4. 线性变换的零化多项式与不变子空间(这部分内容在原始PPT中似乎用更抽象的线性变换语言描述，我尝试对应到矩阵上) B-不变子空间 (B-invariant subspace): 设 $B$ 是一个线性变换 (或矩阵)。子空间 $W$ 称为 $B$-不变的，如果对任意 $\\alpha \\in W$，都有 $B(\\alpha) \\in W$ (或 $B\\alpha \\in W$)。 向量 $\\alpha$ 关于变换 $B$ 的零化多项式 / $B$-次数 ($B$-order of $\\alpha$):对于非零向量 $\\alpha$，考虑序列 $\\alpha, B(\\alpha), B^2(\\alpha), \\dots$。由于向量空间维数有限，这个序列最终会线性相关。存在一个次数最小的、首项为1的多项式 $m_{B,\\alpha}(x)$ 使得 $m_{B,\\alpha}(B)(\\alpha) = 0$。这个多项式称为 $\\alpha$ 关于 $B$ 的零化多项式或最小多项式。其次数 $t = \\text{deg}(m_{B,\\alpha}(x))$ 称为 $\\alpha$ 的 $B$-次数。这意味着 $B^t(\\alpha)$ 可以被 $\\alpha, B(\\alpha), \\dots, B^{t-1}(\\alpha)$ 线性表示，且这 $t$ 个向量是线性无关的。即 $B^t(\\alpha) + c_{t-1}B^{t-1}(\\alpha) + \\dots + c_0\\alpha = 0$。$m_{B,\\alpha}(x) = x^t + c_{t-1}x^{t-1} + \\dots + c_0$。 性质: 如果 $\\alpha$ 的 $B$-次数为 $t$，则 $B(\\alpha)$ 的 $B$-次数为 $t-1$ (除非 $t=0$ 或 $B(\\alpha)=0$ 导致次数更低)。 (更准确地说，如果 $m_{B,\\alpha}(B)(\\alpha)=0$，则 $B \\cdot m_{B,\\alpha}(B)(\\alpha) = m_{B,\\alpha}(B)(B\\alpha) = 0$。若 $m_{B,\\alpha}(x) = x \\cdot q(x)$，则 $q(B)(B\\alpha)=0$。) 特征向量的 $B$-次数为 1 (因为 $(B-\\lambda E)(\\alpha)=0$，所以 $m_{B,\\alpha}(x) = x-\\lambda$)。 若 $\\alpha_1, \\dots, \\alpha_s$ 的 $B$-次数均为 $t_1$，则它们的线性组合 $\\sum k_i \\alpha_i$ 的 $B$-次数 $\\le t_1$。 5. Jordan 链与 Jordan 块 Jordan 链 (Jordan Chain):设 $B$ 是一个线性变换 (通常考虑 $B = A - \\lambda E$ 这样一个幂零变换)。一个由向量 $\\alpha_1, \\alpha_2, \\dots, \\alpha_t$ 组成的序列称为一个长度为 $t$ 的 Jordan 链 (对应于特征值 $\\lambda$，如果 $B=A-\\lambda E$)，如果： $B(\\alpha_1) = 0$ ($\\alpha_1$ 是特征向量) $B(\\alpha_2) = \\alpha_1$ $B(\\alpha_3) = \\alpha_2$ … $B(\\alpha_t) = \\alpha_{t-1}$ 且 $\\alpha_t \\ne 0$。向量 $\\alpha_t$ 称为链首 (generating vector)，$\\alpha_1$ 称为链尾。注意：有时定义是从 $\\alpha_t$ 开始：$B(\\alpha_t) = \\alpha_{t-1}, \\dots, B(\\alpha_2)=\\alpha_1, B(\\alpha_1)=0$。在这种定义下，$\\alpha_t$ 的 $(A-\\lambda E)$-次数 (或 $B$-次数) 是 $t$，且 $(A-\\lambda E)^t(\\alpha_t)=0$ 但 $(A-\\lambda E)^{t-1}(\\alpha_t)=\\alpha_1 \\ne 0$。向量组 $\\{\\alpha_1, \\dots, \\alpha_t\\}$ 是线性无关的。 循环子空间 (Cyclic Subspace) / Jordan 链生成的子空间:由 Jordan 链 $\\{\\alpha_1, \\dots, \\alpha_t\\}$ (其中 $(A-\\lambda E)\\alpha_k = \\alpha_{k-1}$ for $k&gt;1$, $(A-\\lambda E)\\alpha_1=0$) 张成的子空间 $L(\\alpha_t) = \\text{span}\\{\\alpha_1, \\dots, \\alpha_t\\}$ 是 $A$-不变的。(因为 $A\\alpha_k = \\lambda\\alpha_k + \\alpha_{k-1} \\in L(\\alpha_t)$, $A\\alpha_1 = \\lambda\\alpha_1 \\in L(\\alpha_t)$)。在这组基 $\\{\\alpha_1, \\dots, \\alpha_t\\}$ 下，线性变换 $A$ 限制在 $L(\\alpha_t)$ 上的矩阵表示为 Jordan 块:$J_t(\\lambda) = \\begin{pmatrix}\\lambda &amp; 1 &amp; &amp; &amp; \\\\&amp; \\lambda &amp; 1 &amp; &amp; \\\\&amp; &amp; \\ddots &amp; \\ddots &amp; \\\\&amp; &amp; &amp; \\lambda &amp; 1 \\\\&amp; &amp; &amp; &amp; \\lambda\\end{pmatrix}_{t \\times t}$(如果基的顺序是 $\\alpha_t, \\alpha_{t-1}, \\dots, \\alpha_1$，则1在主对角线上方；如果基是 $\\alpha_1, \\alpha_2, \\dots, \\alpha_t$，则1在主对角线下方，具体看定义)。PPT中似乎暗示 $B\\alpha = B\\alpha$, $B$ 限制在 $L(\\alpha_t)$ (这里 $\\alpha_t$ 是链首，即 $B^{t-1}\\alpha_t \\ne 0, B^t \\alpha_t = 0$) 上的矩阵是 $J(0)$ (即特征值为0的Jordan块)。这是因为他们考虑的是幂零变换 $B = A-\\lambda_i E$。$B(\\alpha_t) = \\alpha_{t-1}$…$B(\\alpha_2) = \\alpha_1$$B(\\alpha_1) = 0$若基为 $\\{\\alpha_1, \\alpha_2, \\dots, \\alpha_t\\}$，则 $B$ 在此基下的矩阵为：$B \\rightarrow \\begin{pmatrix}0 &amp; 1 &amp; &amp; &amp; \\\\&amp; 0 &amp; 1 &amp; &amp; \\\\&amp; &amp; \\ddots &amp; \\ddots &amp; \\\\&amp; &amp; &amp; 0 &amp; 1 \\\\&amp; &amp; &amp; &amp; 0\\end{pmatrix}$ (1在主对角线上方)。 6. 广义特征子空间 (Generalized Eigenspace) 与 Jordan 标准型定理6.1 广义特征向量与广义特征子空间 广义特征向量 (Generalized Eigenvector): 对应于特征值 $\\lambda$ 的广义特征向量 $\\alpha$ 是指存在某个正整数 $m$ 使得 $(A-\\lambda E)^m \\alpha = 0$。 广义特征子空间 (Generalized Eigenspace): 对应于特征值 $\\lambda_i$ 的广义特征子空间 $R_{\\lambda_i}$ (或 $K_{\\lambda_i}$) 定义为：$R_{\\lambda_i} = \\{ \\alpha \\in V \\mid (A-\\lambda_i E)^m \\alpha = 0 \\text{ for some positive integer } m \\}$等价地，$R_{\\lambda_i} = \\text{ker}((A-\\lambda_i E)^{k_i})$，其中 $k_i$ 是 $\\lambda_i$ 在特征多项式中的代数重数 (或者取足够大的 $m$，例如 $m=n$)。 $R_{\\lambda_i}$ 是 $A$-不变子空间。 6.2 空间分解定理 (Primary Decomposition Theorem) 定理: 设 $V$ 是一个 $n$ 维复向量空间，$A: V \\to V$ 是一个线性变换。设 $f_A(x) = (x-\\lambda_1)^{k_1}(x-\\lambda_2)^{k_2}\\dots(x-\\lambda_s)^{k_s}$ 是 $A$ 的特征多项式，其中 $\\lambda_i$ 是互异的特征值，$k_i$ 是代数重数。则 $V$ 可以分解为广义特征子空间的直和：$V = R_{\\lambda_1} \\oplus R_{\\lambda_2} \\oplus \\dots \\oplus R_{\\lambda_s}$并且 $\\text{dim}(R_{\\lambda_i}) = k_i$。 证明概要: 令 $f_i(x) = f_A(x) / (x-\\lambda_i)^{k_i}$。则 $f_1(x), \\dots, f_s(x)$ 互质。 因此存在多项式 $u_1(x), \\dots, u_s(x)$ 使得 $\\sum_{j=1}^s u_j(x)f_j(x) = 1$。 对任意 $\\alpha \\in V$，$\\alpha = \\sum_{j=1}^s u_j(A)f_j(A)(\\alpha)$。 令 $\\alpha_j = u_j(A)f_j(A)(\\alpha)$。可以证明 $\\alpha_j \\in R_{\\lambda_j}$ (因为 $(A-\\lambda_j E)^{k_j} \\alpha_j = (A-\\lambda_j E)^{k_j} u_j(A)f_j(A)(\\alpha) = u_j(A)f_A(A)(\\alpha) = 0$ by Cayley-Hamilton)。 这表明 $V = R_{\\lambda_1} + \\dots + R_{\\lambda_s}$。 再证明这个和是直和。设 $\\sum \\gamma_j = 0$ 且 $\\gamma_j \\in R_{\\lambda_j}$。通过乘以适当的 $(A-\\lambda_k E)^{m_k}$ 并利用 $\\lambda_i$ 互异的性质，可以证明每个 $\\gamma_j=0$。 最后证明 $\\text{dim}(R_{\\lambda_i}) = k_i$。 6.3 Jordan 标准型定理 定理: 任何一个定义在代数闭域 (如复数域 $\\mathbb{C}$) 上的 $n$ 阶方阵 $A$ 都相似于一个 Jordan 标准型矩阵 $J$。$P^{-1}AP = J = \\text{diag}(J_1, J_2, \\dots, J_q)$其中每个 $J_k$ 是一个 Jordan 块，形如 $J_{m_k}(\\lambda_{i_k})$，对应某个特征值 $\\lambda_{i_k}$。$J$ 中的 Jordan 块的结构 (大小和对应的特征值) 在不计排列顺序的情况下是唯一的。 构造 Jordan 标准型 (对幂零变换):Jordan 标准型理论的核心在于处理幂零变换 (nilpotent transformation)，即某个幂次为零矩阵的变换 $B$ ($B^t=O$)。 对每个广义特征子空间 $R_{\\lambda_i}$，考虑变换 $B_i = A - \\lambda_i E$ 限制在 $R_{\\lambda_i}$ 上。则 $B_i$ 是幂零变换。 对每个幂零变换 $B_i$ 作用在 $R_{\\lambda_i}$ 上，可以找到一组基，使得 $B_i$ 在这组基下的矩阵是 Jordan 块 $J(\\mathbf{0})$ (对角元为0) 的直和。 这组基是由若干条 Jordan 链的向量组成的。 $A$ 限制在 $R_{\\lambda_i}$ 上的矩阵将是 $J(\\lambda_i)$ (对角元为 $\\lambda_i$) 的直和。 将所有 $R_{\\lambda_i}$ 的 Jordan 基合并起来，就得到 $A$ 的 Jordan 标准型。 Jordan 块的个数和大小:对于特征值 $\\lambda_i$： 尺寸至少为 $k$ 的 Jordan 块的个数为 $\\text{rank}((A-\\lambda_i E)^{k-1}) - \\text{rank}((A-\\lambda_i E)^k)$。 尺寸恰好为 $k$ 的 Jordan 块的个数 $N_k(\\lambda_i)$ 可以通过以下公式计算：$N_k(\\lambda_i) = \\text{rank}((A-\\lambda_i E)^{k-1}) - 2 \\cdot \\text{rank}((A-\\lambda_i E)^k) + \\text{rank}((A-\\lambda_i E)^{k+1})$(需要 $N_0=0$ 和 $(A-\\lambda_i E)^0 = E$)。 对应于 $\\lambda_i$ 的 Jordan 块的总个数 (即 Jordan 链的条数) 等于 $\\lambda_i$ 的几何重数，即 $\\text{dim}(\\text{ker}(A-\\lambda_i E)) = n - \\text{rank}(A-\\lambda_i E)$。 6.4 Jordan 标准型的唯一性 Jordan 标准型在不计 Jordan 块排列顺序的情况下是唯一的。这意味着对于给定的矩阵 $A$，其 Jordan 块的个数、每个 Jordan 块的大小以及对应的特征值都是唯一确定的。 6.5 例子 (PPT 中的例子)PPT 中给出了几个计算 Jordan 标准型的例子，步骤通常是： 计算特征多项式 $f_A(\\lambda)$，找出所有特征值 $\\lambda_i$ 及其代数重数 $k_i$。 对每个特征值 $\\lambda_i$，考虑 $B_i = A - \\lambda_i E$。 计算 $B_i, B_i^2, B_i^3, \\dots$ 的秩，直到秩不再变化 (或幂为零)。 根据秩的变化情况确定 Jordan 块的大小和个数。例如，对于特征值 $\\lambda$： $d_1 = \\text{dim}(\\text{ker}(A-\\lambda E)) = n - \\text{rank}(A-\\lambda E)$ 是 $\\lambda$ 对应的 Jordan 块总数。 $d_2 = \\text{dim}(\\text{ker}((A-\\lambda E)^2)) = n - \\text{rank}((A-\\lambda E)^2)$。 $d_j - d_{j-1}$ 表示长度至少为 $j$ 的 Jordan 链的数目。 一个 $m \\times m$ 的 Jordan 块 $J_m(\\lambda)$ 贡献给 $\\text{dim}(\\text{ker}((A-\\lambda E)^j))$ 的维度是 $\\min(j, m)$。 构造 Jordan 链： 从 $\\text{ker}((A-\\lambda E)^p) / \\text{ker}((A-\\lambda E)^{p-1})$ 中选取向量作为长度为 $p$ 的链的链首。 通过 $(A-\\lambda E)$ 作用得到链中其他向量。 将所有 Jordan 链的基向量组合起来构成相似变换矩阵 $P$。$P^{-1}AP = J$。 PPT 中还提到了 $A$ 和 $A^T$ 有相同的 Jordan 标准型 (因为它们相似于同一个 Jordan 标准型的转置，而 Jordan 块的转置可以通过改变基的顺序变回原来的 Jordan 块，或者说它们有相同的初等因子和不变因子)。 总结这一部分是线性代数中比较高级和核心的内容。 Cayley-Hamilton 定理 表明每个矩阵都满足其自身的特征方程。 最小多项式 是次数最低的零化多项式，它包含了关于矩阵代数性质的关键信息，尤其是与可对角化的关系。 Jordan 标准型 揭示了任何复方阵在相似变换下的最简形式，即使矩阵不可对角化。它由对应于特征值的Jordan块构成。 理解广义特征子空间和Jordan链是构造Jordan标准型的基础。 这部分内容对于理解矩阵的深层结构、求解常系数线性微分方程组等都有重要应用。","link":"/MATH1408/Chapter5-1-MATH1408.html"},{"title":"Chapter4_MATH1408","text":"映射与线性映射1. 映射 (Mapping / Function) 的基本概念1.1 定义 设 $A$ 和 $B$ 是两个非空集合。如果存在一个法则（或对应关系） $\\phi$，使得对于集合 $A$ 中的每一个元素 $a$，在集合 $B$ 中都有唯一确定的元素 $b$ 与之对应，那么称 $\\phi$ 是从集合 $A$ 到集合 $B$ 的一个映射 (mapping) 或函数 (function)。 记作 $\\phi: A \\to B$。 元素 $a \\in A$ 称为原像 (preimage)。 元素 $b \\in B$ 称为 $a$ 在映射 $\\phi$ 下的像 (image)，记作 $b = \\phi(a)$。 集合 $A$ 称为映射 $\\phi$ 的定义域 (domain)。 集合 $B$ 称为映射 $\\phi$ 的陪域 (codomain)。 所有像的集合，即 $\\{\\phi(a) | a \\in A\\}$，称为映射 $\\phi$ 的值域 (range 或 image of $\\phi$)，记作 $\\text{Im}(\\phi)$ 或 $\\phi(A)$。显然，$\\text{Im}(\\phi) \\subseteq B$。 对于 $b \\in B$，集合 $\\{a \\in A | \\phi(a) = b\\}$ 称为 $b$ 的原像集 (preimage set of $b$)，记作 $\\phi^{-1}(b)$。 注意: 定义域中的每个元素都必须有像。 定义域中的每个元素的像是唯一的。 陪域中的元素不一定都有原像，一个元素也可能有多个原像（除非是单射或双射）。 1.2 映射的相等两个映射 $\\phi: A \\to B$ 和 $\\psi: A \\to B$ 相等，当且仅当它们的定义域相同，陪域相同，并且对于任意 $a \\in A$，都有 $\\phi(a) = \\psi(a)$。 1.3 映射的复合 (Composition of Mappings) 设有两个映射 $\\phi_1: A \\to B$ 和 $\\phi_2: B \\to C$。 可以定义一个新的映射，称为 $\\phi_2$ 与 $\\phi_1$ 的复合映射，记作 $\\phi_2 \\circ \\phi_1$ (有时也记作 $\\phi_2\\phi_1$)，它是一个从 $A$ 到 $C$ 的映射：$\\phi_2 \\circ \\phi_1: A \\to C$$(\\phi_2 \\circ \\phi_1)(a) = \\phi_2(\\phi_1(a))$, 对于所有 $a \\in A$。 注意运算顺序: 先作用 $\\phi_1$，再作用 $\\phi_2$。 映射复合的结合律:定理: 设 $\\phi_1: A \\to B$, $\\phi_2: B \\to C$, $\\phi_3: C \\to D$ 是三个映射。则$\\phi_3 \\circ (\\phi_2 \\circ \\phi_1) = (\\phi_3 \\circ \\phi_2) \\circ \\phi_1$。 证明:两个映射相等需要验证它们的定义域、陪域相同，且对定义域中任意元素的作用结果相同。左边映射 $\\phi_3 \\circ (\\phi_2 \\circ \\phi_1)$ 的定义域是 $A$，陪域是 $D$。右边映射 $(\\phi_3 \\circ \\phi_2) \\circ \\phi_1$ 的定义域是 $A$，陪域是 $D$。对于任意 $a \\in A$:$(\\phi_3 \\circ (\\phi_2 \\circ \\phi_1))(a) = \\phi_3((\\phi_2 \\circ \\phi_1)(a)) = \\phi_3(\\phi_2(\\phi_1(a)))$.$((\\phi_3 \\circ \\phi_2) \\circ \\phi_1)(a) = (\\phi_3 \\circ \\phi_2)(\\phi_1(a)) = \\phi_3(\\phi_2(\\phi_1(a)))$.由于对于任意 $a \\in A$，两边作用结果相同，故映射的复合满足结合律。 1.4 恒等映射 (Identity Mapping) 对于任意集合 $A$，定义恒等映射 $\\text{id}_A: A \\to A$ (或 $1_A$) 为：$\\text{id}_A(a) = a$, 对于所有 $a \\in A$。 性质: 对于任意映射 $\\phi: A \\to B$:$\\phi \\circ \\text{id}_A = \\phi$$\\text{id}_B \\circ \\phi = \\phi$恒等映射在映射复合运算中充当单位元的作用。 1.5 逆映射 (Inverse Mapping) 设 $\\phi: A \\to B$ 是一个映射。 如果存在一个映射 $\\psi: B \\to A$ 使得 $\\psi \\circ \\phi = \\text{id}_A$，则称 $\\psi$ 是 $\\phi$ 的一个左逆映射，$\\phi$ 是 $\\psi$ 的一个右逆映射。 如果存在一个映射 $\\psi: B \\to A$ 使得 $\\psi \\circ \\phi = \\text{id}_A$ 并且 $\\phi \\circ \\psi = \\text{id}_B$，则称 $\\psi$ 是 $\\phi$ 的逆映射，记作 $\\psi = \\phi^{-1}$。此时也称映射 $\\phi$ 是可逆的 (invertible) 或双射 (bijective)。 定理 (逆映射的唯一性): 如果映射 $\\phi: A \\to B$ 存在左逆 $\\psi_1$ 和右逆 $\\psi_2$，则 $\\psi_1 = \\psi_2$，并且这个共同的映射是 $\\phi$ 的唯一的逆映射。 证明:假设 $\\psi_1 \\circ \\phi = \\text{id}_A$ 且 $\\phi \\circ \\psi_2 = \\text{id}_B$.考虑 $\\psi_1 = \\psi_1 \\circ \\text{id}_B = \\psi_1 \\circ (\\phi \\circ \\psi_2)$.根据映射复合的结合律: $\\psi_1 \\circ (\\phi \\circ \\psi_2) = (\\psi_1 \\circ \\phi) \\circ \\psi_2 = \\text{id}_A \\circ \\psi_2 = \\psi_2$.所以 $\\psi_1 = \\psi_2$.因此，如果一个映射同时有左逆和右逆，那么它们必然相等，且这个映射就是唯一的逆映射。 推论: 如果映射 $\\phi$ 可逆，则其逆映射 $\\phi^{-1}$ 唯一。 性质: 如果 $\\phi$ 可逆，则 $\\phi^{-1}$ 也可逆，且 $(\\phi^{-1})^{-1} = \\phi$。 如果 $\\phi: A \\to B$ 和 $\\psi: B \\to C$ 都是可逆映射，则复合映射 $\\psi \\circ \\phi: A \\to C$ 也可逆，且 $(\\psi \\circ \\phi)^{-1} = \\phi^{-1} \\circ \\psi^{-1}$ (注意顺序)。 证明: 验证 $(\\phi^{-1} \\circ \\psi^{-1}) \\circ (\\psi \\circ \\phi) = \\text{id}_A$ 和 $(\\psi \\circ \\phi) \\circ (\\phi^{-1} \\circ \\psi^{-1}) = \\text{id}_C$。$(\\phi^{-1} \\circ \\psi^{-1}) \\circ (\\psi \\circ \\phi) = \\phi^{-1} \\circ (\\psi^{-1} \\circ \\psi) \\circ \\phi = \\phi^{-1} \\circ \\text{id}_B \\circ \\phi = \\phi^{-1} \\circ \\phi = \\text{id}_A$.$(\\psi \\circ \\phi) \\circ (\\phi^{-1} \\circ \\psi^{-1}) = \\psi \\circ (\\phi \\circ \\phi^{-1}) \\circ \\psi^{-1} = \\psi \\circ \\text{id}_B \\circ \\psi^{-1} = \\psi \\circ \\psi^{-1} = \\text{id}_C$. 1.6 单射、满射、双射 单射 (Injective / One-to-one):映射 $\\phi: A \\to B$ 称为单射，如果对于任意 $a_1, a_2 \\in A$，当 $a_1 \\neq a_2$ 时，有 $\\phi(a_1) \\neq \\phi(a_2)$。等价地，如果 $\\phi(a_1) = \\phi(a_2)$ 蕴涵 $a_1 = a_2$。(不同的原像有不同的像)。 满射 (Surjective / Onto):映射 $\\phi: A \\to B$ 称为满射，如果对于任意 $b \\in B$，都存在至少一个 $a \\in A$ 使得 $\\phi(a) = b$。等价地，值域 $\\text{Im}(\\phi) = B$ (陪域中的每个元素都是至少一个原像的像)。 双射 (Bijective / One-to-one correspondence):映射 $\\phi: A \\to B$ 称为双射，如果它既是单射又是满射。 定理 (映射类型与逆映射的关系):设 $\\phi: A \\to B$ 是一个映射。(i) $\\phi$ 是单射 $\\iff$ $\\phi$ 存在左逆映射。(ii) $\\phi$ 是满射 $\\iff$ $\\phi$ 存在右逆映射。(iii) $\\phi$ 是双射 $\\iff$ $\\phi$ 存在逆映射 (即同时存在左逆和右逆，且它们相等)。 证明 (i):($\\Rightarrow$) 假设 $\\phi$ 是单射。定义 $\\psi: B \\to A$ 如下：对于 $b \\in \\text{Im}(\\phi)$, 由于 $\\phi$ 是单射，存在唯一的 $a \\in A$ 使得 $\\phi(a)=b$。令 $\\psi(b) = a$。对于 $b \\in B \\setminus \\text{Im}(\\phi)$ (如果 $B \\setminus \\text{Im}(\\phi)$ 非空)，任选一个固定的 $a_0 \\in A$ (假设 $A$ 非空)，令 $\\psi(b) = a_0$。则对于任意 $a \\in A$， $\\psi(\\phi(a)) = a$。所以 $\\psi \\circ \\phi = \\text{id}_A$。$\\psi$ 是 $\\phi$ 的左逆。($\\Leftarrow$) 假设存在左逆 $\\psi: B \\to A$ 使得 $\\psi \\circ \\phi = \\text{id}_A$。如果 $\\phi(a_1) = \\phi(a_2)$，则 $\\psi(\\phi(a_1)) = \\psi(\\phi(a_2))$。即 $(\\psi \\circ \\phi)(a_1) = (\\psi \\circ \\phi)(a_2)$。即 $\\text{id}_A(a_1) = \\text{id}_A(a_2)$，所以 $a_1 = a_2$。因此 $\\phi$ 是单射。 证明 (ii):($\\Rightarrow$) 假设 $\\phi$ 是满射。对于任意 $b \\in B$，由于 $\\phi$ 是满射，原像集 $\\phi^{-1}(b) = \\{a \\in A | \\phi(a)=b\\}$ 非空。对于每个 $b \\in B$，根据选择公理 (Axiom of Choice)，我们可以从 $\\phi^{-1}(b)$ 中选择一个元素，记为 $a_b$。定义 $\\psi: B \\to A$ 为 $\\psi(b) = a_b$。则对于任意 $b \\in B$， $\\phi(\\psi(b)) = \\phi(a_b) = b$。所以 $\\phi \\circ \\psi = \\text{id}_B$。$\\psi$ 是 $\\phi$ 的右逆。($\\Leftarrow$) 假设存在右逆 $\\psi: B \\to A$ 使得 $\\phi \\circ \\psi = \\text{id}_B$。对于任意 $b \\in B$，令 $a = \\psi(b) \\in A$。则 $\\phi(a) = \\phi(\\psi(b)) = (\\phi \\circ \\psi)(b) = \\text{id}_B(b) = b$。这意味着对于任意 $b \\in B$，都存在 $a \\in A$ (即 $\\psi(b)$) 使得 $\\phi(a)=b$。因此 $\\phi$ 是满射。 证明 (iii):$\\phi$ 是双射 $\\iff$ $\\phi$ 既是单射又是满射。根据 (i) 和 (ii)，这等价于 $\\phi$ 既存在左逆又存在右逆。根据逆映射唯一性定理，这意味着 $\\phi$ 存在唯一的逆映射 $\\phi^{-1}$。 1.7 集合的基数 (Cardinality) 与 Schröder-Bernstein 定理 如果集合 $A$ 和 $B$ 之间存在一个单射 $\\phi: A \\to B$，则称 $A$ 的基数不大于 $B$ 的基数，记作 $|A| \\le |B|$ (或 $A \\preceq B$，$A$ 的势小于等于 $B$ 的势)。 如果集合 $A$ 和 $B$ 之间存在一个双射 $\\phi: A \\to B$，则称 $A$ 和 $B$ 等势 (equipollent 或 equinumerous) 或具有相同的基数，记作 $|A| = |B|$ (或 $A \\sim B$)。 如果 $|A| \\le |B|$ 且 $|A| \\neq |B|$，则称 $|A| &lt; |B|$。 Schröder-Bernstein 定理: 如果 $|A| \\le |B|$ 且 $|B| \\le |A|$，则 $|A| = |B|$。即，如果存在单射 $\\phi: A \\to B$ 和单射 $\\psi: B \\to A$，则存在双射 $\\chi: A \\to B$。 证明概要 (一种经典构造思路):考虑元素在两个映射下的“祖先链”。对于 $a \\in A$, 其祖先链形如 $a, \\psi^{-1}(a), \\phi^{-1}(\\psi^{-1}(a)), \\dots$ (如果这些逆像存在且唯一，因为 $\\phi, \\psi$ 是单射，所以逆像如果存在就是唯一的)。将 $A$ 划分成三部分 $A_A, A_B, A_\\infty$： $A_A$: 祖先链终止于 $A$ (即某个祖先在 $A$ 中但没有 $\\psi$ 的原像)。 $A_B$: 祖先链终止于 $B$ (即某个祖先在 $B$ 中但没有 $\\phi$ 的原像)。 $A_\\infty$: 祖先链无限延伸或形成循环。类似地划分 $B$ 为 $B_A, B_B, B_\\infty$。可以证明 $\\phi$ 将 $A_A$ 双射到 $B_A$，$\\phi$ 将 $A_\\infty$ 双射到 $B_\\infty$。而 $\\psi$ 将 $B_B$ 双射到 $A_B$，所以 $\\psi^{-1}$ (在 $A_B$ 上有定义) 将 $A_B$ 双射到 $B_B$。定义 $\\chi: A \\to B$ 为:$\\chi(a) = \\phi(a)$ 如果 $a \\in A_A \\cup A_\\infty$$\\chi(a) = \\psi^{-1}(a)$ 如果 $a \\in A_B$可以证明 $\\chi$ 是一个双射。 1.8 集合的幂集与 Cantor 定理 从集合 $A$ 到集合 $B$ 的所有映射构成的集合记作 $B^A$ 或 $\\text{Map}(A,B)$。 其基数关系为 $|B^A| = |B|^{|A|}$ (对于有限集成立，并推广到无限集)。 一个集合 $A$ 的所有子集构成的集合称为 $A$ 的幂集 (power set)，记作 $\\mathcal{P}(A)$ 或 $2^A$。 幂集 $\\mathcal{P}(A)$ 与从 $A$ 到 $\\{0,1\\}$ 的所有映射的集合 $ \\{0,1\\}^A $ 是等势的。一个子集 $S \\subseteq A$ 对应一个特征函数 $\\chi_S: A \\to \\{0,1\\}$，其中 $\\chi_S(a)=1$ if $a \\in S$, $\\chi_S(a)=0$ if $a \\notin S$。因此 $|\\mathcal{P}(A)| = 2^{|A|}$。 Cantor 定理: 对于任何集合 $A$， $|A| &lt; |\\mathcal{P}(A)|$ (即 $|A| &lt; 2^{|A|}$)。这意味着没有从集合到其幂集的满射。 证明 (反证法):假设存在一个满射 $f: A \\to \\mathcal{P}(A)$。考虑集合 $S = \\{a \\in A | a \\notin f(a) \\}$。由于 $f$ 是满射，且 $S \\subseteq A$ (即 $S \\in \\mathcal{P}(A)$)，所以必定存在某个 $x \\in A$ 使得 $f(x) = S$。现在问：$x \\in S$ 吗？ 如果 $x \\in S$，根据 $S$ 的定义，$x \\notin f(x)$。但 $f(x)=S$，所以 $x \\notin S$。矛盾。 如果 $x \\notin S$，根据 $S$ 的定义，$x \\in f(x)$。但 $f(x)=S$，所以 $x \\in S$。矛盾。无论哪种情况都导致矛盾。因此，不存在这样的满射 $f$。同时，存在一个单射 $g: A \\to \\mathcal{P}(A)$，例如 $g(a) = \\{a\\}$。所以 $|A| \\le |\\mathcal{P}(A)|$。结合不存在满射，得出 $|A| &lt; |\\mathcal{P}(A)|$。 基数: 自然数集 $\\mathbb{N}$ 的基数记为 $\\aleph_0$ (阿列夫零)，称其为可数无限。 实数集 $\\mathbb{R}$ 的基数记为 $\\mathfrak{c}$ (连续统的势)。可以证明 $\\mathfrak{c} = 2^{\\aleph_0}$。 $\\aleph_1$ 是第一个不可数基数。连续统假设 (CH) 断言 $\\aleph_1 = 2^{\\aleph_0}$ (即 $\\mathfrak{c} = \\aleph_1$)。CH独立于ZFC公理系统。 1.9 映射的限制 (Restriction) 设 $\\phi: A \\to B$ 是一个映射，$A_1 \\subseteq A$ 是 $A$ 的一个子集。 $\\phi$ 在 $A_1$ 上的限制是一个新的映射 $\\phi|_{A_1}: A_1 \\to B$，定义为：$(\\phi|_{A_1})(a) = \\phi(a)$ 对于所有 $a \\in A_1$。(定义域缩小，对应法则不变)。 2. 线性映射 (Linear Transformation / Linear Map)2.1 定义 (背景：向量空间) 设 $V$ 和 $V_1$ 是定义在同一数域 $F$ 上的两个向量空间。 一个映射 $A: V \\to V_1$ (PPT中用 $A$ 表示线性映射，通常也用 $T, L$ 等) 称为从 $V$ 到 $V_1$ 的一个线性映射，如果它满足以下两个条件： 可加性: 对于任意 $\\alpha, \\beta \\in V$，有 $A(\\alpha + \\beta) = A(\\alpha) + A(\\beta)$。 齐次性: 对于任意 $\\alpha \\in V$ 和任意标量 $k \\in F$，有 $A(k\\alpha) = k A(\\alpha)$。 这两个条件可以合并为一个：对于任意 $\\alpha, \\beta \\in V$ 和任意标量 $k \\in F$ (或任意标量 $k, l \\in F$)，有$A(k\\alpha + \\beta) = k A(\\alpha) + A(\\beta)$ (PPT中的形式)或更一般地 $A(k\\alpha + l\\beta) = k A(\\alpha) + l A(\\beta)$。 如果 $V = V_1$，则线性映射 $A: V \\to V$ 称为 $V$ 上的一个线性变换 (linear operator)。 2.2 线性映射的例子 数乘变换: 设 $V$ 是数域 $F$ 上的向量空间，$c \\in F$ 是一个固定的标量。定义 $A: V \\to V$ 为 $A(\\alpha) = c\\alpha$ 对于所有 $\\alpha \\in V$。 如果 $c=1$，则 $A(\\alpha) = \\alpha$，这是恒等变换 (identity operator)，记作 $\\text{Id}_V$ 或 $I$ 或 $1_V$。 如果 $c=0$，则 $A(\\alpha) = \\mathbf{0}_V$ (零向量)，这是零变换 (zero operator)，记作 $O$。 矩阵乘法: 设 $A$ 是一个 $m \\times n$ 的实数 (或复数) 矩阵。它可以定义一个从 $F^n$ 到 $F^m$ (其中 $F=\\mathbb{R}$ 或 $\\mathbb{C}$) 的线性映射 $T_A: F^n \\to F^m$ 为 $T_A(\\mathbf{x}) = A\\mathbf{x}$ (矩阵向量乘积)。 $A(\\mathbf{x}+\\mathbf{y}) = A(\\mathbf{x}+\\mathbf{y}) = A\\mathbf{x} + A\\mathbf{y}$ $A(k\\mathbf{x}) = A(k\\mathbf{x}) = k(A\\mathbf{x})$ 微分算子: 设 $V = C^1[a,b]$ (区间 $[a,b]$ 上所有一阶连续可导函数的集合，构成一个向量空间)。定义 $D: V \\to C[a,b]$ (连续函数空间) 为 $D(f(x)) = f’(x)$ (求导)。 $D(f+g) = (f+g)’ = f’ + g’ = D(f) + D(g)$ $D(kf) = (kf)’ = kf’ = kD(f)$ 积分算子: 设 $V = C[a,b]$。定义 $I: V \\to \\mathbb{R}$ (或到某个函数空间，如 $I(f)(x) = \\int_a^x f(t)dt$) 为 $I(f) = \\int_a^b f(x)dx$ (定积分，这是一个线性泛函)。如果 $I(f)(x) = \\int_a^x f(t)dt$，则 $I: C[a,b] \\to C^1[a,b]$ 是一个线性映射。 投影变换: 设 $V = U \\oplus W$ (向量空间 $V$ 是其子空间 $U$ 和 $W$ 的直和)。则任意 $\\alpha \\in V$ 可以唯一分解为 $\\alpha = \\alpha_U + \\alpha_W$，其中 $\\alpha_U \\in U, \\alpha_W \\in W$。定义 $P_U: V \\to V$ (或 $V \\to U$) 为 $P_U(\\alpha) = \\alpha_U$。这称为沿 $W$ 到 $U$ 的投影。它是线性的。 商映射 (自然映射): 设 $W$ 是向量空间 $V$ 的一个子空间。$V/W = \\{\\alpha + W | \\alpha \\in V\\}$ 是商空间。定义自然映射 (或典范映射) $\\pi: V \\to V/W$ 为 $\\pi(\\alpha) = \\alpha + W$。$\\pi(k\\alpha + \\beta) = (k\\alpha + \\beta) + W = (k\\alpha+W) + (\\beta+W)$ (需要定义商空间的运算)商空间中的运算定义为：$(\\alpha+W) + (\\beta+W) = (\\alpha+\\beta)+W$ 和 $k(\\alpha+W) = (k\\alpha)+W$。则 $\\pi(k\\alpha + \\beta) = (k\\alpha+\\beta)+W = k(\\alpha+W) + (\\beta+W) = k\\pi(\\alpha) + \\pi(\\beta)$。所以 $\\pi$ 是一个线性映射。 2.3 线性映射的基本性质设 $A: V \\to V_1$ 是一个线性映射。 $A(\\mathbf{0}_V) = \\mathbf{0}_{V_1}$ (零向量映到零向量)。 证明: $A(\\mathbf{0}_V) = A(\\mathbf{0}_V + \\mathbf{0}_V) = A(\\mathbf{0}_V) + A(\\mathbf{0}_V)$。两边减去 $A(\\mathbf{0}_V)$ 得 $A(\\mathbf{0}_V) = \\mathbf{0}_{V_1}$。或者 $A(\\mathbf{0}_V) = A(0 \\cdot \\alpha) = 0 \\cdot A(\\alpha) = \\mathbf{0}_{V_1}$。 $A(-\\alpha) = -A(\\alpha)$。 证明: $A(-\\alpha) = A((-1)\\alpha) = (-1)A(\\alpha) = -A(\\alpha)$。 线性组合的保持性: 对于任意 $\\alpha_1, \\dots, \\alpha_m \\in V$ 和标量 $k_1, \\dots, k_m \\in F$:$A(k_1\\alpha_1 + k_2\\alpha_2 + \\dots + k_m\\alpha_m) = k_1A(\\alpha_1) + k_2A(\\alpha_2) + \\dots + k_mA(\\alpha_m)$。(可以通过数学归纳法证明)。这意味着线性映射保持向量之间的线性关系。线性相关的向量组的像仍然是线性相关的（或为零向量）。线性无关的向量组的像不一定是线性无关的（可能映为零或线性相关）。 2.4 由基的像确定线性映射 定理: 设 $V$ 和 $V_1$ 是数域 $F$ 上的向量空间。设 $\\{\\alpha_1, \\dots, \\alpha_n\\}$ 是 $V$ 的一组基。设 $\\beta_1, \\dots, \\beta_n$ 是 $V_1$ 中任意 $n$ 个向量。则存在唯一的线性映射 $A: V \\to V_1$ 使得 $A(\\alpha_i) = \\beta_i$ 对于 $i=1, \\dots, n$。 证明:存在性: 对于任意 $\\alpha \\in V$，由于 $\\{\\alpha_i\\}$ 是基，$\\alpha$ 可以唯一表示为 $\\alpha = x_1\\alpha_1 + \\dots + x_n\\alpha_n$，其中 $x_i \\in F$。定义映射 $A: V \\to V_1$ 为 $A(\\alpha) = x_1\\beta_1 + \\dots + x_n\\beta_n$。验证 $A$ 是线性的：设 $\\alpha = \\sum x_i \\alpha_i$ 和 $\\gamma = \\sum y_i \\alpha_i$。$A(\\alpha + \\gamma) = A(\\sum (x_i+y_i)\\alpha_i) = \\sum (x_i+y_i)\\beta_i = \\sum x_i\\beta_i + \\sum y_i\\beta_i = A(\\alpha) + A(\\gamma)$。$A(k\\alpha) = A(\\sum (kx_i)\\alpha_i) = \\sum (kx_i)\\beta_i = k \\sum x_i\\beta_i = k A(\\alpha)$。且 $A(\\alpha_i) = A(0\\alpha_1 + \\dots + 1\\alpha_i + \\dots + 0\\alpha_n) = 1\\beta_i = \\beta_i$。所以这样的线性映射存在。唯一性: 假设存在另一个线性映射 $B: V \\to V_1$ 使得 $B(\\alpha_i)=\\beta_i$。对于任意 $\\alpha = \\sum x_i \\alpha_i \\in V$：$B(\\alpha) = B(\\sum x_i \\alpha_i) = \\sum x_i B(\\alpha_i)$ (因为 $B$ 线性)$= \\sum x_i \\beta_i = A(\\alpha)$。由于对任意 $\\alpha \\in V$，$B(\\alpha)=A(\\alpha)$，所以 $B=A$。 结论: 一个线性映射完全由它在一组基上的作用所确定。 2.5 线性映射的同构 (Isomorphism) 如果线性映射 $A: V \\to V_1$ 是一个双射 (即既单射又满射)，则称 $A$ 是一个线性同构 (linear isomorphism)，并称向量空间 $V$ 与 $V_1$ 线性同构 (linearly isomorphic)，记作 $V \\cong V_1$。 定理: 设 $V$ 是 $n$ 维向量空间，$\\{\\alpha_1, \\dots, \\alpha_n\\}$ 是 $V$ 的一组基。则线性映射 $A: V \\to V_1$ 是线性同构的充要条件是像集 $\\{A(\\alpha_1), \\dots, A(\\alpha_n)\\}$ 是 $V_1$ 的一组基。 证明:($\\Rightarrow$) 假设 $A$ 是同构。首先证明 $\\{A(\\alpha_i)\\}$ 线性无关：若 $\\sum k_i A(\\alpha_i) = \\mathbf{0}_{V_1}$，则 $A(\\sum k_i \\alpha_i) = \\mathbf{0}_{V_1}$。因为 $A$ 是单射 (同构必单射)，所以 $A(\\mathbf{0}_V) = \\mathbf{0}_{V_1}$，故 $\\sum k_i \\alpha_i = \\mathbf{0}_V$。由于 $\\{\\alpha_i\\}$ 线性无关，所以所有 $k_i=0$。因此 $\\{A(\\alpha_i)\\}$ 线性无关。其次证明 $\\{A(\\alpha_i)\\}$ 生成 $V_1$：对于任意 $\\beta \\in V_1$，因为 $A$ 是满射 (同构必满射)，存在 $\\alpha \\in V$ 使得 $A(\\alpha)=\\beta$。设 $\\alpha = \\sum x_i \\alpha_i$。则 $\\beta = A(\\alpha) = A(\\sum x_i \\alpha_i) = \\sum x_i A(\\alpha_i)$。所以 $\\{A(\\alpha_i)\\}$ 生成 $V_1$。因此 $\\{A(\\alpha_i)\\}$ 是 $V_1$ 的基。这也意味着 $\\dim V_1 = n = \\dim V$。($\\Leftarrow$) 假设 $\\{A(\\alpha_i)\\}$ 是 $V_1$ 的一组基。这意味着 $\\dim V_1 = n = \\dim V$。证明 $A$ 是满射：对于任意 $\\beta \\in V_1$，由于 $\\{A(\\alpha_i)\\}$ 是 $V_1$ 的基，$\\beta = \\sum y_i A(\\alpha_i) = A(\\sum y_i \\alpha_i)$。令 $\\alpha = \\sum y_i \\alpha_i \\in V$，则 $A(\\alpha)=\\beta$。所以 $A$ 是满射。证明 $A$ 是单射：设 $A(\\alpha) = \\mathbf{0}_{V_1}$。设 $\\alpha = \\sum x_i \\alpha_i$。则 $A(\\alpha) = \\sum x_i A(\\alpha_i) = \\mathbf{0}_{V_1}$。由于 $\\{A(\\alpha_i)\\}$ 线性无关，所以所有 $x_i=0$。因此 $\\alpha = \\mathbf{0}_V$。所以 $A$ 是单射 (核为零)。因此 $A$ 是线性同构。 推论: 有限维向量空间 $V$ 上的线性变换 $A: V \\to V$ 是同构的 $\\iff A$ 是单射 $\\iff A$ 是满射。 (这将在后续的秩-零度定理中体现)。 重要结论: 任何 $n$ 维数域 $F$ 上的向量空间都与 $F^n$ 线性同构。(选择一组基 $\\{\\alpha_i\\}$，映射 $\\alpha = \\sum x_i \\alpha_i \\mapsto (x_1, \\dots, x_n)^T$ 就是一个线性同构)。 2.6 线性映射的空间 $\\text{Hom}(V, V_1)$ 设 $V, V_1$ 是数域 $F$ 上的向量空间。所有从 $V$ 到 $V_1$ 的线性映射的集合记作 $\\text{Hom}_F(V, V_1)$ 或 $L(V, V_1)$。 可以对 $\\text{Hom}(V, V_1)$ 中的元素 (即线性映射) 定义加法和标量乘法： 加法: $(A+B)(\\alpha) = A(\\alpha) + B(\\alpha)$，对于所有 $\\alpha \\in V$。 ($A, B \\in \\text{Hom}(V,V_1)$) 标量乘法: $(kA)(\\alpha) = k(A(\\alpha))$，对于所有 $\\alpha \\in V$。 ($k \\in F, A \\in \\text{Hom}(V,V_1)$) 定理: 在上述运算下，$\\text{Hom}(V, V_1)$ 构成数域 $F$ 上的一个向量空间。 证明: 需要验证向量空间的八条公理。例如，零元素是零映射 $O(\\alpha)=\\mathbf{0}_{V_1}$。$A$ 的负元素是 $(-1)A$。 如果 $\\dim V = n, \\dim V_1 = m$，则 $\\dim \\text{Hom}(V, V_1) = mn$。(可以通过将线性映射表示为 $m \\times n$ 矩阵来理解)。 特别地，$\\text{Hom}(V,V)$ (也记作 $L(V)$ 或 $\\text{End}(V)$，表示 $V$ 上的所有线性变换) 是一个向量空间。 2.7 线性变换的代数 $\\text{Hom}(V,V)$ (或称算子代数) 除了向量空间结构，$\\text{Hom}(V,V)$ 还具有乘法运算，即映射的复合。如果 $A, B \\in \\text{Hom}(V,V)$，则 $B \\circ A$ (简记 $BA$) 也是 $\\text{Hom}(V,V)$ 中的一个线性变换。 $(BA)(k\\alpha+\\beta) = B(A(k\\alpha+\\beta)) = B(kA(\\alpha)+A(\\beta)) = kB(A(\\alpha)) + B(A(\\beta)) = k(BA)(\\alpha) + (BA)(\\beta)$。 线性变换的复合满足结合律: $C(BA) = (CB)A$。 线性变换的复合对加法满足分配律:$A(B+C) = AB+AC$$(B+C)A = BA+CA$ 标量乘法与复合的兼容性: $k(AB) = (kA)B = A(kB)$。 $\\text{Hom}(V,V)$ 在加法、标量乘法和映射复合下构成一个带有单位元 (恒等变换 $I$) 的结合代数 (associative algebra over $F$)。(代数是一个向量空间，其上还定义了一个双线性的乘法运算)。具体来说，$\\text{Hom}(V,V)$ 是一个环 (ring)，并且由于可以与数域 $F$ 中的标量相乘，它是一个 $F$-代数。 数域 $F$ 上的 $n \\times n$ 矩阵环 $M_n(F)$ 也是一个 $F$-代数，并且与 $n$ 维空间 $V$ 上的 $\\text{Hom}(V,V)$ 同构。 2.8 投影算子 (Projection Operator) 回顾: 若 $V = U \\oplus W$ (直和分解)，则任意 $\\alpha \\in V$ 可唯一表示为 $\\alpha = u+w$，$u \\in U, w \\in W$。定义 $P_U: V \\to V$ 为 $P_U(\\alpha) = u$，称为沿 $W$ 到 $U$ 的投影算子。 性质: $P_U$ 是线性的。 $P_U^2 = P_U$ (即 $P_U$ 是幂等的 idempotent)。 证明: $P_U^2(\\alpha) = P_U(P_U(\\alpha)) = P_U(u) = u = P_U(\\alpha)$。 $\\text{Im}(P_U) = U$ (投影算子的值域是投影到的子空间)。 $\\text{Ker}(P_U) = W$ (投影算子的核是沿其投影的子空间)。 定理: 一个线性变换 $A: V \\to V$ 是投影算子的充要条件是 $A^2=A$ (即 $A$ 是幂等的)。 证明:($\\Rightarrow$) 已证。($\\Leftarrow$) 假设 $A^2=A$。令 $U = \\text{Im}(A)$ 和 $W = \\text{Ker}(A)$。我们需要证明 $V = U \\oplus W$。 $U+W=V$: 对于任意 $\\alpha \\in V$，可以写成 $\\alpha = A(\\alpha) + (\\alpha - A(\\alpha))$。$A(\\alpha) \\in \\text{Im}(A) = U$。$A(\\alpha - A(\\alpha)) = A(\\alpha) - A^2(\\alpha) = A(\\alpha) - A(\\alpha) = \\mathbf{0}_V$。所以 $\\alpha - A(\\alpha) \\in \\text{Ker}(A) = W$。因此 $V = U+W$。 $U \\cap W = \\{\\mathbf{0}_V\\}$: 设 $\\beta \\in U \\cap W$。因为 $\\beta \\in U = \\text{Im}(A)$，所以存在 $\\gamma \\in V$ 使得 $\\beta = A(\\gamma)$。因为 $\\beta \\in W = \\text{Ker}(A)$，所以 $A(\\beta) = \\mathbf{0}_V$。$A(\\beta) = A(A(\\gamma)) = A^2(\\gamma) = A(\\gamma) = \\beta$。所以 $\\beta = \\mathbf{0}_V$。因此 $V = U \\oplus W$。此时，$A$ 的作用是将 $\\alpha = u+w$ 映为 $A(\\alpha) = A(u) + A(w) = A(u) + \\mathbf{0}_V$。由于 $u \\in U = \\text{Im}(A)$, $u=A(\\gamma’)$ for some $\\gamma’$. Then $A(u) = A^2(\\gamma’) = A(\\gamma’)=u$.所以 $A(\\alpha)=u$。这正是沿 $W$ 到 $U$ 的投影。 如果 $P_U$ 是沿 $W$ 到 $U$ 的投影，$P_W$ 是沿 $U$ 到 $W$ 的投影，则 $P_U + P_W = I$ 且 $P_U P_W = P_W P_U = O$ (零变换)。如果两个投影算子 $A, B$ 满足 $AB=BA=O$，则称它们是正交投影 (如果定义了内积且 $U \\perp W$) 或互补投影。此时 $A+B$ 也是投影算子，投影到 $\\text{Im}(A) \\oplus \\text{Im}(B)$。 2.9 幂零算子 (Nilpotent Operator) 一个线性变换 $A: V \\to V$ 称为幂零算子，如果存在正整数 $k$ 使得 $A^k = O$ (零变换)。 满足 $A^k=O$ 的最小正整数 $k$ 称为 $A$ 的幂零指数。 3. 线性映射的核与像 (Kernel and Image)3.1 核 (Kernel / Null Space) 设 $A: V \\to V_1$ 是一个线性映射。 $A$ 的核 (kernel)，记作 $\\text{Ker}(A)$ 或 $\\text{ker}(A)$ (有时也用 $N(A)$ 表示 null space)，定义为：$\\text{Ker}(A) = \\{\\alpha \\in V | A(\\alpha) = \\mathbf{0}_{V_1}\\}$。(即被 $A$ 映为 $V_1$ 中零向量的所有 $V$ 中向量的集合)。 定理: $\\text{Ker}(A)$ 是 $V$ 的一个子空间。 证明: $\\mathbf{0}_V \\in \\text{Ker}(A)$ 因为 $A(\\mathbf{0}_V) = \\mathbf{0}_{V_1}$。所以 $\\text{Ker}(A)$ 非空。 若 $\\alpha, \\beta \\in \\text{Ker}(A)$, 则 $A(\\alpha)=\\mathbf{0}_{V_1}, A(\\beta)=\\mathbf{0}_{V_1}$。$A(\\alpha+\\beta) = A(\\alpha)+A(\\beta) = \\mathbf{0}_{V_1}+\\mathbf{0}_{V_1} = \\mathbf{0}_{V_1}$。所以 $\\alpha+\\beta \\in \\text{Ker}(A)$。 若 $\\alpha \\in \\text{Ker}(A)$ 且 $k \\in F$, 则 $A(\\alpha)=\\mathbf{0}_{V_1}$。$A(k\\alpha) = kA(\\alpha) = k\\mathbf{0}_{V_1} = \\mathbf{0}_{V_1}$。所以 $k\\alpha \\in \\text{Ker}(A)$。因此 $\\text{Ker}(A)$ 是 $V$ 的子空间。 定理: 线性映射 $A: V \\to V_1$ 是单射的充要条件是 $\\text{Ker}(A) = \\{\\mathbf{0}_V\\}$。 证明:($\\Rightarrow$) 假设 $A$ 是单射。已知 $A(\\mathbf{0}_V) = \\mathbf{0}_{V_1}$。如果存在 $\\alpha \\in \\text{Ker}(A)$ 且 $\\alpha \\neq \\mathbf{0}_V$，则 $A(\\alpha) = \\mathbf{0}_{V_1} = A(\\mathbf{0}_V)$。由于 $A$ 单射，必有 $\\alpha = \\mathbf{0}_V$，矛盾。所以 $\\text{Ker}(A)$ 中只有零向量。($\\Leftarrow$) 假设 $\\text{Ker}(A) = \\{\\mathbf{0}_V\\}$。如果 $A(\\alpha_1) = A(\\alpha_2)$，则 $A(\\alpha_1) - A(\\alpha_2) = \\mathbf{0}_{V_1}$。由于 $A$ 线性，$A(\\alpha_1 - \\alpha_2) = \\mathbf{0}_{V_1}$。这意味着 $\\alpha_1 - \\alpha_2 \\in \\text{Ker}(A)$。因为 $\\text{Ker}(A) = \\{\\mathbf{0}_V\\}$，所以 $\\alpha_1 - \\alpha_2 = \\mathbf{0}_V$，即 $\\alpha_1 = \\alpha_2$。因此 $A$ 是单射。 3.2 像 (Image / Range) 设 $A: V \\to V_1$ 是一个线性映射。 $A$ 的像 (image) 或值域 (range)，记作 $\\text{Im}(A)$ 或 $A(V)$，定义为：$\\text{Im}(A) = \\{A(\\alpha) | \\alpha \\in V\\}$。(即 $V$ 中所有向量在 $A$ 下的像构成的集合)。 定理: $\\text{Im}(A)$ 是 $V_1$ 的一个子空间。 证明: $A(\\mathbf{0}_V) = \\mathbf{0}_{V_1} \\in \\text{Im}(A)$。所以 $\\text{Im}(A)$ 非空。 若 $\\beta_1, \\beta_2 \\in \\text{Im}(A)$, 则存在 $\\alpha_1, \\alpha_2 \\in V$ 使得 $A(\\alpha_1)=\\beta_1, A(\\alpha_2)=\\beta_2$。$\\beta_1+\\beta_2 = A(\\alpha_1)+A(\\alpha_2) = A(\\alpha_1+\\alpha_2)$。由于 $\\alpha_1+\\alpha_2 \\in V$，所以 $\\beta_1+\\beta_2 \\in \\text{Im}(A)$。 若 $\\beta \\in \\text{Im}(A)$ 且 $k \\in F$, 则存在 $\\alpha \\in V$ 使得 $A(\\alpha)=\\beta$。$k\\beta = kA(\\alpha) = A(k\\alpha)$。由于 $k\\alpha \\in V$，所以 $k\\beta \\in \\text{Im}(A)$。因此 $\\text{Im}(A)$ 是 $V_1$ 的子空间。 显然，$A$ 是满射的充要条件是 $\\text{Im}(A) = V_1$。 3.3 秩-零度定理 (Rank-Nullity Theorem / Dimension Theorem) 第一同构定理 (线性映射形式):设 $A: V \\to V_1$ 是一个线性映射。则商空间 $V/\\text{Ker}(A)$ 与像空间 $\\text{Im}(A)$ 线性同构。$V/\\text{Ker}(A) \\cong \\text{Im}(A)$。同构映射 $\\sigma: V/\\text{Ker}(A) \\to \\text{Im}(A)$ 定义为 $\\sigma(\\alpha + \\text{Ker}(A)) = A(\\alpha)$。需要验证 $\\sigma$ 是良定义的 (well-defined)：如果 $\\alpha + \\text{Ker}(A) = \\beta + \\text{Ker}(A)$，则 $\\alpha-\\beta \\in \\text{Ker}(A)$，所以 $A(\\alpha-\\beta)=\\mathbf{0}_{V_1}$，$A(\\alpha)=A(\\beta)$。然后验证 $\\sigma$ 是线性的、单射的、满射的。 秩-零度定理: 设 $V$ 是有限维向量空间，$A: V \\to V_1$ 是一个线性映射。则$\\dim(\\text{Ker}(A)) + \\dim(\\text{Im}(A)) = \\dim(V)$。 $\\dim(\\text{Ker}(A))$ 称为 $A$ 的零度 (nullity of $A$)。 $\\dim(\\text{Im}(A))$ 称为 $A$ 的秩 (rank of $A$)，记作 $\\text{rank}(A)$ 或 $r(A)$。 定理可写为: $\\text{nullity}(A) + \\text{rank}(A) = \\dim(V)$。 证明:设 $\\dim(V)=n$ 和 $\\dim(\\text{Ker}(A))=m \\le n$。取 $\\text{Ker}(A)$ 的一组基 $\\{\\alpha_1, \\dots, \\alpha_m\\}$。将这组基扩充为 $V$ 的一组基 $\\{\\alpha_1, \\dots, \\alpha_m, \\alpha_{m+1}, \\dots, \\alpha_n\\}$。我们想证明 $\\{A(\\alpha_{m+1}), \\dots, A(\\alpha_n)\\}$ 是 $\\text{Im}(A)$ 的一组基。 生成性: 任意 $\\beta \\in \\text{Im}(A)$，存在 $\\alpha \\in V$ 使得 $A(\\alpha)=\\beta$。$\\alpha = c_1\\alpha_1 + \\dots + c_m\\alpha_m + c_{m+1}\\alpha_{m+1} + \\dots + c_n\\alpha_n$。$\\beta = A(\\alpha) = c_1A(\\alpha_1) + \\dots + c_mA(\\alpha_m) + c_{m+1}A(\\alpha_{m+1}) + \\dots + c_nA(\\alpha_n)$。由于 $\\alpha_1, \\dots, \\alpha_m \\in \\text{Ker}(A)$, $A(\\alpha_1)=\\dots=A(\\alpha_m)=\\mathbf{0}_{V_1}$。所以 $\\beta = c_{m+1}A(\\alpha_{m+1}) + \\dots + c_nA(\\alpha_n)$。这表明 $\\{A(\\alpha_{m+1}), \\dots, A(\\alpha_n)\\}$ 生成 $\\text{Im}(A)$。 线性无关性: 设 $k_{m+1}A(\\alpha_{m+1}) + \\dots + k_nA(\\alpha_n) = \\mathbf{0}_{V_1}$。则 $A(k_{m+1}\\alpha_{m+1} + \\dots + k_n\\alpha_n) = \\mathbf{0}_{V_1}$。这意味着向量 $\\gamma = k_{m+1}\\alpha_{m+1} + \\dots + k_n\\alpha_n$ 属于 $\\text{Ker}(A)$。所以 $\\gamma$ 可以表示为 $\\text{Ker}(A)$ 的基的线性组合: $\\gamma = d_1\\alpha_1 + \\dots + d_m\\alpha_m$。因此 $k_{m+1}\\alpha_{m+1} + \\dots + k_n\\alpha_n - d_1\\alpha_1 - \\dots - d_m\\alpha_m = \\mathbf{0}_V$。由于 $\\{\\alpha_1, \\dots, \\alpha_n\\}$ 是 $V$ 的基，它们线性无关，所以所有系数 $k_j=0$ 和 $d_i=0$。特别是 $k_{m+1} = \\dots = k_n = 0$。所以 $\\{A(\\alpha_{m+1}), \\dots, A(\\alpha_n)\\}$ 线性无关。因此，$\\{A(\\alpha_{m+1}), \\dots, A(\\alpha_n)\\}$ 是 $\\text{Im}(A)$ 的基，其元素个数为 $n-m$。所以 $\\dim(\\text{Im}(A)) = n-m = \\dim(V) - \\dim(\\text{Ker}(A))$。即 $\\dim(\\text{Ker}(A)) + \\dim(\\text{Im}(A)) = \\dim(V)$。 推论 (有限维空间中单射、满射、同构的等价性):设 $V, V_1$ 是有限维向量空间且 $\\dim V = \\dim V_1 = n$。对于线性映射 $A: V \\to V_1$，下列条件等价：(a) $A$ 是单射 ($\\text{Ker}(A) = \\{\\mathbf{0}_V\\}$，即 $\\dim(\\text{Ker}(A))=0$)。(b) $A$ 是满射 ($\\text{Im}(A) = V_1$，即 $\\dim(\\text{Im}(A))=n$)。(c) $A$ 是线性同构。 证明:(a) $\\Rightarrow$ (b): 若 $A$ 单射，则 $\\dim(\\text{Ker}(A))=0$。由秩-零度定理，$\\dim(\\text{Im}(A)) = \\dim(V) - 0 = n$。因为 $\\text{Im}(A)$ 是 $V_1$ 的 $n$ 维子空间且 $\\dim V_1 = n$，所以 $\\text{Im}(A) = V_1$，$A$ 是满射。(b) $\\Rightarrow$ (a): 若 $A$ 满射，则 $\\dim(\\text{Im}(A))=n$。由秩-零度定理，$\\dim(\\text{Ker}(A)) = \\dim(V) - n = n-n=0$。所以 $\\text{Ker}(A) = \\{\\mathbf{0}_V\\}$，$A$ 是单射。(a) and (b) $\\Rightarrow$ (c): $A$ 是单射且满射，所以是同构。(c) $\\Rightarrow$ (a) and (b): 同构定义包含单射和满射。 特别地，对于有限维空间 $V$ 上的线性变换 $A: V \\to V$，$A$ 是同构 (可逆) $\\iff A$ 是单射 $\\iff A$ 是满射。 3.4 线性变换的不变子空间与矩阵表示 不变子空间 (Invariant Subspace):设 $A: V \\to V$ 是一个线性变换，$U$ 是 $V$ 的一个子空间。如果对于任意 $\\alpha \\in U$，都有 $A(\\alpha) \\in U$ (即 $A(U) \\subseteq U$)，则称 $U$ 是 $A$ 的不变子空间 (或 $A$-不变子空间)。 此时，$A$ 在 $U$ 上的限制 $A|_U: U \\to U$ 是一个定义在 $U$ 上的线性变换。 平凡的不变子空间: $\\{\\mathbf{0}_V\\}$ 和 $V$ 本身。 $\\text{Ker}(A)$ 和 $\\text{Im}(A)$ 都是 $A$ 的不变子空间 (对于 $\\text{Im}(A)$，需要 $A(\\text{Im}(A)) \\subseteq \\text{Im}(A)$，这是显然的)。 特征子空间 (eigenspace) $V_\\lambda = \\{\\alpha \\in V | A\\alpha = \\lambda\\alpha\\}$ 是 $A$ 的不变子空间。 不变子空间与矩阵的块上三角化:如果 $U$ 是 $A$ 的 $r$ 维不变子空间，选择 $U$ 的一组基 $\\{\\alpha_1, \\dots, \\alpha_r\\}$，并将其扩充为 $V$ 的一组基 $\\{\\alpha_1, \\dots, \\alpha_r, \\alpha_{r+1}, \\dots, \\alpha_n\\}$。则 $A$ 在这组基下的矩阵表示具有块上三角形式:$M = \\begin{pmatrix} A_{11} &amp; A_{12} \\\\ O &amp; A_{22} \\end{pmatrix}$其中 $A_{11}$ 是 $A|_U$ 在基 $\\{\\alpha_1, \\dots, \\alpha_r\\}$ 下的 $r \\times r$ 矩阵。$A_{12}$ 是 $m \\times (n-m)$ 矩阵，$A_{22}$ 是 $(n-m) \\times (n-m)$ 矩阵。左下角的 $O$ 是 $(n-r) \\times r$ 的零矩阵，表示 $A(\\alpha_i)$ ($i=1..r$) 的分量中，对应于 $\\alpha_{r+1}, \\dots, \\alpha_n$ 的系数全为零。 不变子空间的直和分解与矩阵的块对角化:如果 $V = U_1 \\oplus U_2 \\oplus \\dots \\oplus U_k$，其中每个 $U_i$ 都是 $A$ 的不变子空间。选择 $V$ 的一组基，该基是由每个 $U_i$ 的基拼接而成。则 $A$ 在这组基下的矩阵表示为块对角形式:$M = \\text{diag}(A_1, A_2, \\dots, A_k) = \\begin{pmatrix} A_1 &amp; &amp; &amp; O \\\\ &amp; A_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ O &amp; &amp; &amp; A_k \\end{pmatrix}$其中 $A_i$ 是 $A|_{U_i}$ 在 $U_i$ 的基下的矩阵。寻找不变子空间并将向量空间分解为不变子空间的直和，是简化线性变换 (及其矩阵表示) 的核心策略 (例如，对角化、Jordan标准型)。 可交换线性变换族与公共不变子空间/特征向量:如果一组线性变换 $A, B$ 可交换，即 $AB=BA$。那么 $B$ 的核 $\\text{Ker}(B)$、像 $\\text{Im}(B)$ 以及任何特征子空间 $V_\\lambda(B)$ 都是 $A$ 的不变子空间。 证明 (以 $V_\\lambda(B)$ 为例): 设 $\\alpha \\in V_\\lambda(B)$, 即 $B\\alpha = \\lambda\\alpha$。我们需要证明 $A\\alpha \\in V_\\lambda(B)$, 即 $B(A\\alpha) = \\lambda(A\\alpha)$。$B(A\\alpha) = (BA)\\alpha = (AB)\\alpha = A(B\\alpha) = A(\\lambda\\alpha) = \\lambda(A\\alpha)$。所以 $A\\alpha \\in V_\\lambda(B)$。这个性质对于同时对角化一组可交换的矩阵非常重要。 4. 线性变换/线性映射的矩阵表示4.1 线性变换的矩阵 (Matrix of a Linear Operator) 设 $V$ 是 $n$ 维向量空间，$\\mathcal{B} = \\{\\alpha_1, \\dots, \\alpha_n\\}$ 是 $V$ 的一组有序基。 设 $A: V \\to V$ 是一个线性变换。 对于每个基向量 $\\alpha_j$, $A(\\alpha_j)$ 是 $V$ 中的向量，可以唯一表示为基 $\\mathcal{B}$ 的线性组合:$A(\\alpha_1) = a_{11}\\alpha_1 + a_{21}\\alpha_2 + \\dots + a_{n1}\\alpha_n$$A(\\alpha_2) = a_{12}\\alpha_1 + a_{22}\\alpha_2 + \\dots + a_{n2}\\alpha_n$$\\vdots$$A(\\alpha_n) = a_{1n}\\alpha_1 + a_{2n}\\alpha_2 + \\dots + a_{nn}\\alpha_n$ 将 $A(\\alpha_j)$ 在基 $\\mathcal{B}$ 下的坐标向量 $(a_{1j}, a_{2j}, \\dots, a_{nj})^T$ 作为矩阵的第 $j$ 列，得到一个 $n \\times n$ 矩阵:$[A]_{\\mathcal{B}} = \\begin{pmatrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn}\\end{pmatrix}$这个矩阵称为线性变换 $A$ 在基 $\\mathcal{B}$ 下的矩阵表示 (或简称矩阵)。 坐标表示: 如果向量 $\\alpha \\in V$ 在基 $\\mathcal{B}$ 下的坐标向量为 $[\\alpha]_{\\mathcal{B}} = \\mathbf{x} = (x_1, \\dots, x_n)^T$ (即 $\\alpha = \\sum x_j \\alpha_j$)，向量 $A(\\alpha) \\in V$ 在基 $\\mathcal{B}$ 下的坐标向量为 $[A(\\alpha)]_{\\mathcal{B}} = \\mathbf{y} = (y_1, \\dots, y_n)^T$。则 $\\mathbf{y} = [A]_{\\mathcal{B}} \\mathbf{x}$。 证明:$A(\\alpha) = A(\\sum_j x_j \\alpha_j) = \\sum_j x_j A(\\alpha_j)$$= \\sum_j x_j (\\sum_i a_{ij} \\alpha_i) = \\sum_i (\\sum_j a_{ij} x_j) \\alpha_i$.所以 $y_i = \\sum_j a_{ij} x_j$，这正是矩阵乘法 $\\mathbf{y} = M \\mathbf{x}$ 的第 $i$ 个分量。(PPT中可能写成行向量形式 $A(\\alpha_1, \\dots, \\alpha_n) = (\\alpha_1, \\dots, \\alpha_n) [A]_{\\mathcal{B}}$，如果将基向量看作行向量的组合)。 例子: $V = P_3(\\mathbb{R})$ (次数不超过3的多项式空间)，基 $\\mathcal{B} = \\{1, x, x^2, x^3\\}$。线性变换 $D: V \\to V$，$D(p(x)) = p’(x)$ (微分算子)。$D(1) = 0 = 0 \\cdot 1 + 0 \\cdot x + 0 \\cdot x^2 + 0 \\cdot x^3 \\implies \\text{col}_1 = (0,0,0,0)^T$$D(x) = 1 = 1 \\cdot 1 + 0 \\cdot x + 0 \\cdot x^2 + 0 \\cdot x^3 \\implies \\text{col}_2 = (1,0,0,0)^T$$D(x^2) = 2x = 0 \\cdot 1 + 2 \\cdot x + 0 \\cdot x^2 + 0 \\cdot x^3 \\implies \\text{col}_3 = (0,2,0,0)^T$$D(x^3) = 3x^2 = 0 \\cdot 1 + 0 \\cdot x + 3 \\cdot x^2 + 0 \\cdot x^3 \\implies \\text{col}_4 = (0,0,3,0)^T$$[D]_{\\mathcal{B}} = \\begin{pmatrix}0 &amp; 1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 2 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 3 \\\\0 &amp; 0 &amp; 0 &amp; 0\\end{pmatrix}$。 4.2 线性映射的矩阵 (Matrix of a Linear Map) 设 $V$ 是 $n$ 维向量空间，$\\mathcal{B}_V = \\{\\alpha_1, \\dots, \\alpha_n\\}$ 是 $V$ 的一组有序基。 设 $V_1$ 是 $m$ 维向量空间，$\\mathcal{B}_{V_1} = \\{\\beta_1, \\dots, \\beta_m\\}$ 是 $V_1$ 的一组有序基。 设 $A: V \\to V_1$ 是一个线性映射。 对于每个 $V$ 的基向量 $\\alpha_j$, $A(\\alpha_j)$ 是 $V_1$ 中的向量，可以唯一表示为 $V_1$ 的基 $\\mathcal{B}_{V_1}$ 的线性组合:$A(\\alpha_j) = a_{1j}\\beta_1 + a_{2j}\\beta_2 + \\dots + a_{mj}\\beta_m$ 将 $A(\\alpha_j)$ 在基 $\\mathcal{B}_{V_1}$ 下的坐标向量 $(a_{1j}, a_{2j}, \\dots, a_{mj})^T$ 作为矩阵的第 $j$ 列，得到一个 $m \\times n$ 矩阵:$[A]_{\\mathcal{B}_V, \\mathcal{B}_{V_1}} = \\begin{pmatrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn}\\end{pmatrix}$这个矩阵称为线性映射 $A$ 关于基 $\\mathcal{B}_V$ 和 $\\mathcal{B}_{V_1}$ 的矩阵表示。 坐标表示: 如果 $[\\alpha]_{\\mathcal{B}_V} = \\mathbf{x}$，则 $[A(\\alpha)]_{\\mathcal{B}_{V_1}} = [A]_{\\mathcal{B}_V, \\mathcal{B}_{V_1}} \\mathbf{x}$。 同构关系: $\\text{Hom}(V, V_1)$ 与 $m \\times n$ 矩阵的空间 $M_{m \\times n}(F)$ 线性同构。选择固定的基后，每个线性映射对应一个唯一的矩阵，反之亦然。这个同构保持加法和标量乘法。如果 $A, B: V \\to V_1$ 对应矩阵 $M_A, M_B$，则 $A+B$ 对应 $M_A+M_B$，$kA$ 对应 $kM_A$。 复合映射与矩阵乘积:设 $A: V \\to U$，$B: U \\to W$ 是线性映射。$\\mathcal{B}_V, \\mathcal{B}_U, \\mathcal{B}_W$ 分别是 $V, U, W$ 的基。$[A]_{\\mathcal{B}_V, \\mathcal{B}_U}$ 是 $A$ 的矩阵，$M_A$。$[B]_{\\mathcal{B}_U, \\mathcal{B}_W}$ 是 $B$ 的矩阵，$M_B$。则复合映射 $B \\circ A: V \\to W$ 的矩阵是 $[B \\circ A]_{\\mathcal{B}_V, \\mathcal{B}_W} = M_B M_A$ (矩阵乘积)。 秩与核的维数:$\\text{rank}(A)$ (线性映射 $A$ 的秩) 等于其任一矩阵表示 $M_A$ 的秩 $\\text{rank}(M_A)$。$\\dim(\\text{Ker}(A))$ (线性映射 $A$ 的零度) 等于矩阵方程 $M_A \\mathbf{x} = \\mathbf{0}$ 的解空间的维数 (即 $n - \\text{rank}(M_A)$)。 4.3 基变换与坐标变换 (Change of Basis) 设 $V$ 是 $n$ 维向量空间，$\\mathcal{B} = \\{\\alpha_1, \\dots, \\alpha_n\\}$ 和 $\\mathcal{B}’ = \\{\\alpha’_1, \\dots, \\alpha’_n\\}$ 是 $V$ 的两组有序基。 过渡矩阵 (Transition Matrix):将新基 $\\mathcal{B}’$ 中的每个向量 $\\alpha’_j$ 用旧基 $\\mathcal{B}$ 表示:$\\alpha’_j = p_{1j}\\alpha_1 + p_{2j}\\alpha_2 + \\dots + p_{nj}\\alpha_n$。矩阵 $P = (p_{ij})$ (其中第 $j$ 列是 $\\alpha’_j$ 在基 $\\mathcal{B}$ 下的坐标向量 $[\\alpha’_j]_{\\mathcal{B}}$) 称为从旧基 $\\mathcal{B}$ 到新基 $\\mathcal{B}’$ 的过渡矩阵 (Change-of-basis matrix from $\\mathcal{B}$ to $\\mathcal{B}’$)。(注意：有些教材定义 $P$ 为从 $\\mathcal{B}’$ 到 $\\mathcal{B}$ 的过渡矩阵，此时列是旧基在新基下的坐标，与这里的定义相反，导致后续公式中 $P$ 和 $P^{-1}$ 的位置不同。这里的定义更符合 $[\\mathbf{x}]_{\\mathcal{B}} = P [\\mathbf{x}]_{\\mathcal{B}’}$ 的形式。)PPT的写法 $A(\\alpha_1, \\dots, \\alpha_n) = (\\alpha_1, \\dots, \\alpha_n) A$ 暗示 $(\\alpha’_1, \\dots, \\alpha’_n) = (\\alpha_1, \\dots, \\alpha_n) P$。即 $\\alpha’_j = \\sum_i \\alpha_i P_{ij}$。 坐标变换公式:设向量 $v \\in V$。它在旧基 $\\mathcal{B}$ 下的坐标向量为 $[\\mathbf{v}]_{\\mathcal{B}} = \\mathbf{x}$。它在新基 $\\mathcal{B}’$ 下的坐标向量为 $[\\mathbf{v}]_{\\mathcal{B}’} = \\mathbf{x}’$。则 $\\mathbf{x} = P \\mathbf{x}’$ 或 $\\mathbf{x}’ = P^{-1} \\mathbf{x}$。(如果 $P$ 的列是新基在旧基下的坐标)。 证明:$v = \\sum_j x’_j \\alpha’_j = \\sum_j x’_j (\\sum_i p_{ij} \\alpha_i) = \\sum_i (\\sum_j p_{ij} x’_j) \\alpha_i$.同时 $v = \\sum_i x_i \\alpha_i$.所以 $x_i = \\sum_j p_{ij} x’_j$，即 $\\mathbf{x} = P\\mathbf{x}’$. 线性变换矩阵在不同基下的关系 (相似变换):设 $A: V \\to V$ 是一个线性变换。$M = [A]_{\\mathcal{B}}$ 是 $A$ 在旧基 $\\mathcal{B}$ 下的矩阵。$M’ = [A]_{\\mathcal{B}’}$ 是 $A$ 在新基 $\\mathcal{B}’$ 下的矩阵。$P$ 是从基 $\\mathcal{B}$ 到基 $\\mathcal{B}’$ 的过渡矩阵 (其列为新基向量在旧基下的坐标)。则 $M’ = P^{-1} M P$。 证明:对于任意 $v \\in V$，设其在新基 $\\mathcal{B}’$ 下的坐标为 $\\mathbf{x}’$。则其在旧基 $\\mathcal{B}$ 下的坐标为 $\\mathbf{x} = P\\mathbf{x}’$。$A(v)$ 在旧基 $\\mathcal{B}$ 下的坐标为 $M\\mathbf{x} = MP\\mathbf{x}’$。$A(v)$ 在新基 $\\mathcal{B}’$ 下的坐标为 $M’\\mathbf{x}’$。由于 $P$ 将新基坐标转换为旧基坐标，所以 $P^{-1}$ 将旧基坐标转换为新基坐标。因此 $M’\\mathbf{x}’ = [A(v)]_{\\mathcal{B}’} = P^{-1} [A(v)]_{\\mathcal{B}} = P^{-1} (M\\mathbf{x}) = P^{-1} (MP\\mathbf{x}’)$。由于这对任意 $\\mathbf{x}’$ 成立，所以 $M’ = P^{-1}MP$。 称矩阵 $M$ 和 $M’$ 相似 (similar)。相似的矩阵代表同一个线性变换在不同基下的表示。它们有相同的行列式、迹、特征值、秩等不变量。 5. Kronecker 积 (Tensor Product of Matrices) 与线性矩阵方程(这部分内容在典型的初等线性代数中不常见，属于更高级的主题) 5.1 vec 算子 (Vectorization) 对于 $m \\times n$ 矩阵 $A=(a_{ij})$，$\\text{vec}(A)$ (或 $\\text{cs}(A)$ 表示 column stacking) 是一个 $mn \\times 1$ 的列向量，通过将 $A$ 的列按顺序堆叠而成：$\\text{vec}(A) = (a_{11}, a_{21}, \\dots, a_{m1}, a_{12}, a_{22}, \\dots, a_{m2}, \\dots, a_{1n}, a_{2n}, \\dots, a_{mn})^T$。(PPT中也用了行堆叠 $rs(A)$，但列堆叠更常见)。 5.2 Kronecker 积 (Kronecker Product) 设 $A$ 是 $m \\times n$ 矩阵，$B$ 是 $p \\times q$ 矩阵。 $A$ 与 $B$ 的 Kronecker 积 (或张量积)，记作 $A \\otimes B$，是一个 $mp \\times nq$ 的分块矩阵：$A \\otimes B = \\begin{pmatrix}a_{11}B &amp; a_{12}B &amp; \\cdots &amp; a_{1n}B \\\\a_{21}B &amp; a_{22}B &amp; \\cdots &amp; a_{2n}B \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\a_{m1}B &amp; a_{m2}B &amp; \\cdots &amp; a_{mn}B\\end{pmatrix}$。 5.3 Kronecker 积的性质 结合律: $(A \\otimes B) \\otimes C = A \\otimes (B \\otimes C)$。 双线性 (分配律):$(A_1+A_2) \\otimes B = A_1 \\otimes B + A_2 \\otimes B$$A \\otimes (B_1+B_2) = A \\otimes B_1 + A \\otimes B_2$$(kA) \\otimes B = A \\otimes (kB) = k(A \\otimes B)$ 混合乘积性质: $(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD)$ (要求矩阵乘积 $AC$ 和 $BD$ 有意义)。 证明思路: 将 $A \\otimes B$ 和 $C \\otimes D$ 写成分块矩阵形式，然后进行分块矩阵乘法。$(A \\otimes B)_{ik,jl} = A_{ij}B_{kl}$ (这里用索引表示块中的元素)。$(AC \\otimes BD)_{ik,jl} = (AC)_{ij}(BD)_{kl} = (\\sum_p A_{ip}C_{pj}) (\\sum_q B_{kq}D_{ql})$$((A \\otimes B)(C \\otimes D))_{ik,jl} = \\sum_{pq} (A \\otimes B)_{ik,pq} (C \\otimes D)_{pq,jl} = \\sum_{pq} A_{ip}B_{kq} C_{pj}D_{ql}$两者通过重新排列求和顺序可以证明相等。 逆: 如果 $A, B$ 均可逆，则 $A \\otimes B$ 也可逆，且 $(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}$。 证明: 利用混合乘积性质：$(A^{-1} \\otimes B^{-1})(A \\otimes B) = (A^{-1}A) \\otimes (B^{-1}B) = I_m \\otimes I_p = I_{mp}$ (假设 $A$ 是 $m \\times m$, $B$ 是 $p \\times p$)。 转置: $(A \\otimes B)^T = A^T \\otimes B^T$。 行列式: 若 $A$ 是 $m \\times m$ 矩阵, $B$ 是 $n \\times n$ 矩阵，则 $|A \\otimes B| = |A|^n |B|^m$。 证明思路:可以将 $A$ 通过相似变换化为上三角阵 (Jordan标准型也可)。$A = PJP^{-1}$。$A \\otimes B = (PJP^{-1}) \\otimes (Q L Q^{-1})$ (如果 $B$ 也能三角化)。$A \\otimes B = (P \\otimes I)(J \\otimes B)(P^{-1} \\otimes I)$ (如果 $B$ 是三角阵，或者用更一般的性质)。若 $A$ 是上三角，则 $A \\otimes B$ 是分块上三角，其对角块是 $a_{ii}B$。$|A \\otimes B| = \\prod_i |a_{ii}B| = \\prod_i a_{ii}^n |B| = (\\prod_i a_{ii})^n |B|^m$ (这里有个小错误，应该是 $(\\prod_i a_{ii})^n |B|^m$ if B is $m \\times m$ and $A$ is $n \\times n$ from the formula)If A is $m \\times m$ and B is $n \\times n$: $\\prod_i |a_{ii}B| = \\prod_i (a_{ii}^n |B|) = (\\prod_i a_{ii})^n |B|^m = |A|^n |B|^m$. 秩: $\\text{rank}(A \\otimes B) = \\text{rank}(A) \\text{rank}(B)$。 证明思路: 将 $A, B$ 化为等价标准形 $A = P_A D_A Q_A$, $B = P_B D_B Q_B$，其中 $D_A, D_B$ 是对角线上为1或0的矩阵。$A \\otimes B = (P_A D_A Q_A) \\otimes (P_B D_B Q_B) = (P_A \\otimes P_B)(D_A \\otimes D_B)(Q_A \\otimes Q_B)$。$P_A \\otimes P_B$ 和 $Q_A \\otimes Q_B$ 是可逆的 (因为 $P_A, P_B, Q_A, Q_B$ 可逆)。$D_A \\otimes D_B$ 是一个对角矩阵，其对角线上的1的个数等于 $\\text{rank}(A) \\text{rank}(B)$。所以 $\\text{rank}(A \\otimes B) = \\text{rank}(D_A \\otimes D_B) = \\text{rank}(A)\\text{rank}(B)$。 迹: $\\text{tr}(A \\otimes B) = \\text{tr}(A) \\text{tr}(B)$。 证明:$\\text{tr}(A \\otimes B) = \\sum_i (A \\otimes B)_{ii,ii}$ (对角块的迹之和)。对角块为 $a_{ii}B$。所以 $\\text{tr}(A \\otimes B) = \\sum_i \\text{tr}(a_{ii}B) = \\sum_i a_{ii}\\text{tr}(B) = (\\sum_i a_{ii})\\text{tr}(B) = \\text{tr}(A)\\text{tr}(B)$。 5.4 线性矩阵方程 $AXB=C$ 的解 vec 算子与 Kronecker 积的关系:$\\text{vec}(AXB) = (B^T \\otimes A) \\text{vec}(X)$。其中 $A$ 是 $m \\times n$, $X$ 是 $n \\times p$, $B$ 是 $p \\times q$。则 $AXB$ 是 $m \\times q$。$\\text{vec}(AXB)$ 是 $mq \\times 1$。$B^T$ 是 $q \\times p$, $A$ 是 $m \\times n$。$B^T \\otimes A$ 是 $mq \\times np$。$\\text{vec}(X)$ 是 $np \\times 1$。 求解线性矩阵方程:考虑矩阵方程 $\\sum_{i=1}^s A_i X B_i = C$。对其两边作用 $\\text{vec}$ 算子：$\\text{vec}(\\sum_{i=1}^s A_i X B_i) = \\text{vec}(C)$$\\sum_{i=1}^s \\text{vec}(A_i X B_i) = \\text{vec}(C)$$\\sum_{i=1}^s (B_i^T \\otimes A_i) \\text{vec}(X) = \\text{vec}(C)$令 $G = \\sum_{i=1}^s (B_i^T \\otimes A_i)$，$\\mathbf{x} = \\text{vec}(X)$，$\\mathbf{c} = \\text{vec}(C)$。则原矩阵方程等价于线性方程组 $G\\mathbf{x} = \\mathbf{c}$。 解的存在性: 矩阵方程 $\\sum A_i X B_i = C$ 有解的充要条件是线性方程组 $G\\mathbf{x} = \\mathbf{c}$ 有解，即 $\\text{rank}(G) = \\text{rank}((G|\\mathbf{c}))$。 唯一解: 如果 $G$ 可逆 (当 $X$ 是方阵且 $G$ 是方阵时)，则有唯一解 $\\mathbf{x} = G^{-1}\\mathbf{c}$，从而得到唯一的 $X$。 5.5 线性映射的张量积 (Tensor Product of Linear Maps) 设 $A: V \\to V’$ 和 $B: W \\to W’$ 是线性映射。 它们的张量积 $A \\otimes B: V \\otimes W \\to V’ \\otimes W’$ 是一个线性映射，定义在简单张量上为：$(A \\otimes B)(v \\otimes w) = A(v) \\otimes B(w)$并线性扩展到整个张量积空间 $V \\otimes W$。 如果 $V, W, V’, W’$ 都是有限维的，并选择了各自的基。设 $A$ 在相应基下的矩阵为 $M_A$，$B$ 的矩阵为 $M_B$。则线性映射 $A \\otimes B$ 在由 $V,W$ 的基构成的张量积基下的矩阵表示就是 $M_A \\otimes M_B$ (Kronecker 积)。(张量积基例如: 如果 $\\{\\mathbf{e}_i\\}$ 是 $V$ 的基, $\\{\\mathbf{f}_j\\}$ 是 $W$ 的基, 则 $\\{\\mathbf{e}_i \\otimes \\mathbf{f}_j\\}$ 是 $V \\otimes W$ 的基)。 这部分内容涵盖了抽象代数中映射的基本概念，然后深入到线性代数的核心——线性映射，包括其性质、与矩阵的关系、秩-零度定理、不变子空间等，最后还介绍了Kronecker积及其在线性矩阵方程中的应用。","link":"/MATH1408/Chapter4-MATH1408.html"},{"title":"Chapter6 MATH1408","text":"矩阵函数及其应用1. 矩阵序列与矩阵级数 (Sequences and Series of Matrices)1.1 矩阵序列的极限 (Limit of a Matrix Sequence) 定义: 设 $\\{A_n = (a_{ij}^{(n)})\\}$ 是一个 $s \\times t$ 矩阵序列。如果对所有的 $i, j$，标量序列 $\\{a_{ij}^{(n)}\\}$ 收敛于 $a_{ij}$，即 $\\lim_{n \\to \\infty} a_{ij}^{(n)} = a_{ij}$，则称矩阵序列 $\\{A_n\\}$ 收敛于矩阵 $A = (a_{ij})$，记为 $\\lim_{n \\to \\infty} A_n = A$。 例子:$\\lim_{n \\to \\infty} \\begin{pmatrix} 1 &amp; 1 - \\frac{1}{n} \\\\ \\log(1+\\frac{1}{n}) &amp; (1+\\frac{1}{n})^n \\end{pmatrix}^3 = \\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; e \\end{pmatrix}^3$ 1.2 矩阵序列极限的性质 (Properties of Limits of Matrix Sequences)设 $\\lim_{n \\to \\infty} A_n = A$ 和 $\\lim_{n \\to \\infty} B_n = B$。 $\\lim_{n \\to \\infty} (A_n \\pm B_n) = A \\pm B$ $\\lim_{n \\to \\infty} (A_n B_n) = AB$ (如果乘积有意义) (i) $\\lim_{n \\to \\infty} (kA_n) = kA$ (k为标量) (ii) $\\lim_{n \\to \\infty} (BA_nC) = BAC$ (B, C为常数矩阵，使得乘积有意义) (iii) 如果 $f(x)$ 是一个在 $A_n$ 的特征值及 $A$ 的特征值附近解析的函数 (PPT中表述为“任何多项式$f(x)$”，更一般地，如果$f$是连续的，且$A_n \\to A$，则$f(A_n) \\to f(A)$，特别是对多项式成立)。则 $\\lim_{n \\to \\infty} f(A_n) = f(\\lim_{n \\to \\infty} A_n) = f(A)$。 1.3 矩阵级数 (Series of Matrices) 定义: 设 $\\{A_n\\}$ ($n=1, 2, \\dots$) 是一个矩阵序列。考虑部分和序列 $S_n = A_1 + A_2 + \\dots + A_n$。如果序列 $\\{S_n\\}$ 收敛于矩阵 $S$，则称矩阵级数 $\\sum_{n=1}^{\\infty} A_n = A_1 + A_2 + \\dots + A_n + \\dots$ 收敛于 $S$。 性质: (i) 如果矩阵级数 $\\sum_{n=1}^{\\infty} A_n$ 收敛，则 $\\lim_{n \\to \\infty} A_n = O$ (零矩阵)。(这是收敛的必要条件，非充分条件) (ii) 如果级数 $\\sum A_n$ 和 $\\sum B_n$ 都收敛，则对于任意标量 $k, l$，级数 $\\sum (kA_n + lB_n)$ 收敛，且 $\\sum (kA_n + lB_n) = k \\sum A_n + l \\sum B_n$。 2. 矩阵幂级数与矩阵函数 (Matrix Power Series and Matrix Functions)2.1 矩阵幂级数 (Power Series of a Matrix) 定义: 对于一个 $n \\times n$ 的方阵 $A$，形如 $a_0 E + a_1 A + a_2 A^2 + \\dots + a_k A^k + \\dots = \\sum_{k=0}^{\\infty} a_k A^k$ 的级数称为矩阵 $A$ 的幂级数。 收敛性: 设标量幂级数 $f(x) = \\sum_{k=0}^{\\infty} a_k x^k$ 的收敛半径为 $R$。 如果矩阵 $A$ 的所有特征值 $\\lambda_i$ 都满足 $|\\lambda_i| &lt; R$，则矩阵幂级数 $\\sum_{k=0}^{\\infty} a_k A^k$ 绝对收敛。(这是一个充分条件)。 更准确地说，级数 $\\sum a_k A^k$ 收敛的充要条件是对于 $A$ 的每个Jordan块 $J_m(\\lambda)$，级数 $\\sum a_k J_m^k(\\lambda)$ 收敛。而后者收敛的充要条件是： $|\\lambda| &lt; R$。 如果 $|\\lambda| = R$，则级数 $\\sum_{k=0}^{\\infty} a_k x^k$ 在 $x=\\lambda$ 处收敛，并且其直到 $m-1$ 阶的导数级数也在 $x=\\lambda$ 处收敛。 2.2 通过 Jordan 标准型定义矩阵函数 设 $f(x)$ 是一个在包含矩阵 $A$ 的所有特征值的某个区域内解析的函数。 关键思想: 利用 $A$ 的 Jordan 标准型 $J = P^{-1}AP = \\text{diag}(J_1, J_2, \\dots, J_s)$，其中 $J_i$ 是 Jordan 块。则 $f(A) = P f(J) P^{-1} = P \\text{diag}(f(J_1), f(J_2), \\dots, f(J_s)) P^{-1}$。 计算 $f(J_i)$:设 $J_i = \\lambda_i E + N_i$ 是一个 $n_i \\times n_i$ 的 Jordan 块，其中 $N_i$ 是相应的幂零矩阵 ($N_i^{n_i}=O$)。如果 $f(x)$ 在 $\\lambda_i$ 处有泰勒展开 $f(x) = f(\\lambda_i) + f’(\\lambda_i)(x-\\lambda_i) + \\frac{f’’(\\lambda_i)}{2!}(x-\\lambda_i)^2 + \\dots$则 $f(J_i) = f(\\lambda_i E + N_i) = f(\\lambda_i)E + f’(\\lambda_i)N_i + \\frac{f’’(\\lambda_i)}{2!}N_i^2 + \\dots + \\frac{f^{(n_i-1)}(\\lambda_i)}{(n_i-1)!}N_i^{n_i-1}$。(因为 $N_i^k = O$ for $k \\ge n_i$)。 $f(J_i)$ 的矩阵形式:$f(J_i) = \\begin{pmatrix}f(\\lambda_i) &amp; f’(\\lambda_i) &amp; \\frac{f’’(\\lambda_i)}{2!} &amp; \\dots &amp; \\frac{f^{(n_i-1)}(\\lambda_i)}{(n_i-1)!} \\\\0 &amp; f(\\lambda_i) &amp; f’(\\lambda_i) &amp; \\dots &amp; \\frac{f^{(n_i-2)}(\\lambda_i)}{(n_i-2)!} \\\\\\vdots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\0 &amp; \\dots &amp; 0 &amp; f(\\lambda_i) &amp; f’(\\lambda_i) \\\\0 &amp; \\dots &amp; \\dots &amp; 0 &amp; f(\\lambda_i)\\end{pmatrix}$ 例子: 设 $J = \\begin{pmatrix} 3 &amp; 1 &amp; 0 \\\\ 0 &amp; 3 &amp; 1 \\\\ 0 &amp; 0 &amp; 3 \\end{pmatrix}$。这里 $\\lambda=3, n=3$。 计算 $e^J$: $f(x)=e^x, f’(x)=e^x, f’’(x)=e^x$。$e^J = \\begin{pmatrix}e^3 &amp; e^3 &amp; \\frac{e^3}{2!} \\\\0 &amp; e^3 &amp; e^3 \\\\0 &amp; 0 &amp; e^3\\end{pmatrix}$ 计算 $\\sin J$: $f(x)=\\sin x, f’(x)=\\cos x, f’’(x)=-\\sin x$。$\\sin J = \\begin{pmatrix}\\sin 3 &amp; \\cos 3 &amp; \\frac{-\\sin 3}{2!} \\\\0 &amp; \\sin 3 &amp; \\cos 3 \\\\0 &amp; 0 &amp; \\sin 3\\end{pmatrix}$ 定理 (矩阵函数收敛性):设幂级数 $f(x) = \\sum a_n x^n$ 的收敛半径为 $R$。如果矩阵 $A$ 的所有特征值的模都小于 $R$ (即谱半径 $\\rho(A) &lt; R$)，则矩阵幂级数 $f(A) = \\sum a_n A^n$ 收敛。 证明思路:将 $A$ 化为 Jordan 标准型 $J = P^{-1}AP$。则 $S_N(A) = P S_N(J) P^{-1}$，其中 $S_N(x) = \\sum_{k=0}^N a_k x^k$。当 $N \\to \\infty$时，$S_N(\\lambda_i) \\to f(\\lambda_i)$，$S_N^{(k)}(\\lambda_i) \\to f^{(k)}(\\lambda_i)$。因此 $S_N(J_i) \\to f(J_i)$ 对每个 Jordan 块成立。所以 $S_N(J) \\to f(J)$，从而 $S_N(A) \\to f(A)$。□ 2.3 常见矩阵函数 (Common Matrix Functions)(定义基于其泰勒级数，收敛半径为 $\\infty$) 指数函数 (Exponential Function):$e^A = \\sum_{n=0}^{\\infty} \\frac{A^n}{n!} = E + A + \\frac{A^2}{2!} + \\dots$ 正弦函数 (Sine Function):$\\sin A = \\sum_{n=1}^{\\infty} (-1)^{n-1} \\frac{A^{2n-1}}{(2n-1)!} = A - \\frac{A^3}{3!} + \\frac{A^5}{5!} - \\dots$ 余弦函数 (Cosine Function):$\\cos A = \\sum_{n=0}^{\\infty} (-1)^n \\frac{A^{2n}}{(2n)!} = E - \\frac{A^2}{2!} + \\frac{A^4}{4!} - \\dots$ 二项式级数 (Binomial Series):如果 $A$ 的所有特征值的模都小于 1 (即 $\\rho(A) &lt; 1$)，则$(E+A)^\\alpha = \\sum_{n=0}^{\\infty} \\binom{\\alpha}{n} A^n = E + \\alpha A + \\frac{\\alpha(\\alpha-1)}{2!}A^2 + \\dots$特别地，$(E-A)^{-1} = \\sum_{n=0}^{\\infty} A^n = E + A + A^2 + \\dots$ (Neumann 级数) 对数函数 (Logarithm Function):如果 $A$ 的所有特征值的模都小于 1 (即 $\\rho(A) &lt; 1$)，则$\\ln(E+A) = \\sum_{n=1}^{\\infty} (-1)^{n-1} \\frac{A^n}{n} = A - \\frac{A^2}{2} + \\frac{A^3}{3} - \\dots$ 2.4 矩阵值函数的导数 (Derivative of a Matrix-valued Function) 定义: 设 $A(t) = (a_{ij}(t))$ 是一个矩阵，其元素 $a_{ij}(t)$ 是变量 $t$ 的可微函数。则 $A(t)$ 的导数定义为 $\\frac{d}{dt}A(t) = A’(t) = (\\frac{d}{dt}a_{ij}(t))$。 性质: $(e^{At})’ = A e^{At}$ $(\\sin(At))’ = A \\cos(At)$ $(\\cos(At))’ = -A \\sin(At)$ 证明 $(e^{At})’$:$e^{At} = E + At + \\frac{A^2t^2}{2!} + \\dots + \\frac{A^n t^n}{n!} + \\dots$逐项求导 (在收敛域内可以这样做):$\\frac{d}{dt}(e^{At}) = 0 + A + \\frac{A^2 \\cdot 2t}{2!} + \\dots + \\frac{A^n \\cdot nt^{n-1}}{n!} + \\dots$$= A + A^2t + \\dots + \\frac{A^n t^{n-1}}{(n-1)!} + \\dots$$= A (E + At + \\dots + \\frac{A^{n-1}t^{n-1}}{(n-1)!} + \\dots) = A e^{At}$□ 3. 计算矩阵函数的其他方法 (Other Methods for Computing Matrix Functions)除了使用 Jordan 标准型，还有基于插值多项式的方法。 3.1 基于 Cayley-Hamilton 定理的插值 (Interpolation based on Cayley-Hamilton Theorem) 核心思想: 任何矩阵 $A$ 都满足其自身的特征方程。更一般地，它满足其最小多项式 $m_A(x)$。即 $m_A(A) = O$。 方法: 找到矩阵 $A$ 的最小多项式 $m_A(x)$。设 $\\deg(m_A(x)) = d \\le n$。 构造一个次数小于 $d$ 的多项式 $p(x) = c_0 + c_1 x + \\dots + c_{d-1} x^{d-1}$。 使得 $p(x)$ 与函数 $f(x)$ 在 $A$ 的特征值处 “一致”。具体来说，如果 $\\lambda_i$ 是 $m_A(x)$ 的一个 $k_i$ 重根 (也是 $A$ 的特征值)，则需要满足：$p(\\lambda_i) = f(\\lambda_i)$$p’(\\lambda_i) = f’(\\lambda_i)$…$p^{(k_i-1)}(\\lambda_i) = f^{(k_i-1)}(\\lambda_i)$对 $m_A(x)$ 的所有不同根 $\\lambda_i$ 及其重数 $k_i$ 都成立。 解出系数 $c_0, \\dots, c_{d-1}$。 则 $f(A) = p(A) = c_0 E + c_1 A + \\dots + c_{d-1} A^{d-1}$。 这个 $p(x)$ 称为 $f(x)$ 关于矩阵 $A$ (或其最小多项式) 的插值多项式 (Hermite 插值)。 定理: 如果 $p(x)$ 是满足上述条件的插值多项式，则 $f(A)=p(A)$。 证明思路:令 $g(x) = f(x) - p(x)$。则 $g^{(j)}(\\lambda_i) = 0$ for $j=0, \\dots, k_i-1$ 对 $m_A(x)$ 的每个根 $\\lambda_i$ 及其重数 $k_i$ 成立。这意味着 $m_A(x)$ 整除 $g(x)$，即 $g(x) = m_A(x) h(x)$ 对某个多项式 $h(x)$ 成立。因此 $g(A) = m_A(A) h(A) = O \\cdot h(A) = O$。所以 $f(A) - p(A) = O \\Rightarrow f(A) = p(A)$。□ 例子 1: $A = \\begin{pmatrix} -1 &amp; 4 \\\\ 3 &amp; 2 \\end{pmatrix}$，求 $e^A$。 特征多项式: $|\\lambda E - A| = \\lambda^2 - \\lambda - 10 = (\\lambda-5)(\\lambda+2)$。 最小多项式 $m_A(x) = (x-5)(x+2)$ (因为根是单根)。 设 $p(x) = ax+b$。$f(x)=e^x$。 $p(5) = f(5) \\Rightarrow 5a+b = e^5$ $p(-2) = f(-2) \\Rightarrow -2a+b = e^{-2}$ 解得 $a = \\frac{1}{7}(e^5 - e^{-2})$, $b = \\frac{1}{7}(2e^5 + 5e^{-2})$。 $e^A = p(A) = aA + bE = \\frac{1}{7}(e^5 - e^{-2})A + \\frac{1}{7}(2e^5 + 5e^{-2})E$。 例子 2: $A = \\begin{pmatrix} -2 &amp; 2 &amp; -2 \\\\ 4 &amp; -1 &amp; 2 \\\\ -1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ -2 &amp; 1 &amp; -1 &amp; 4 \\end{pmatrix}$ (这个矩阵似乎不完整或有误，假设是一个4x4矩阵)。特征多项式 $|\\lambda E - A| = (\\lambda-1)^3(\\lambda-2)$。假设最小多项式 $m_A(x) = (\\lambda-1)^2(\\lambda-2)$ (这取决于Jordan块的结构)。求 $\\cos(\\pi A)$。设 $f(x) = \\cos(\\pi x)$。设 $p(x) = ax^2+bx+c$ (次数小于 $m_A(x)$ 的次数3)。条件: $p(2) = f(2) \\Rightarrow 4a+2b+c = \\cos(2\\pi) = 1$ $p(1) = f(1) \\Rightarrow a+b+c = \\cos(\\pi) = -1$ $p’(1) = f’(1)$: $p’(x)=2ax+b$, $f’(x)=-\\pi \\sin(\\pi x)$。$2a+b = -\\pi \\sin(\\pi) = 0 \\Rightarrow b = -2a$。代入(2): $a-2a+c = -1 \\Rightarrow -a+c=-1 \\Rightarrow c=a-1$。代入(1): $4a+2(-2a)+(a-1) = 1 \\Rightarrow 4a-4a+a-1=1 \\Rightarrow a-1=1 \\Rightarrow a=2$。$b = -2(2) = -4$。$c = 2-1 = 1$。$p(x) = 2x^2 - 4x + 1$。$\\cos(\\pi A) = p(A) = 2A^2 - 4A + E$。 3.2 Lagrange-Sylvester 插值公式 (Lagrange-Sylvester Interpolation Formula)这是上述方法的一个更明确的公式化。 设矩阵 $A$ 的最小多项式为 $m_A(x) = \\prod_{i=1}^s (x-\\lambda_i)^{\\alpha_i}$，其中 $\\lambda_i$ 是互不相同的特征值，$\\sum \\alpha_i = d = \\deg(m_A(x))$。 $f(A)$ 可以表示为：$f(A) = p(A) = \\sum_{i=1}^s \\left[ \\sum_{j=0}^{\\alpha_i-1} \\frac{1}{j!} \\left( \\frac{d^j}{dx^j} \\frac{f(x)}{m_i(x)} \\right)_{x=\\lambda_i} (A-\\lambda_i E)^j \\right] m_i(A)$其中 $m_i(x) = \\frac{m_A(x)}{(x-\\lambda_i)^{\\alpha_i}}$ (即除去 $(x-\\lambda_i)^{\\alpha_i}$ 因子后的部分)。 特例 (最小多项式无重根):$m_A(x) = (x-\\lambda_1)(x-\\lambda_2)\\dots(x-\\lambda_d)$ (所有 $\\alpha_i=1$, $d=s$)$f(A) = \\sum_{i=1}^d f(\\lambda_i) Z_i(A)$其中 $Z_i(x) = \\frac{\\prod_{j \\ne i} (x-\\lambda_j)}{\\prod_{j \\ne i} (\\lambda_i-\\lambda_j)}$ 是 Lagrange 插值基多项式。$Z_i(A)$ 是投影算子。 例子 (最小多项式无重根): $A = \\begin{pmatrix} 2 &amp; -3 &amp; 0 \\\\ 1 &amp; -2 &amp; 0 \\\\ 2 &amp; 3 &amp; \\sqrt{3} \\end{pmatrix}$，求 $\\arctan A$。特征多项式: $|\\lambda E - A| = ((\\lambda-2)(\\lambda+2) - (-3)(1))(\\lambda-\\sqrt{3}) = (\\lambda^2-4+3)(\\lambda-\\sqrt{3}) = (\\lambda^2-1)(\\lambda-\\sqrt{3}) = (\\lambda-1)(\\lambda+1)(\\lambda-\\sqrt{3})$。这是最小多项式 (因为根是单根)。 $\\lambda_1=1, \\lambda_2=-1, \\lambda_3=\\sqrt{3}$。$p_1(x) = \\frac{(x+1)(x-\\sqrt{3})}{(1+1)(1-\\sqrt{3})} = \\frac{(x+1)(x-\\sqrt{3})}{2(1-\\sqrt{3})}$$p_2(x) = \\frac{(x-1)(x-\\sqrt{3})}{(-1-1)(-1-\\sqrt{3})} = \\frac{(x-1)(x-\\sqrt{3})}{2(1+\\sqrt{3})}$$p_3(x) = \\frac{(x-1)(x+1)}{(\\sqrt{3}-1)(\\sqrt{3}+1)} = \\frac{x^2-1}{2}$$\\arctan A = (\\arctan 1) p_1(A) + (\\arctan(-1)) p_2(A) + (\\arctan \\sqrt{3}) p_3(A)$$= \\frac{\\pi}{4} p_1(A) - \\frac{\\pi}{4} p_2(A) + \\frac{\\pi}{3} p_3(A)$代入 $p_i(A)$ 并化简得到 $\\frac{\\pi}{24}(5A^2 + 3\\sqrt{3}A - 8E)$。 特例 (最小多项式只有一个根):$m_A(x) = (x-\\lambda_1)^\\alpha$。则 $f(A) = p(A) = f(\\lambda_1)E + f’(\\lambda_1)(A-\\lambda_1 E) + \\dots + \\frac{f^{(\\alpha-1)}(\\lambda_1)}{(\\alpha-1)!}(A-\\lambda_1 E)^{\\alpha-1}$。 例子: $A = \\begin{pmatrix} 0 &amp; -1 \\\\ 4 &amp; 4 \\end{pmatrix}$，求 $f(A) = \\arcsin(A/4)$。特征多项式: $|\\lambda E - A| = \\lambda(\\lambda-4) - (-1)(4) = \\lambda^2-4\\lambda+4 = (\\lambda-2)^2$。最小多项式 $m_A(x) = (x-2)^2$ (因为 $A-2E = \\begin{pmatrix} -2 &amp; -1 \\\\ 4 &amp; 2 \\end{pmatrix} \\ne O$ )。$\\lambda_1=2, \\alpha=2$。$f(x) = \\arcsin(x/4)$。$f(2) = \\arcsin(2/4) = \\arcsin(1/2) = \\pi/6$。$f’(x) = \\frac{1}{4} \\frac{1}{\\sqrt{1-(x/4)^2}} = \\frac{1}{\\sqrt{16-x^2}}$。$f’(2) = \\frac{1}{\\sqrt{16-4}} = \\frac{1}{\\sqrt{12}} = \\frac{1}{2\\sqrt{3}}$。$f(A) = p(A) = f(2)E + f’(2)(A-2E) = \\frac{\\pi}{6}E + \\frac{1}{2\\sqrt{3}}(A-2E)$。 3.3 基于待定系数的插值 (PPT中的”待定系数法”)这是方法3.1的另一种表述，通过解线性方程组确定插值多项式的系数。 思想: 找到矩阵 $A$ 的最小多项式 $m_A(x) = (x-\\lambda_1)^{\\alpha_1} \\dots (x-\\lambda_s)^{\\alpha_s}$，$\\deg(m_A(x))=d$。 设插值多项式 $p(x) = c_0 + c_1 x + \\dots + c_{d-1} x^{d-1}$。 建立方程组:$p^{(k)}(\\lambda_i) = f^{(k)}(\\lambda_i)$ for $k=0, \\dots, \\alpha_i-1$ and $i=1, \\dots, s$。这会得到 $d$ 个关于 $c_0, \\dots, c_{d-1}$ 的线性方程。 解出 $c_j$，则 $f(A) = p(A)$。 例子: $A = \\begin{pmatrix} 3 &amp; 1 &amp; -3 \\\\ -7 &amp; -2 &amp; 9 \\\\ -2 &amp; -1 &amp; 4 \\end{pmatrix}$，求 $e^A$。特征多项式 $|\\lambda E - A| = (\\lambda-1)(\\lambda-2)^2$。假设这就是最小多项式 $m_A(x) = (x-1)(x-2)^2$ (需要验证 $(A-E)(A-2E) \\ne O$)。$d=3$。设 $p(x) = ax^2+bx+c$。 $f(x)=e^x$。条件: $p(1) = f(1) \\Rightarrow a+b+c = e^1 = e$ $p(2) = f(2) \\Rightarrow 4a+2b+c = e^2$ $p’(2) = f’(2)$: $p’(x)=2ax+b$, $f’(x)=e^x$。$4a+b = e^2$解这个关于 $a,b,c$ 的方程组。PPT中给出的答案 $e^A = e(A-2E)^2 + e^2(A-E)$ 是错误的。它似乎试图用一种特定形式的基多项式，但系数不对。正确的解法是通过解上述方程组得到 $a,b,c$，然后 $e^A = aA^2+bA+cE$。或者使用 PPT 最后一种方法:$f(A) = f(1)B_{10} + f(2)B_{20} + f’(2)B_{21}$，其中 $B_{ij}$ 是待定的常数矩阵。选择特定的函数 $f_1(x)=(x-2)^2$, $f_2(x)=(x-1)(x-2)$, $f_3(x)=(x-1)$ (使其在特征值处取特定值和导数值) 来确定 $B_{ij}$。例如，取 $f_1(x)=(x-2)^2$:$f_1(1)=1, f_1(2)=0, f_1’(2)=0$。则 $(A-2E)^2 = 1 \\cdot B_{10} + 0 \\cdot B_{20} + 0 \\cdot B_{21} = B_{10}$。取 $f_2(x)=(x-1)(x-2)$:$f_2(1)=0, f_2(2)=0, f_2’(2)=(x-1)|_{x=2} = 1$。则 $(A-E)(A-2E) = 0 \\cdot B_{10} + 0 \\cdot B_{20} + 1 \\cdot B_{21} = B_{21}$。取 $f_3(x)=x-1$:$f_3(1)=0, f_3(2)=1, f_3’(2)=1$。则 $A-E = 0 \\cdot B_{10} + 1 \\cdot B_{20} + 1 \\cdot B_{21} = B_{20} + B_{21}$。所以 $B_{20} = (A-E) - (A-E)(A-2E)$。因此 $e^A = e^1 (A-2E)^2 + e^2 [(A-E)-(A-E)(A-2E)] + e^2 (A-E)(A-2E)$$e^A = e(A-2E)^2 + e^2(A-E)$。 (PPT这个形式是对的，之前的 $f(1)B_{10}$ 等项应该对应 $e$, $e^2$, $e^2$)PPT的结果 $e^A = e \\begin{pmatrix} \\dots \\end{pmatrix}$ 给出具体数值。 4. 矩阵函数的性质 (Properties of Matrix Functions)设 $f(A)$ 和 $g(A)$ 是矩阵 $A$ 的函数 (例如通过收敛幂级数定义，或通过插值多项式定义)。 可交换性: $f(A)g(A) = g(A)f(A)$。 证明思路: 如果 $f(A)=p(A)$ 和 $g(A)=q(A)$，其中 $p, q$ 是多项式，则 $p(A)q(A)=q(A)p(A)$ 因为矩阵多项式可交换。对于更一般的函数，可以通过幂级数逼近或Jordan型证明。□ 复合函数: 如果 $h(x) = f(x)g(x)$，则 $h(A) = f(A)g(A)$。 证明思路:设 $p(x), q(x), r(x)$ 分别是 $f(x), g(x), h(x)$ 关于 $A$ 的插值多项式。需要证明 $r(A) = p(A)q(A)$。关键在于证明 $r(x)$ 和 $p(x)q(x)$ 在 $A$ 的谱值 (特征值及其导数值) 上一致。$r^{(k)}(\\lambda_i) = h^{(k)}(\\lambda_i) = (fg)^{(k)}(\\lambda_i) = \\sum_{j=0}^k \\binom{k}{j} f^{(j)}(\\lambda_i) g^{(k-j)}(\\lambda_i)$而 $(pq)^{(k)}(\\lambda_i) = \\sum_{j=0}^k \\binom{k}{j} p^{(j)}(\\lambda_i) q^{(k-j)}(\\lambda_i)$。由于 $p^{(j)}(\\lambda_i) = f^{(j)}(\\lambda_i)$ 和 $q^{(j)}(\\lambda_i) = g^{(j)}(\\lambda_i)$ 在谱点上成立，所以 $r(x)$ 和 $p(x)q(x)$ 在谱点上一致。因此 $r(A) = (pq)(A) = p(A)q(A)$。□ 逆函数: $(e^A)^{-1} = e^{-A}$。 证明: $e^x e^{-x} = e^0 = 1$。所以 $e^A e^{-A} = E$。□ 例子: $A = \\begin{pmatrix} -1 &amp; 0 \\\\ -1 &amp; 1 \\end{pmatrix}$，求 $(e^A)^{-1}$。$(e^A)^{-1} = e^{-A}$。计算 $e^{-A}$。$-A = \\begin{pmatrix} 1 &amp; 0 \\\\ 1 &amp; -1 \\end{pmatrix}$。特征值是 $1, -1$。$e^{-A} = c_1(-A) + c_0 E$。$c_1(1)+c_0 = e^1$$c_1(-1)+c_0 = e^{-1}$$2c_1 = e-e^{-1} \\Rightarrow c_1 = \\frac{e-e^{-1}}{2}$$2c_0 = e+e^{-1} \\Rightarrow c_0 = \\frac{e+e^{-1}}{2}$$e^{-A} = \\frac{e-e^{-1}}{2} (-A) + \\frac{e+e^{-1}}{2} E = \\begin{pmatrix} e &amp; 0 \\\\ \\frac{e-e^{-1}}{2} &amp; e^{-1} \\end{pmatrix}$。 (PPT的计算有误，它是对$e^A$的元素求倒数，这是错的)正确的 $e^A$ 应该是 $\\begin{pmatrix} e^{-1} &amp; 0 \\\\ \\frac{e^{-1}-e}{2} &amp; e \\end{pmatrix}$ (基于 $A$ 的特征值 $-1, 1$)然后 $e^{-A}$ 应该是 $\\begin{pmatrix} e &amp; 0 \\\\ \\frac{e-e^{-1}}{2} &amp; e^{-1} \\end{pmatrix}$。 5. 线性常系数微分方程组 (Systems of Linear Differential Equations with Constant Coefficients)5.1 齐次方程组 (Homogeneous Systems) 方程: $\\frac{d}{dt}X(t) = AX(t)$ 初始条件: $X(0) = X_0 = (c_1, c_2, \\dots, c_s)^T$ 解: $X(t) = e^{At} X_0$ 验证 (唯一性):设 $Y(t)$ 是另一个解。令 $Z(t) = e^{-At}Y(t)$。$\\frac{d}{dt}Z(t) = (\\frac{d}{dt}e^{-At})Y(t) + e^{-At}(\\frac{d}{dt}Y(t))$$= (-Ae^{-At})Y(t) + e^{-At}(AY(t))$$= -Ae^{-At}Y(t) + Ae^{-At}Y(t) = O$。所以 $Z(t)$ 是一个常数向量。 $Z(t) = Z(0) = e^{-A \\cdot 0}Y(0) = X_0$。因此 $Y(t) = e^{At}Z(t) = e^{At}X_0$。□ 例子: $\\frac{d}{dt}X(t) = \\begin{pmatrix} -7 &amp; -7 &amp; 5 \\\\ -8 &amp; -8 &amp; -5 \\\\ 0 &amp; -5 &amp; 0 \\end{pmatrix} X(t)$，初始条件 $X(0) = (3, -2, 1)^T$。令 $A = \\begin{pmatrix} -7 &amp; -7 &amp; 5 \\\\ -8 &amp; -8 &amp; -5 \\\\ 0 &amp; -5 &amp; 0 \\end{pmatrix}$。特征多项式 $|\\lambda E - A| = (\\lambda-5)(\\lambda+5)(\\lambda+15)$。使用 Lagrange 插值计算 $e^{At}$ (因为根是单根):$e^{At} = \\frac{(A+5E)(A+15E)}{(5+5)(5+15)} e^{5t} + \\frac{(A-5E)(A+15E)}{(-5-5)(-5+15)} e^{-5t} + \\frac{(A-5E)(A+5E)}{(-15-5)(-15+5)} e^{-15t}$$= \\frac{e^{5t}}{200}(A^2+20A+75E) - \\frac{e^{-5t}}{100}(A^2+10A-75E) + \\frac{e^{-15t}}{200}(A^2-25E)$(PPT中的第二项 $A^2+20A-75E$ 应该是 $A^2+10A-75E$ 因为 $(-5+15)=10$)。代入 $A, A^2$ 计算出 $e^{At}$ 的具体矩阵形式。然后 $X(t) = e^{At} X_0$。 5.2 非齐次方程组 (Non-homogeneous Systems) 方程: $\\frac{d}{dt}X(t) = AX(t) + u(t)$ 初始条件: $X(t_0) = X_0$ 解 (常数变易法 / Variation of Parameters):$X(t) = e^{A(t-t_0)}X_0 + \\int_{t_0}^{t} e^{A(t-s)}u(s)ds$ 推导:设解的形式为 $X(t) = e^{At}c(t)$。$\\frac{d}{dt}X(t) = Ae^{At}c(t) + e^{At}c’(t)$。代入原方程: $Ae^{At}c(t) + e^{At}c’(t) = A e^{At}c(t) + u(t)$。$e^{At}c’(t) = u(t) \\Rightarrow c’(t) = e^{-At}u(t)$。$c(t) = \\int_{t_0}^{t} e^{-As}u(s)ds + C_0$ (其中 $C_0$ 是常数向量)。$X(t) = e^{At} \\left( \\int_{t_0}^{t} e^{-As}u(s)ds + C_0 \\right)$$= \\int_{t_0}^{t} e^{A(t-s)}u(s)ds + e^{At}C_0$。使用初始条件 $X(t_0)=X_0$:$X_0 = e^{At_0}C_0 \\Rightarrow C_0 = e^{-At_0}X_0$。$X(t) = \\int_{t_0}^{t} e^{A(t-s)}u(s)ds + e^{A(t-t_0)}X_0$。□ 例子: $\\frac{d}{dt}X(t) = \\begin{pmatrix} 3 &amp; -1 &amp; 1 \\\\ 2 &amp; 0 &amp; -1 \\\\ 1 &amp; -1 &amp; 2 \\end{pmatrix} X(t) + \\begin{pmatrix} 0 \\\\ 0 \\\\ e^{2t} \\end{pmatrix}$，初始条件 $X(0)=(1,1,1)^T$。$A = \\begin{pmatrix} 3 &amp; -1 &amp; 1 \\\\ 2 &amp; 0 &amp; -1 \\\\ 1 &amp; -1 &amp; 2 \\end{pmatrix}$, $u(s) = \\begin{pmatrix} 0 \\\\ 0 \\\\ e^{2s} \\end{pmatrix}$。特征多项式 $|\\lambda E - A| = \\lambda(\\lambda-2)(\\lambda-3)$。特征值 $0, 2, 3$。找到特征向量，构造 $P$ 使得 $P^{-1}AP = D = \\text{diag}(0,2,3)$。$e^{At} = P e^{Dt} P^{-1} = P \\text{diag}(1, e^{2t}, e^{3t}) P^{-1}$。计算 $e^{At}X_0$ 和 $\\int_0^t e^{A(t-s)}u(s)ds$。 5.3 高阶线性常系数微分方程 (Higher-order Linear ODEs with Constant Coefficients) 齐次方程: $y^{(n)} + a_1 y^{(n-1)} + \\dots + a_n y = 0$初始条件 $y(0)=y_0, y’(0)=y_1, \\dots, y^{(n-1)}(0)=y_{n-1}$。 转换为一阶方程组:令 $x_1(t) = y(t), x_2(t) = y’(t), \\dots, x_n(t) = y^{(n-1)}(t)$。$X(t) = (x_1, \\dots, x_n)^T$。$\\frac{d}{dt}X(t) = A X(t)$，其中 $A$ 是友矩阵 (Companion Matrix):$A = \\begin{pmatrix}0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\\\-a_n &amp; -a_{n-1} &amp; -a_{n-2} &amp; \\dots &amp; -a_1\\end{pmatrix}$初始条件 $X(0) = (y_0, y_1, \\dots, y_{n-1})^T$。 解: $X(t) = e^{At}X_0$。$y(t)$ 是 $X(t)$ 的第一个分量: $y(t) = (1, 0, \\dots, 0) e^{At} X_0$。 非齐次方程: $y^{(n)} + a_1 y^{(n-1)} + \\dots + a_n y = u(t)$转换为 $\\frac{d}{dt}X(t) = AX(t) + U(t)$，其中 $U(t) = (0, \\dots, 0, u(t))^T$。 解: $y(t) = (1, 0, \\dots, 0) \\left( e^{At}X_0 + \\int_0^t e^{A(t-s)}U(s)ds \\right)$。 例子: $y’’’ - 3y’’ - 6y’ + 8y = e^{2t}$，初始条件 $(y(0), y’(0), y’’(0)) = (1,0,1)$。$A = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ -8 &amp; 6 &amp; 3 \\end{pmatrix}$。特征多项式 $|\\lambda E - A| = \\lambda^3 - 3\\lambda^2 - 6\\lambda + 8 = (\\lambda-1)(\\lambda+2)(\\lambda-4)$。计算 $e^{At}$，然后代入公式。 6. 与矩阵可交换的矩阵 (Matrices Commuting with a Given Matrix)这部分讨论中心化子 $C(A) = \\{B \\mid AB=BA\\}$。 6.1 基本思想与 Jordan 标准型的作用 设 $A = PJP^{-1}$，其中 $J = \\text{diag}(J_1, \\dots, J_s)$ 是 $A$ 的 Jordan 标准型。$B$ 与 $A$ 可交换 ($AB=BA$) 当且仅当 $PJP^{-1}B = B PJP^{-1}$。令 $B_1 = P^{-1}BP$，则 $JB_1 = B_1J$。即，与 $A$ 可交换的矩阵 $B$ 具有形式 $B = PB_1P^{-1}$，其中 $B_1$ 与 $J$ 可交换。 将 $B_1$ 分块为 $B_1 = (B_{ij})$，与 $J$ 的分块对应。$JB_1 = B_1J \\iff J_i B_{ij} = B_{ij} J_j$ for all $i,j=1, \\dots, s$。 6.2 $J_i B_{ij} = B_{ij} J_j$ 的解 情况 1: $\\lambda_i \\ne \\lambda_j$ (对应 $J_i$ 和 $J_j$ 的特征值不同)$J_i = \\lambda_i E_{k_i} + N_i$, $J_j = \\lambda_j E_{k_j} + N_j$。$(\\lambda_i E_{k_i} + N_i)B_{ij} = B_{ij}(\\lambda_j E_{k_j} + N_j)$$N_i B_{ij} - B_{ij} N_j = (\\lambda_j - \\lambda_i) B_{ij}$。由于 $N_i$ 和 $N_j$ 是幂零的，算子 $L(X) = N_i X - X N_j$ 也是幂零的。而 $(\\lambda_j - \\lambda_i)$ 是非零常数。如果 $B_{ij} \\ne O$，则 $L(B_{ij})$ 是幂零的，但 $(\\lambda_j - \\lambda_i)B_{ij}$ 不是幂零的 (除非 $B_{ij}=O$)。因此，当 $\\lambda_i \\ne \\lambda_j$ 时，必有 $B_{ij} = O$ (零矩阵)。这意味着 $B_1$ 是一个分块对角矩阵 (如果 $A$ 的所有特征值都不同，则 $J$ 是对角阵，$B_1$ 也是对角阵)。 情况 2: $\\lambda_i = \\lambda_j$ (特征值相同)$N_i B_{ij} = B_{ij} N_j$。这种矩阵方程的解 $B_{ij}$ 是一个特定形式的上三角 Toeplitz 型矩阵 (或更复杂的结构，取决于 $N_i, N_j$ 的大小)。如果 $i=j$ (对角块 $B_{ii}$)，则 $N_i B_{ii} = B_{ii} N_i$。$B_{ii}$ 是一个与单个 Jordan 块 $N_i$ (或 $J_i$) 可交换的矩阵。这种矩阵具有上三角 Toeplitz 结构：$B_{ii} = \\begin{pmatrix}b_0 &amp; b_1 &amp; \\dots &amp; b_{k_i-1} \\\\0 &amp; b_0 &amp; \\dots &amp; b_{k_i-2} \\\\\\vdots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\0 &amp; \\dots &amp; 0 &amp; b_0\\end{pmatrix}$(即 $B_{ii}$ 是 $N_i$ 的多项式: $B_{ii} = p(N_i)$)。 6.3 $C(C(A))$: 二次中心化子 定理 (二次中心化子定理 ): 对于任何 $n \\times n$ 复矩阵 $A$，$C(C(A)) = \\{p(A) \\mid p \\text{ is a polynomial}\\}$。即，与所有和 $A$ 可交换的矩阵都可交换的矩阵，恰好是 $A$ 的多项式。 证明思路 (非常复杂，依赖于上述 $B_{ij}$ 的结构分析): $C(A)$ 中的元素 $B=PB_1P^{-1}$，其中 $B_1$ 的块 $B_{ij}$ 满足 $J_iB_{ij}=B_{ij}J_j$。 考虑 $C \\in C(C(A))$，则 $C$ 与 $C(A)$ 中的所有 $B$ 可交换。$C = PC_1P^{-1}$，则 $C_1$ 与 $C(A)$ 对应的所有 $B_1$ 可交换。 通过仔细分析 $B_{ij}$ 的结构 (特别是当 $\\lambda_i=\\lambda_j$ 时，这些块可以取得相当的自由度，例如可以构造出某些特定位置为1其他为0的 $B_{ij}$)，可以证明 $C_1$ 必须具有如下形式：$C_1 = \\text{diag}(g_1(J_1), g_2(J_2), \\dots, g_s(J_s))$，其中 $g_k(x)$ 是多项式。并且，如果 $\\lambda_i = \\lambda_j$，则 $g_i(x)$ 和 $g_j(x)$ 的系数必须满足一定关系，使得它们在谱点上一致，最终可以统一为一个多项式 $g(x)$。即 $C_1 = g(J)$。 因此 $C = Pg(J)P^{-1} = g(PJP^{-1}) = g(A)$。□ 推论: 如果一个矩阵 $B$ 与所有和 $A$ 可交换的矩阵都可交换，那么 $B$ 一定是 $A$ 的多项式。 特例: 如果 $A$ 的多项式构成了 $C(A)$ (例如 $A$ 是非减损的，即最小多项式等于特征多项式)，则 $C(C(A)) = C(A)$。","link":"/MATH1408/Chapter6-MATH1408.html"},{"title":"Chapter5 MATH1408","text":"矩阵的相似变换与特征值理论1. 矩阵的相似 (Similarity) 与对角化 (Diagonalization)1.1 相似矩阵 (Similar Matrices) 定义: 设 $A, B$ 都是 $n$ 阶方阵。如果存在一个 $n$ 阶可逆矩阵 $P$，使得$P^{-1}AP = B$则称矩阵 $A$ 相似于 (similar to) 矩阵 $B$，记作 $A \\sim B$。称 $P$ 为将 $A$ 变换为 $B$ 的相似变换矩阵。 相似矩阵的性质: 自反性: $A \\sim A$ (取 $P=E$, 单位矩阵) 对称性: 若 $A \\sim B$，则 $B \\sim A$ (若 $P^{-1}AP = B$，则 $(P^{-1})^{-1}BP^{-1} = A$) 传递性: 若 $A \\sim B$，$B \\sim C$，则 $A \\sim C$ 相似矩阵有相同的特征多项式，因此有相同的特征值、行列式、迹。 若 $A \\sim B$，则 $A^k \\sim B^k$ 对任意正整数 $k$ 成立。 若 $A \\sim B$ 且 $A$ 可逆，则 $B$ 也可逆，且 $A^{-1} \\sim B^{-1}$。 定理: 若 $A \\sim B$，即 $P^{-1}AP = B$，则对任意多项式 $g(x)$，有 $P^{-1}g(A)P = g(B)$。 证明:设 $g(x) = b_m x^m + b_{m-1} x^{m-1} + \\dots + b_1 x + b_0$。则 $g(A) = b_m A^m + b_{m-1} A^{m-1} + \\dots + b_1 A + b_0 E$。因为 $B = P^{-1}AP$，所以 $A = PBP^{-1}$。$A^k = (PBP^{-1})^k = (PBP^{-1})(PBP^{-1})\\dots(PBP^{-1}) = PB^kP^{-1}$。因此，$P^{-1}g(A)P = P^{-1}(b_m A^m + \\dots + b_0 E)P$$= b_m P^{-1}A^m P + \\dots + b_1 P^{-1}AP + b_0 P^{-1}EP$$= b_m (P^{-1}AP)^m + \\dots + b_1 (P^{-1}AP) + b_0 E$ (或者使用 $A^k=PB^kP^{-1}$)$= b_m B^m + \\dots + b_1 B + b_0 E = g(B)$。□ 1.2 矩阵的对角化 (Diagonalization) 定义: 如果一个 $n$ 阶方阵 $A$ 相似于一个对角矩阵 $\\Lambda$，即存在可逆矩阵 $P$ 使得$P^{-1}AP = \\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)$则称矩阵 $A$ 可对角化 (diagonalizable)。 Jordan标准型 (Jordan Canonical Form): 并非所有矩阵都可对角化。 任何一个复数域上的 $n$ 阶方阵 $A$ 都相似于一个 Jordan标准型矩阵 $J$。 $J = \\text{diag}(J_1(\\lambda_1), J_2(\\lambda_2), \\dots, J_k(\\lambda_k))$ 其中 $J_i(\\lambda_i)$ 是对应于特征值 $\\lambda_i$ 的 Jordan块，形如：$J_r(\\lambda) = \\begin{pmatrix} \\lambda &amp; 1 &amp; &amp; \\\\ &amp; \\lambda &amp; \\ddots &amp; \\\\ &amp; &amp; \\ddots &amp; 1 \\\\ &amp; &amp; &amp; \\lambda \\end{pmatrix}_{r \\times r}$ 若矩阵 $A$ 可对角化，则其Jordan标准型就是对角矩阵 (所有Jordan块都是 $1 \\times 1$)。 2. 特征值 (Eigenvalues) 与特征向量 (Eigenvectors)2.1 定义 设 $A$ 是一个 $n$ 阶方阵。如果存在一个数 $\\lambda$ 和一个非零的 $n$ 维列向量 $\\alpha$，使得$A\\alpha = \\lambda\\alpha$则称 $\\lambda$ 是矩阵 $A$ 的一个特征值 (eigenvalue)，称非零向量 $\\alpha$ 是矩阵 $A$ 对应于特征值 $\\lambda$ 的一个特征向量 (eigenvector)。 2.2 特征方程与特征多项式 特征值方程 $A\\alpha = \\lambda\\alpha$ 可以改写为 $(\\lambda E - A)\\alpha = 0$。 这是一个齐次线性方程组。它有非零解 $\\alpha$ 的充要条件是系数行列式为零：$|\\lambda E - A| = 0$这个方程称为矩阵 $A$ 的特征方程 (characteristic equation)。 $f_A(\\lambda) = |\\lambda E - A|$ 称为矩阵 $A$ 的特征多项式 (characteristic polynomial)。它是一个关于 $\\lambda$ 的 $n$ 次多项式。$f_A(\\lambda) = \\begin{vmatrix} \\lambda - a_{11} &amp; -a_{12} &amp; \\dots &amp; -a_{1n} \\\\ -a_{21} &amp; \\lambda - a_{22} &amp; \\dots &amp; -a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ -a_{n1} &amp; -a_{n2} &amp; \\dots &amp; \\lambda - a_{nn} \\end{vmatrix} = \\lambda^n - (\\text{tr}A)\\lambda^{n-1} + \\dots + (-1)^n |A|$ 特征值是特征方程的根。根据代数基本定理，一个 $n$ 次多项式在复数域内恰有 $n$ 个根 (计重数)。 2.3 特征子空间 (Eigenspace) 对于矩阵 $A$ 的一个特征值 $\\lambda_0$，齐次线性方程组 $(\\lambda_0 E - A)\\alpha = 0$ 的解空间 $V_{\\lambda_0} = \\{\\alpha | (\\lambda_0 E - A)\\alpha = 0\\}$ 称为对应于特征值 $\\lambda_0$ 的特征子空间。 特征子空间中的所有非零向量都是对应于 $\\lambda_0$ 的特征向量，再加上零向量。 2.4 特征值的性质 定理: 设 $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ 是 $n$ 阶矩阵 $A=(a_{ij})$ 的 $n$ 个特征值 (计重数)。则： $\\sum_{i=1}^n \\lambda_i = \\sum_{i=1}^n a_{ii} = \\text{tr}(A)$ (特征值之和等于矩阵的迹) $\\prod_{i=1}^n \\lambda_i = |A|$ (特征值之积等于矩阵的行列式) 证明概要:特征多项式 $f_A(\\lambda) = |\\lambda E - A| = (\\lambda - \\lambda_1)(\\lambda - \\lambda_2)\\dots(\\lambda - \\lambda_n)$。同时，$f_A(\\lambda) = \\lambda^n - (a_{11}+\\dots+a_{nn})\\lambda^{n-1} + \\dots + (-1)^n|A|$。比较 $\\lambda^{n-1}$ 的系数：$-(a_{11}+\\dots+a_{nn}) = -(\\lambda_1+\\dots+\\lambda_n)$，得 $\\sum \\lambda_i = \\text{tr}(A)$。比较常数项 (令 $\\lambda=0$): $|-A| = (-1)^n |A| = (-\\lambda_1)(-\\lambda_2)\\dots(-\\lambda_n) = (-1)^n \\prod \\lambda_i$，得 $\\prod \\lambda_i = |A|$。□ 更一般地，特征值 $\\lambda_{j_1}, \\dots, \\lambda_{j_k}$ 的所有 $k$ 阶初等对称多项式之和等于 $A$ 的所有 $k$ 阶主子式之和。 2.5 特征向量的线性无关性 定理: 设 $\\lambda_1, \\lambda_2, \\dots, \\lambda_s$ 是矩阵 $A$ 的 $s$ 个互不相同的特征值，$\\alpha_1, \\alpha_2, \\dots, \\alpha_s$ 分别是与之对应的特征向量，则 $\\alpha_1, \\alpha_2, \\dots, \\alpha_s$ 线性无关。 证明 (数学归纳法):当 $s=1$ 时，$\\alpha_1 \\ne 0$，显然线性无关。假设当 $s=k-1$ 时结论成立。考虑 $s=k$ 的情况。设 $c_1\\alpha_1 + c_2\\alpha_2 + \\dots + c_k\\alpha_k = 0 \\quad (*)$。用 $A$ 左乘 $(*)$ 式：$c_1 A\\alpha_1 + c_2 A\\alpha_2 + \\dots + c_k A\\alpha_k = 0$即 $c_1 \\lambda_1\\alpha_1 + c_2 \\lambda_2\\alpha_2 + \\dots + c_k \\lambda_k\\alpha_k = 0 \\quad (**)$。用 $\\lambda_k$ 乘 $(*)$ 式：$c_1 \\lambda_k\\alpha_1 + c_2 \\lambda_k\\alpha_2 + \\dots + c_k \\lambda_k\\alpha_k = 0 \\quad (***)$。$(**)-(***)$ 得：$c_1(\\lambda_1-\\lambda_k)\\alpha_1 + c_2(\\lambda_2-\\lambda_k)\\alpha_2 + \\dots + c_{k-1}(\\lambda_{k-1}-\\lambda_k)\\alpha_{k-1} = 0$。根据归纳假设，$\\alpha_1, \\dots, \\alpha_{k-1}$ 线性无关。又因为 $\\lambda_i \\ne \\lambda_k$ for $i=1, \\dots, k-1$，所以 $\\lambda_i - \\lambda_k \\ne 0$。因此 $c_1=c_2=\\dots=c_{k-1}=0$。代回 $(*)$ 式，得 $c_k\\alpha_k=0$。由于 $\\alpha_k \\ne 0$，故 $c_k=0$。所以 $c_1=c_2=\\dots=c_k=0$，即 $\\alpha_1, \\dots, \\alpha_k$ 线性无关。□ 2.6 代数重数与几何重数 代数重数 (Algebraic Multiplicity): 特征值 $\\lambda_i$ 作为特征方程 $f_A(\\lambda)=0$ 的根的重数，记为 $n_i$。$\\sum n_i = n$ (矩阵阶数)。 几何重数 (Geometric Multiplicity): 特征值 $\\lambda_i$ 对应的特征子空间 $V_{\\lambda_i}$ 的维数，即线性无关特征向量的最大个数，记为 $g_i$。$g_i = \\text{dim}(V_{\\lambda_i}) = n - \\text{rank}(\\lambda_i E - A)$。 定理: 对任意特征值 $\\lambda_i$，其几何重数 $g_i$ 不大于其代数重数 $n_i$。即 $1 \\le g_i \\le n_i$。 证明:设 $\\lambda_0$ 是 $A$ 的一个特征值，其几何重数为 $g_0$。则存在 $g_0$ 个线性无关的特征向量 $\\alpha_1, \\dots, \\alpha_{g_0}$ 使得 $A\\alpha_j = \\lambda_0\\alpha_j$ for $j=1, \\dots, g_0$。将这 $g_0$ 个向量扩充为 $n$ 维空间的一组基 $\\alpha_1, \\dots, \\alpha_{g_0}, \\alpha_{g_0+1}, \\dots, \\alpha_n$。令 $P = (\\alpha_1, \\dots, \\alpha_n)$。则 $P$ 可逆。$AP = (A\\alpha_1, \\dots, A\\alpha_{g_0}, A\\alpha_{g_0+1}, \\dots, A\\alpha_n) = (\\lambda_0\\alpha_1, \\dots, \\lambda_0\\alpha_{g_0}, A\\alpha_{g_0+1}, \\dots, A\\alpha_n)$。$P^{-1}AP = P^{-1} (\\lambda_0\\alpha_1, \\dots, \\lambda_0\\alpha_{g_0}, A\\alpha_{g_0+1}, \\dots, A\\alpha_n)$。由于 $P^{-1}P = E$, $P^{-1}\\alpha_j = e_j$ (标准单位向量) for $j=1, \\dots, n$ (这里理解为 $P^{-1}$ 作用于 $P$ 的列向量)。所以 $P^{-1}AP = \\begin{pmatrix} \\lambda_0 E_{g_0} &amp; B_{12} \\\\ O &amp; B_{22} \\end{pmatrix}$。由于相似矩阵有相同的特征多项式，$f_A(\\lambda) = f_{P^{-1}AP}(\\lambda) = |\\lambda E - P^{-1}AP| = \\begin{vmatrix} (\\lambda-\\lambda_0)E_{g_0} &amp; -B_{12} \\\\ O &amp; \\lambda E_{n-g_0} - B_{22} \\end{vmatrix}$$= |(\\lambda-\\lambda_0)E_{g_0}| \\cdot |\\lambda E_{n-g_0} - B_{22}| = (\\lambda-\\lambda_0)^{g_0} |\\lambda E_{n-g_0} - B_{22}|$。这表明 $(\\lambda-\\lambda_0)$ 至少是 $f_A(\\lambda)$ 的 $g_0$ 次因子，所以 $\\lambda_0$ 的代数重数 $n_0 \\ge g_0$。□ 3. 矩阵可对角化的条件 定理 1: $n$ 阶方阵 $A$ 可对角化的充要条件是 $A$ 有 $n$ 个线性无关的特征向量。 证明:($\\Rightarrow$) 若 $A$ 可对角化，则存在可逆 $P$ 使 $P^{-1}AP = \\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)$。$AP = P\\Lambda$。令 $P = (p_1, p_2, \\dots, p_n)$。$A(p_1, \\dots, p_n) = (p_1, \\dots, p_n) \\begin{pmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n \\end{pmatrix}$$(Ap_1, \\dots, Ap_n) = (\\lambda_1 p_1, \\dots, \\lambda_n p_n)$。即 $Ap_j = \\lambda_j p_j$ for $j=1, \\dots, n$。由于 $P$ 可逆，其列向量 $p_1, \\dots, p_n$ 线性无关且非零。因此它们是 $A$ 的 $n$ 个线性无关的特征向量。($\\Leftarrow$) 若 $A$ 有 $n$ 个线性无关的特征向量 $p_1, \\dots, p_n$，对应特征值为 $\\lambda_1, \\dots, \\lambda_n$ (不必互异)。令 $P = (p_1, \\dots, p_n)$。则 $P$ 可逆。$AP = (Ap_1, \\dots, Ap_n) = (\\lambda_1 p_1, \\dots, \\lambda_n p_n)$$= (p_1, \\dots, p_n) \\begin{pmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n \\end{pmatrix} = P\\Lambda$。所以 $P^{-1}AP = \\Lambda$。$A$ 可对角化。□ 定理 2: $n$ 阶方阵 $A$ 可对角化的充要条件是 $A$ 的每个特征值 $\\lambda_i$ 的几何重数 $g_i$ 等于其代数重数 $n_i$。 证明概要:($\\Rightarrow$) 若 $A$ 可对角化, $A \\sim \\Lambda = \\text{diag}(\\underbrace{\\lambda_1,\\dots,\\lambda_1}_{n_1 \\text{ times}}, \\dots, \\underbrace{\\lambda_s,\\dots,\\lambda_s}_{n_s \\text{ times}})$.则 $g_k = n - \\text{rank}(\\lambda_k E - A) = n - \\text{rank}(\\lambda_k E - \\Lambda)$.$\\lambda_k E - \\Lambda$ 是对角矩阵，对角线上有 $n-n_k$ 个非零元，所以 $\\text{rank}(\\lambda_k E - \\Lambda) = n-n_k$.故 $g_k = n - (n-n_k) = n_k$.($\\Leftarrow$) 若对每个特征值 $\\lambda_i$ 都有 $g_i = n_i$.设 $A$ 有 $s$ 个互异特征值 $\\lambda_1, \\dots, \\lambda_s$，代数重数分别为 $n_1, \\dots, n_s$ ( $\\sum n_i = n$ )。则对应的几何重数 $g_i=n_i$。这意味着我们可以为每个 $\\lambda_i$ 找到 $n_i$ 个线性无关的特征向量。所有这些特征向量集合的总数为 $\\sum g_i = \\sum n_i = n$。不同特征子空间的特征向量是线性无关的。同一特征子空间内部选取的基向量也是线性无关的。因此，这 $n$ 个特征向量共同构成了 $n$ 维空间的一组基，即 $A$ 有 $n$ 个线性无关的特征向量。由定理1，$A$ 可对角化。□ 推论: 如果 $n$ 阶方阵 $A$ 有 $n$ 个互不相同的特征值，则 $A$ 一定可以对角化。 证明: 若 $A$ 有 $n$ 个互异特征值 $\\lambda_1, \\dots, \\lambda_n$，则每个特征值的代数重数 $n_i=1$。又因为 $1 \\le g_i \\le n_i$，所以 $g_i=1$。因此 $g_i=n_i=1$ 对所有 $i$ 成立。由定理2，$A$ 可对角化。(或者直接由特征向量线性无关性定理，$A$ 有 $n$ 个线性无关的特征向量，由定理1，$A$ 可对角化)。□ 对角化步骤总结: 求 $A$ 的特征多项式 $|\\lambda E - A| = 0$。 解特征方程，得到所有特征值 $\\lambda_1, \\dots, \\lambda_s$ 及其代数重数 $n_1, \\dots, n_s$。 对每个特征值 $\\lambda_i$，解齐次方程组 $(\\lambda_i E - A)X = 0$，求出基础解系。基础解系的个数即为几何重数 $g_i$。 若对所有 $i$ 都有 $g_i = n_i$，则 $A$ 可对角化。将所有基础解系中的向量合起来构成可逆矩阵 $P$。则 $P^{-1}AP = \\Lambda$，其中 $\\Lambda$ 的对角元是 $P$ 中列向量对应的特征值。 若存在某个 $i$ 使得 $g_i &lt; n_i$，则 $A$ 不可对角化。 4. 特征值估计：Gershgorin 圆盘定理 (Gershgorin Circle Theorem) Gershgorin 圆盘: 对 $n$ 阶复矩阵 $A=(a_{ij})$，第 $i$ 个 Gershgorin圆盘 $D_i(A)$ 定义为复平面上的一个闭圆盘：$D_i(A) = \\{ z \\in \\mathbb{C} : |z - a_{ii}| \\le R_i(A) \\}$其中 $a_{ii}$ 是圆心，$R_i(A) = \\sum_{j \\ne i} |a_{ij}|$ 是半径 (第 $i$ 行非对角元素绝对值之和)。类似地，可以定义列圆盘 $D’_j(A) = \\{ z \\in \\mathbb{C} : |z - a_{jj}| \\le C_j(A) \\}$，其中 $C_j(A) = \\sum_{i \\ne j} |a_{ij}|$ (第 $j$ 列非对角元素绝对值之和)。 定理 1 (Gershgorin第一定理): 矩阵 $A$ 的所有特征值都位于所有行Gershgorin圆盘的并集内，即 $\\lambda \\in \\bigcup_{i=1}^n D_i(A)$。类似地，所有特征值也位于所有列Gershgorin圆盘的并集内 $\\lambda \\in \\bigcup_{j=1}^n D’_j(A)$。 证明 (行圆盘):设 $\\lambda$ 是 $A$ 的一个特征值，$\\alpha=(x_1, \\dots, x_n)^T \\ne 0$ 是对应的特征向量，即 $A\\alpha = \\lambda\\alpha$。展开第 $i$ 行：$\\sum_{j=1}^n a_{ij}x_j = \\lambda x_i$。移项得：$(\\lambda - a_{ii})x_i = \\sum_{j \\ne i} a_{ij}x_j$。选取 $k$ 使得 $|x_k| = \\max_{j} |x_j|$。由于 $\\alpha \\ne 0$，所以 $|x_k| &gt; 0$。对于第 $k$ 行，有 $(\\lambda - a_{kk})x_k = \\sum_{j \\ne k} a_{kj}x_j$。两边取绝对值：$|\\lambda - a_{kk}| |x_k| = |\\sum_{j \\ne k} a_{kj}x_j| \\le \\sum_{j \\ne k} |a_{kj}| |x_j|$。由于 $|x_j| \\le |x_k|$，所以 $|\\lambda - a_{kk}| |x_k| \\le \\sum_{j \\ne k} |a_{kj}| |x_k| = |x_k| \\sum_{j \\ne k} |a_{kj}| = |x_k| R_k(A)$。因为 $|x_k| &gt; 0$，两边除以 $|x_k|$ 得：$|\\lambda - a_{kk}| \\le R_k(A)$。这表明特征值 $\\lambda$ 位于第 $k$ 个Gershgorin圆盘 $D_k(A)$ 内。因此 $\\lambda \\in \\bigcup_{i=1}^n D_i(A)$。列圆盘的证明类似，考虑 $A^T$ (或 $A^H$)，其特征值与 $A$ 相同。□ 定理 2 (Gershgorin第二定理 / Taussky定理): 如果 $k$ 个Gershgorin圆盘的并集 $G = \\bigcup_{i \\in S_k} D_i(A)$ 与其余 $n-k$ 个圆盘的并集 $H = \\bigcup_{j \\notin S_k} D_j(A)$ 不相交 (即 $G \\cap H = \\emptyset$)，则 $G$ 中恰好包含 $A$ 的 $k$ 个特征值 (计重数)，$H$ 中恰好包含 $A$ 的 $n-k$ 个特征值。 证明思路:构造矩阵 $A(t) = \\text{diag}(a_{11}, \\dots, a_{nn}) + t \\cdot (A - \\text{diag}(a_{11}, \\dots, a_{nn}))$ for $0 \\le t \\le 1$。$A(0) = \\text{diag}(a_{11}, \\dots, a_{nn})$，其特征值为 $a_{11}, \\dots, a_{nn}$。$A(1) = A$。矩阵的特征值是其特征多项式系数的连续函数，而特征多项式的系数是矩阵元素的连续函数。因此，特征值是 $t$ 的连续函数。$A(t)$ 的Gershgorin圆盘为 $D_i(A(t)) = \\{z : |z-a_{ii}| \\le t R_i(A)\\}$。显然 $D_i(A(t)) \\subseteq D_i(A(1)) = D_i(A)$。当 $t=0$ 时，$k$ 个特征值 ($a_{ii}$ for $i \\in S_k$) 位于 $G$ 中 (因为 $R_i(A(0))=0$)。当 $t$ 从 $0$ 连续变到 $1$ 时，特征值也连续变化。由于 $G$ 和 $H$ 不相交，特征值不能从 $G$ “跳到” $H$ (或反之)，否则会违背连续性。因此，原来在 $G$ 中的 $k$ 个特征值 (当 $t=0$ 时) 必须在 $t=1$ 时仍然停留在 $G$ 中。□ 推论 (严格对角占优矩阵): 如果矩阵 $A$ 是严格对角占优的，即对所有 $i$， $|a_{ii}| &gt; \\sum_{j \\ne i} |a_{ij}|$ (或列严格对角占优)，则 $A$ 是可逆的。 证明: 若 $A$ 严格对角占优，则对所有 $i$，$R_i(A) &lt; |a_{ii}|$。这意味着 $0$ 不在任何一个Gershgorin圆盘 $D_i(A)$ 内 (因为 $|0 - a_{ii}| = |a_{ii}| &gt; R_i(A)$)。因此，$0$ 不在 $\\bigcup D_i(A)$ 内。根据Gershgorin第一定理， $0$ 不是 $A$ 的特征值。所以 $A$ 可逆。□ Ostrowski定理: 对任意 $p \\in [0, 1]$，矩阵 $A$ 的任一特征值 $\\lambda$ 必满足至少存在一个 $i \\in \\{1, \\dots, n\\}$ 使得:$|\\lambda - a_{ii}| \\le (R_i(A))^p (C_i(A))^{1-p}$(当 $p=1$ 时是行圆盘，当 $p=0$ 时是列圆盘)。 Brauer’s Cassini Ovals: 矩阵 $A$ 的所有特征值包含在由下式定义的 $n(n-1)/2$ 个 Cassini 卵形线的并集中：$\\bigcup_{1 \\le i &lt; j \\le n} \\{z \\in \\mathbb{C} : |z-a_{ii}||z-a_{jj}| \\le R_i(A)R_j(A) \\}$ 5. Schur 定理与正规矩阵5.1 Schur 分解定理 (Schur’s Theorem / Schur’s Triangularization) 酉矩阵 (Unitary Matrix): 若复方阵 $U$ 满足 $U^H U = U U^H = E$ (其中 $U^H$ 是 $U$ 的共轭转置)，则称 $U$ 为酉矩阵。若实方阵 $Q$ 满足 $Q^T Q = Q Q^T = E$，则称 $Q$ 为正交矩阵。 酉相似 (Unitary Similarity): 若存在酉矩阵 $U$ 使得 $U^H A U = B$，则称 $A$ 酉相似于 $B$。 定理 (Schur): 对任意 $n$ 阶复方阵 $A$，存在一个酉矩阵 $U$，使得$U^H A U = T$其中 $T$ 是一个上三角矩阵。并且 $T$ 的对角元是 $A$ 的特征值。若 $A$ 是实矩阵且特征值均为实数，则 $U$ 可以取为正交矩阵 $Q$，使得 $Q^T A Q = T$ (实Schur分解)。 证明 (数学归纳法对矩阵阶数 $n$):当 $n=1$ 时，$A=(a_{11})$ 是标量，$U=(1)$，$T=(a_{11})$，结论成立。假设对任意 $k$ 阶矩阵结论成立。考虑 $n=k+1$ 阶矩阵 $A$。设 $\\lambda_1$ 是 $A$ 的一个特征值，$\\alpha_1$ 是对应的单位特征向量 (即 $||\\alpha_1||_2=1$)，所以 $A\\alpha_1 = \\lambda_1\\alpha_1$。可以将 $\\alpha_1$ 扩充为 $\\mathbb{C}^{k+1}$ 的一组标准正交基 $\\{\\alpha_1, \\alpha_2, \\dots, \\alpha_{k+1}\\}$。令 $U_1 = (\\alpha_1, \\alpha_2, \\dots, \\alpha_{k+1})$。则 $U_1$ 是酉矩阵。$U_1^H A U_1 = U_1^H (A\\alpha_1, A\\alpha_2, \\dots, A\\alpha_{k+1})$$= U_1^H (\\lambda_1\\alpha_1, A\\alpha_2, \\dots, A\\alpha_{k+1})$$= \\begin{pmatrix} \\alpha_1^H \\\\ \\vdots \\\\ \\alpha_{k+1}^H \\end{pmatrix} (\\lambda_1\\alpha_1, A\\alpha_2, \\dots, A\\alpha_{k+1}) = \\begin{pmatrix} \\lambda_1\\alpha_1^H\\alpha_1 &amp; \\alpha_1^H A\\alpha_2 &amp; \\dots &amp; \\alpha_1^H A\\alpha_{k+1} \\\\ \\lambda_1\\alpha_2^H\\alpha_1 &amp; \\alpha_2^H A\\alpha_2 &amp; \\dots &amp; \\alpha_2^H A\\alpha_{k+1} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\lambda_1\\alpha_{k+1}^H\\alpha_1 &amp; \\alpha_{k+1}^H A\\alpha_2 &amp; \\dots &amp; \\alpha_{k+1}^H A\\alpha_{k+1} \\end{pmatrix}$由于 $\\alpha_i^H \\alpha_j = \\delta_{ij}$ (Kronecker delta)，第一列变为 $(\\lambda_1, 0, \\dots, 0)^T$。所以 $U_1^H A U_1 = \\begin{pmatrix} \\lambda_1 &amp; \\mathbf{b}^H \\\\ \\mathbf{0} &amp; A_k \\end{pmatrix}$，其中 $A_k$ 是一个 $k$ 阶方阵。根据归纳假设，存在 $k$ 阶酉矩阵 $V_k$ 使得 $V_k^H A_k V_k = T_k$ (上三角)。令 $U = U_1 \\begin{pmatrix} 1 &amp; \\mathbf{0}^T \\\\ \\mathbf{0} &amp; V_k \\end{pmatrix}$。这个 $U$ 也是酉矩阵 (两个酉矩阵之积)。$U^H A U = \\begin{pmatrix} 1 &amp; \\mathbf{0}^T \\\\ \\mathbf{0} &amp; V_k^H \\end{pmatrix} U_1^H A U_1 \\begin{pmatrix} 1 &amp; \\mathbf{0}^T \\\\ \\mathbf{0} &amp; V_k \\end{pmatrix}$$= \\begin{pmatrix} 1 &amp; \\mathbf{0}^T \\\\ \\mathbf{0} &amp; V_k^H \\end{pmatrix} \\begin{pmatrix} \\lambda_1 &amp; \\mathbf{b}^H \\\\ \\mathbf{0} &amp; A_k \\end{pmatrix} \\begin{pmatrix} 1 &amp; \\mathbf{0}^T \\\\ \\mathbf{0} &amp; V_k \\end{pmatrix}$$= \\begin{pmatrix} \\lambda_1 &amp; \\mathbf{b}^H V_k \\\\ \\mathbf{0} &amp; V_k^H A_k V_k \\end{pmatrix} = \\begin{pmatrix} \\lambda_1 &amp; \\mathbf{b}^H V_k \\\\ \\mathbf{0} &amp; T_k \\end{pmatrix} = T$。$T$ 是上三角矩阵。其对角元是 $A$ 的特征值 (因为相似矩阵特征值相同)。□ 5.2 正规矩阵 (Normal Matrix) 定义: 若 $n$ 阶复方阵 $A$ 满足 $A^H A = A A^H$，则称 $A$ 为正规矩阵。 常见的正规矩阵： Hermitian 矩阵 ($A^H = A$)，实对称矩阵 ($A^T=A$) Skew-Hermitian 矩阵 ($A^H = -A$)，实反对称矩阵 ($A^T=-A$) 酉矩阵 ($A^H A = E$)，正交矩阵 ($A^T A = E$) 对角矩阵 定理: $n$ 阶复方阵 $A$ 是正规矩阵的充要条件是 $A$ 酉相似于一个对角矩阵 (即 $A$ 可以被酉矩阵对角化)。 证明:($\\Leftarrow$) 若存在酉矩阵 $U$ 使得 $U^H A U = \\Lambda$ (对角矩阵)。则 $A = U \\Lambda U^H$。$A^H = (U \\Lambda U^H)^H = U \\Lambda^H U^H$。$A A^H = (U \\Lambda U^H)(U \\Lambda^H U^H) = U \\Lambda \\Lambda^H U^H$。$A^H A = (U \\Lambda^H U^H)(U \\Lambda U^H) = U \\Lambda^H \\Lambda U^H$。因为 $\\Lambda$ 是对角矩阵，所以 $\\Lambda \\Lambda^H = \\Lambda^H \\Lambda$ (对角矩阵与其共轭转置可交换)。因此 $A A^H = A^H A$，即 $A$ 是正规矩阵。($\\Rightarrow$) 若 $A$ 是正规矩阵。根据Schur定理，存在酉矩阵 $U$ 使得 $U^H A U = T$ ($T$ 是上三角矩阵)。我们需要证明 $T$ 实际上是对角矩阵。因为 $A$ 正规，所以 $T = U^H A U$ 也是正规矩阵。($T^H T = (U^H A U)^H (U^H A U) = U^H A^H U U^H A U = U^H A^H A U$)($T T^H = (U^H A U) (U^H A U)^H = U^H A U U^H A^H U = U^H A A^H U$)由于 $A^H A = A A^H$，所以 $T^H T = T T^H$。设 $T = (t_{ij})$，其中 $t_{ij}=0$ for $i&gt;j$。比较 $T^H T$ 和 $T T^H$ 的 $(1,1)$ 元素：$(T^H T)_{11} = \\sum_k \\overline{t_{k1}} t_{k1} = |t_{11}|^2$ (因为 $t_{k1}=0$ for $k&gt;1$)。$(T T^H)_{11} = \\sum_k t_{1k} \\overline{t_{1k}} = |t_{11}|^2 + |t_{12}|^2 + \\dots + |t_{1n}|^2$。由于 $(T^H T)_{11} = (T T^H)_{11}$，所以 $|t_{11}|^2 = |t_{11}|^2 + |t_{12}|^2 + \\dots + |t_{1n}|^2$。这迫使 $t_{12} = t_{13} = \\dots = t_{1n} = 0$。现在比较 $(2,2)$ 元素：$(T^H T)_{22} = \\overline{t_{12}}t_{12} + \\overline{t_{22}}t_{22} + \\dots = |t_{12}|^2 + |t_{22}|^2 = |t_{22}|^2$ (因为 $t_{12}=0$)。$(T T^H)_{22} = |t_{21}|^2 + |t_{22}|^2 + |t_{23}|^2 + \\dots + |t_{2n}|^2 = |t_{22}|^2 + |t_{23}|^2 + \\dots + |t_{2n}|^2$ (因为 $t_{21}=0$ due to upper triangular)。所以 $|t_{22}|^2 = |t_{22}|^2 + |t_{23}|^2 + \\dots + |t_{2n}|^2$。这迫使 $t_{23} = t_{24} = \\dots = t_{2n} = 0$。以此类推，可以证明 $T$ 的所有非对角元素都为0。因此 $T$ 是对角矩阵。□ Schur 不等式: 若 $\\lambda_1, \\dots, \\lambda_n$ 是 $A=(a_{ij})$ 的特征值，则 $\\sum_{i=1}^n |\\lambda_i|^2 \\le \\sum_{i=1}^n \\sum_{j=1}^n |a_{ij}|^2 = ||A||_F^2$ (Frobenius范数的平方)。等号成立的充要条件是 $A$ 是正规矩阵。 证明:由Schur分解，$U^H A U = T$，其中 $T$ 是上三角，对角元为 $\\lambda_i$。$||A||_F^2 = \\text{tr}(A^H A)$。由于迹在酉相似下不变:$\\text{tr}(A^H A) = \\text{tr}((UTU^H)^H (UTU^H)) = \\text{tr}(U T^H U^H U T U^H) = \\text{tr}(U T^H T U^H) = \\text{tr}(T^H T)$。$T^H T$ 的对角元是 $\\sum_{k=1}^i |t_{ki}|^2$ (这里假设 $T$ 是下三角，或者直接算 $\\text{tr}(T^H T) = \\sum_{i,j} |t_{ij}|^2$)。$\\text{tr}(T^H T) = \\sum_{i=1}^n (T^H T)_{ii} = \\sum_{i=1}^n \\sum_{k=1}^n \\overline{t_{ki}} t_{ki} = \\sum_{i,j} |t_{ij}|^2$。由于 $T$ 是上三角，其对角元 $t_{ii} = \\lambda_i$。所以 $\\sum_{i,j} |t_{ij}|^2 = \\sum_{i=1}^n |t_{ii}|^2 + \\sum_{i&lt;j} |t_{ij}|^2 = \\sum_{i=1}^n |\\lambda_i|^2 + \\sum_{i&lt;j} |t_{ij}|^2$。因此 $\\sum_{i=1}^n |\\lambda_i|^2 + \\sum_{i&lt;j} |t_{ij}|^2 = ||A||_F^2$。由于 $\\sum_{i&lt;j} |t_{ij}|^2 \\ge 0$，所以 $\\sum_{i=1}^n |\\lambda_i|^2 \\le ||A||_F^2$。等号成立当且仅当 $\\sum_{i&lt;j} |t_{ij}|^2 = 0$，即所有非对角元 $t_{ij}=0$ ($i&lt;j$)。这意味着 $T$ 是对角矩阵。当 $T$ 是对角矩阵时，$A$ 酉相似于对角矩阵，因此 $A$ 是正规矩阵。□ 5.3 特殊的正规矩阵 Hermitian 矩阵 ($A^H=A$): 特征值必为实数。 不同特征值对应的特征向量相互正交。 必酉相似于实对角矩阵。 实对称矩阵 ($A^T=A$): 是Hermitian矩阵的特例，特征值必为实数。 必正交相似于实对角矩阵 ($Q^T A Q = \\Lambda$)。 酉矩阵 ($A^H A = E$): 特征值的模长必为1 (即 $|\\lambda|=1$)。 不同特征值对应的特征向量相互正交。 必酉相似于对角元模长为1的对角矩阵。 实正规矩阵: 若 $A$ 是实正规矩阵 ($A^T A = A A^T$)，则 $A$ 正交相似于一个实分块对角矩阵，其对角块为 $1 \\times 1$ 的实特征值，或 $2 \\times 2$ 的形如 $\\begin{pmatrix} a &amp; b \\\\ -b &amp; a \\end{pmatrix}$ (对应共轭复特征值 $a \\pm ib, b \\ne 0$) 的块。 5.4 谱分解 (Spectral Decomposition) 定理 (正规矩阵的谱分解): 设 $A$ 是 $n$ 阶正规矩阵，$\\lambda_1, \\dots, \\lambda_n$ 是其特征值 (不必互异)，$u_1, \\dots, u_n$ 是对应的一组标准正交特征向量。则 $A$ 可以表示为：$A = \\sum_{i=1}^n \\lambda_i u_i u_i^H$如果 $A$ 有 $s$ 个互异特征值 $\\lambda_1, \\dots, \\lambda_s$，则 $A = \\sum_{j=1}^s \\lambda_j P_j$，其中 $P_j = \\sum_{u_k \\in V_{\\lambda_j}} u_k u_k^H$ 是到特征子空间 $V_{\\lambda_j}$ 的正交投影算子。这些投影算子满足 $P_j^H=P_j$, $P_j^2=P_j$, $P_i P_j = O$ ($i \\ne j$), $\\sum_{j=1}^s P_j = E$。 证明:因为 $A$ 正规，所以 $A = U \\Lambda U^H$，其中 $U=(u_1, \\dots, u_n)$ 列向量是标准正交特征向量，$\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)$。$A = (u_1, \\dots, u_n) \\begin{pmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n \\end{pmatrix} \\begin{pmatrix} u_1^H \\\\ \\vdots \\\\ u_n^H \\end{pmatrix}$$= (u_1, \\dots, u_n) \\begin{pmatrix} \\lambda_1 u_1^H \\\\ \\vdots \\\\ \\lambda_n u_n^H \\end{pmatrix} = \\lambda_1 u_1 u_1^H + \\lambda_2 u_2 u_2^H + \\dots + \\lambda_n u_n u_n^H$。□ 可对角化矩阵的谱分解 (一般情况):若 $A$ 可对角化 (不一定是正规的)，$P^{-1}AP = \\Lambda$。设 $P=(p_1, \\dots, p_n)$，$P^{-1} = \\begin{pmatrix} q_1^H \\\\ \\vdots \\\\ q_n^H \\end{pmatrix}$ (这里 $q_i^H p_j = \\delta_{ij}$)。则 $A = P \\Lambda P^{-1} = \\sum_{i=1}^n \\lambda_i p_i q_i^H$。令 $G_i = p_i q_i^H$。则 $G_i$ 是投影算子 (幂等 $G_i^2=G_i$，但通常不是正交投影)，且 $G_i G_j = O$ ($i \\ne j$), $\\sum G_i = E$。$A = \\sum_{j=1}^s \\lambda_j G_j’$, 其中 $G_j’$ 是到对应广义特征空间的投影。","link":"/MATH1408/Chapter5-MATH1408.html"},{"title":"组合，概率与计算专栏","text":"专栏简介记录了一些与概率论结合的算法。本专栏难度较大，如有理解不精的地方，请多包涵~ 专栏目录专题：超图注：本专题参考Morris讲义.定理即定义编号与讲义一致. 第一节：当你的一阶矩方法失效时，该怎么办？—随机图的无三角形子图问题 第二节：更一般的容器定理 第三节：容器定理应用进阶 专题：马尔科夫链与随机游走注：本专题参考Michael Mitzenmacher书第七章，部分定理即定义沿用书中编号 第一节：Definitions and Representations 应用案例1：2-SAT问题 应用案例2：3-SAT问题 第二节： Classification of states &amp; 第三节： Stationary Distributions 第四节：Random walks on Undirected Graphs 参考资料 Probability and Computing by Michael Mitzenmacher &amp; Eli Upfal","link":"/ProbabilityAndComputing/index.html"},{"title":"Chapter8 MATH1408","text":"对偶空间、伴随算子与双线性型1. 对偶空间 (Dual Space)1.1 线性函数/线性泛函 (Linear Functional) 设 $V$ 是数域 $F$ 上的一个向量空间。一个从 $V$ 到 $F$ 的线性映射 $f: V \\to F$ 称为 $V$ 上的一个线性函数或线性泛函。 即，对任意 $\\alpha, \\beta \\in V$ 和任意标量 $k \\in F$，满足： $f(\\alpha + \\beta) = f(\\alpha) + f(\\beta)$ $f(k\\alpha) = k f(\\alpha)$ $V$ 上所有线性泛函的集合，关于通常的函数加法和标量乘法，构成数域 $F$ 上的一个向量空间，称为 $V$ 的对偶空间 (Dual Space)，记作 $V^{*}$。 1.2 Riesz 表示定理 (针对有限维内积空间) 定理: 设 $V$ 是一个 $n$ 维（欧几里得或酉）内积空间 (Inner Product Space)，其内积记为 $(\\cdot, \\cdot)$。则对于 $V$ 上的任意一个线性泛函 $f \\in V^{*}$，存在唯一的向量 $v \\in V$，使得对所有 $x \\in V$，都有：$f(x) = (x, v)$ 证明思路: 存在性: 取 $V$ 的一组标准正交基 $\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_n$。 令 $v = \\overline{f(\\varepsilon_1)}\\varepsilon_1 + \\overline{f(\\varepsilon_2)}\\varepsilon_2 + \\dots + \\overline{f(\\varepsilon_n)}\\varepsilon_n$。（在实空间中，共轭符号可以省略）。 对于任意 $x = \\sum x_i \\varepsilon_i \\in V$：$(x, v) = (\\sum x_i \\varepsilon_i, \\sum \\overline{f(\\varepsilon_j)}\\varepsilon_j) = \\sum_i \\sum_j x_i f(\\varepsilon_j) (\\varepsilon_i, \\varepsilon_j) = \\sum_i x_i f(\\varepsilon_i)$$f(x) = f(\\sum x_i \\varepsilon_i) = \\sum x_i f(\\varepsilon_i)$所以 $(x,v) = f(x)$。 唯一性: 假设存在另一个向量 $u \\in V$ 使得 $f(x) = (x, u)$ 对所有 $x \\in V$ 成立。 则 $(x, v) = (x, u)$，即 $(x, v-u) = 0$ 对所有 $x \\in V$ 成立。 特别地，取 $x = v-u$，则 $(v-u, v-u) = 0$，这意味着 $v-u = 0$，即 $v=u$。□ 1.3 对偶基 (Dual Basis) 设 $V$ 是数域 $F$ 上的 $n$ 维向量空间，$\\{e_1, e_2, \\dots, e_n\\}$ 是 $V$ 的一组基。 存在 $V^{*}$ 中的一组唯一的基 $\\{f^1, f^2, \\dots, f^n\\}$，称为相对于基 $\\{e_i\\}$ 的对偶基，满足：$f^i(e_j) = \\delta_{ij} = \\begin{cases} 1, &amp; \\text{if } i=j \\\\ 0, &amp; \\text{if } i \\ne j \\end{cases}$ 证明: 构造与存在性: 对每个 $i=1, \\dots, n$，定义 $f^i: V \\to F$ 如下：对任意 $x = \\sum_{k=1}^n x_k e_k \\in V$，令 $f^i(x) = x_i$。容易验证 $f^i$ 是线性的且 $f^i(e_j) = \\delta_{ij}$。 线性无关: 设 $\\sum_{i=1}^n k_i f^i = 0$ (零泛函)。将其作用于 $e_j$：$(\\sum k_i f^i)(e_j) = \\sum k_i f^i(e_j) = \\sum k_i \\delta_{ij} = k_j = 0(e_j) = 0$。由于对所有 $j$ 都成立，所以 $k_1 = \\dots = k_n = 0$。因此 $\\{f^i\\}$ 线性无关。 生成性: 对任意 $f \\in V^{*} $，令 $a_i = f(e_i)$。考虑泛函 $g = \\sum_{i=1}^n a_i f^i$。对任意基向量 $e_j$，$g(e_j) = (\\sum a_i f^i)(e_j) = \\sum a_i \\delta_{ij} = a_j = f(e_j)$。由于 $f$ 和 $g$ 在基向量上的取值相同，所以 $f=g$。因此 $\\{f^i\\}$ 生成 $V^{*}$。□ 推论: 如果 $V$ 是有限维的，则 $\\dim V = \\dim V^{*}$。 1.4 自然配对 (Natural Pairing) / 典范配对 (Canonical Pairing) 定义一个映射 $\\langle \\cdot, \\cdot \\rangle : V^{*} \\times V \\to F$ (或 $V \\times V^{*} \\to F$) 为：$\\langle f, x \\rangle = f(x)$ (或 $\\langle x, f \\rangle = f(x)$) 这个配对是非退化的 (non-degenerate)： 如果对所有 $x \\in V$，$\\langle f, x \\rangle = 0$，则 $f=0$ (零泛函)。 如果对所有 $f \\in V^{*}$，$\\langle f, x \\rangle = 0$，则 $x=0$ (零向量)。 证明(2): 若 $x \\ne 0$，则可以将其扩充为 $V$ 的一组基 $\\{x, e_2, \\dots, e_n\\}$。构造其对偶基的第一个元素 $f^1$ 使得 $f^1(x)=1 \\ne 0$，矛盾。 1.5 再对偶空间 (Double Dual Space) $V^{**}$ $V^{**} = (V^{*})^{*}$ 是 $V^{*}$ 的对偶空间。 存在一个从 $V$到 $V^{**}$ 的典范同构 (Canonical Isomorphism) $\\Phi: V \\to V^{**}$，定义为：对于任意 $x \\in V$，$\\Phi(x)$ 是 $V^{*}$ 上的一个线性泛函，它作用于 $f \\in V^{*}$ 的方式是：$(\\Phi(x))(f) = f(x) = \\langle f, x \\rangle$ 证明 $\\Phi$ 是同构: 线性: 容易验证 $\\Phi(kx_1+x_2) = k\\Phi(x_1) + \\Phi(x_2)$。 单射 (Injective): 若 $\\Phi(x) = 0$ ( $V^{**}$ 中的零泛函)，则对所有 $f \\in V^{*}$，$(\\Phi(x))(f) = f(x) = 0$。由上面自然配对的非退化性，知 $x=0$。所以 $\\ker \\Phi = \\{0\\}$。 同维: 因为 $\\dim V = \\dim V^{*} = \\dim V^{**}$ (对于有限维空间)。 因此 $\\Phi$ 是一个同构。 由于这个同构是“自然的”（不依赖于基的选择），我们通常将 $V$ 和 $V^{**}$ 等同 (identify) 起来，即 $V \\cong V^{**}$。我们可以说 $V$ 上的元素 $x$ 可以看作是 $V^{*}$ 上的一个线性泛函，其作用方式是 $x(f) = f(x)$。 2. 伴随算子 (Adjoint Operator)2.1 定义 (针对内积空间) 设 $V$ 是一个有限维（欧几里得或酉）内积空间，$\\phi: V \\to V$ 是一个线性算子。 存在唯一的线性算子 $\\phi^{*}: V \\to V$，称为 $\\phi$ 的伴随算子 (Adjoint Operator)，满足对所有 $u, v \\in V$：$(\\phi(u), v) = (u, \\phi{^*}(v))$ 证明: 存在性: 对固定的 $v \\in V$，考虑映射 $L_v: u \\mapsto (\\phi(u), v)$。$L_v$ 是 $V$ 上的一个线性泛函。由 Riesz 表示定理，存在唯一的向量 $v_1 \\in V$ 使得 $L_v(u) = (\\phi(u), v) = (u, v_1)$ 对所有 $u \\in V$ 成立。定义 $\\phi{^*}(v) = v_1$。 线性: 设 $\\phi{^*}(v_1) = w_1, \\phi^{*}(v_2) = w_2$。则$(\\phi(u), kv_1+v_2) = \\overline{k}(\\phi(u), v_1) + (\\phi(u), v_2)$ (在复空间中)$= \\overline{k}(u, w_1) + (u, w_2) = (u, kw_1+w_2)$。所以 $\\phi^{*}(kv_1+v_2) = kw_1+w_2 = k\\phi^{*}(v_1) + \\phi^{*}(v_2)$。 唯一性: 假设存在另一个算子 $\\psi$ 满足 $(\\phi(u), v) = (u, \\psi(v))$。则 $(u, \\phi^{*}(v)) = (u, \\psi(v))$ 对所有 $u,v$ 成立。即 $(u, \\phi^{*}(v) - \\psi(v)) = 0$。取 $u = \\phi^{*}(v) - \\psi(v)$，得到 $\\phi^{*}(v) - \\psi(v) = 0$，所以 $\\phi^{*} = \\psi$。□ 2.2 在标准正交基下的矩阵表示 设 $\\varepsilon_1, \\dots, \\varepsilon_n$ 是 $V$ 的一组标准正交基。 若 $\\phi$ 在此基下的矩阵为 $A = (a_{ij})$，则 $\\phi(\\varepsilon_j) = \\sum_k a_{kj} \\varepsilon_k$。 $(\\phi(\\varepsilon_j), \\varepsilon_i) = (\\sum_k a_{kj} \\varepsilon_k, \\varepsilon_i) = a_{ij}$。 设 $\\phi^{*}$ 在此基下的矩阵为 $B = (b_{ij})$，则 $\\phi^{*}(\\varepsilon_j) = \\sum_k b_{kj} \\varepsilon_k$。 $(\\varepsilon_j, \\phi^{*}(\\varepsilon_i)) = (\\varepsilon_j, \\sum_k b_{ki} \\varepsilon_k) = \\overline{b_{ji}}$ (在复空间中；实空间中为 $b_{ji}$)。 由伴随定义 $a_{ij} = \\overline{b_{ji}}$。所以 $B = A^{*}$ (矩阵的共轭转置，或实数域中的转置 $A^T$)。 结论: 在标准正交基下，伴随算子 $\\phi^{*}$ 的矩阵是原算子 $\\phi$ 的矩阵的共轭转置 (Hermitian transpose) $A^{*}$ (或转置 $A^T$ 如果是实空间)。 2.3 伴随算子的性质 $(\\phi + \\psi)^{*} = \\phi^{*} + \\psi^{*}$ $(k\\phi)^{*} = \\overline{k} \\phi^{*}$ (在实空间中为 $k\\phi^{*}$) $(\\phi\\psi)^{*} = \\psi^{*} \\phi^{*}$ $(\\phi^{*})^{*} = \\phi$ 2.4 自伴算子 (Self-Adjoint Operator) 如果 $\\phi^{*} = \\phi$，则称 $\\phi$ 是自伴算子。 在实欧氏空间中，称为对称算子 (Symmetric Operator)。其矩阵表示为对称矩阵 $A^T=A$ (在标准正交基下)。 在复酉空间中，称为埃尔米特算子 (Hermitian Operator)。其矩阵表示为埃尔米特矩阵 $A^{*}=A$ (在标准正交基下)。 性质: 自伴算子的特征值都是实数。 自伴算子对应于不同特征值的特征向量是正交的。 自伴算子总可以被正交/酉对角化 (存在一组标准正交的特征向量基)。 如果 $U$ 是 $\\phi$-不变子空间，则 $U^\\perp$ 也是 $\\phi$-不变子空间。 3. 对偶变换 (Dual Transformation) / 转置映射 (Transpose Map) 设 $U, V$ 是数域 $F$ 上的向量空间，$A: U \\to V$ 是一个线性映射。 定义 $A$ 的对偶变换 (Dual Transformation) 或转置映射 (Transpose Map) $A^{*}: V^{*} \\to U^{*}$ (注意方向相反) 如下：对于任意 $f \\in V^{*}$ (即 $f: V \\to F$ 是线性泛函)，$A^{*}(f)$ 是 $U^{*}$ 中的一个元素 (即 $A^{*}(f): U \\to F$ 是线性泛函)，其定义为：$(A^{*}(f))(x) = f(A(x))$ 对所有 $x \\in U$。也可以写作 $A^{*}(f) = f \\circ A$。 验证 $A^{*}$ 是线性的:设 $f_1, f_2 \\in V^{*}$, $k \\in F$。$(A^{*}(k f_1 + f_2))(x) = (k f_1 + f_2)(A(x)) = k f_1(A(x)) + f_2(A(x))$$= k (A^{*}(f_1))(x) + (A^{*}(f_2))(x) = (k A^{*}(f_1) + A^{*}(f_2))(x)$。所以 $A^{*}(k f_1 + f_2) = k A^{*}(f_1) + A^{*}(f_2)$。 与自然配对的关系:$\\langle A^{*}(f), x \\rangle_U = \\langle f, A(x) \\rangle_V$(其中 $\\langle \\cdot, \\cdot \\rangle_U$ 是 $U^{*} \\times U \\to F$ 的配对, $\\langle \\cdot, \\cdot \\rangle_V$ 是 $V^{*} \\times V \\to F$ 的配对)这个形式非常类似于内积空间中伴随算子的定义。 3.1 对偶变换的性质设 $A: U \\to V$, $B: V \\to W$ 是线性映射。 唯一性: 满足 $\\langle A^{*}(f), x \\rangle_U = \\langle f, A(x) \\rangle_V$ 的线性映射 $A^{*}: V^{*} \\to U^{*}$ 是唯一的。 $(BA)^{*} = A^{*} B^{*}$ (注意顺序) 若 $I: V \\to V$ 是恒等映射，则 $I^{*}: V^{*} \\to V^{*}$ 也是恒等映射。 $A$ 是单射 $\\iff A^{*}$ 是满射。 $A$ 是满射 $\\iff A^{*}$ 是单射。 $A$ 是同构 $\\iff A^{*}$ 是同构。 3.2 对偶变换的矩阵表示 设 $\\{e_i\\}$ 是 $U$ 的基，$\\{u_j\\}$ 是 $V$ 的基。 设 $\\{f^i\\}$ 是 $U^{*}$ 中 $\\{e_i\\}$ 的对偶基，$\\{v^j\\}$ 是 $V^{*}$ 中 $\\{u_j\\}$ 的对偶基。 若 $A$ 关于基 $\\{e_i\\}$ 和 $\\{u_j\\}$ 的矩阵为 $M = (m_{ji})$ (即 $A(e_i) = \\sum_j m_{ji} u_j$)。 则 $A^{*}$ 关于基 $\\{v^j\\}$ 和 $\\{f^i\\}$ 的矩阵为 $M^T$ (矩阵的转置)。 证明: 设 $A^{*}(v^k) = \\sum_l c_{lk} f^l$。$c_{ik} = (A^{*}(v^k))(e_i) = v^k(A(e_i)) = v^k(\\sum_j m_{ji} u_j) = \\sum_j m_{ji} v^k(u_j) = \\sum_j m_{ji} \\delta_{kj} = m_{ki}$。所以 $c_{ik} = m_{ki}$，这意味着 $A^{*}$ 的矩阵是 $M^T$。 3.3 行向量空间与列向量空间 如果 $V = F^n$ (列向量空间)，$V_1 = F^m$ (行向量空间)。 则 $V_1$ 可以看作是 $V$ 的对偶空间 $V^{*}$ (通过标准内积/点积 $Y X$)。 若线性映射 $A: F^n \\to F^m$ 由矩阵 $M$ 定义 ($A(X) = MX$)。 则其对偶变换 $A^{*}: (F^m)^{*} \\to (F^n)^{*}$ 可以看作是从 $F^m$ (行向量) 到 $F^n$ (行向量) 的映射，由 $A^{*}(Y) = YM$ 定义。$\\langle Y, AX \\rangle = Y(AX) = (YA)X = \\langle YA, X \\rangle$。 4. 双线性型 (Bilinear Form)4.1 定义 设 $U, V$ 是数域 $F$ 上的向量空间。一个映射 $g: U \\times V \\to F$ 称为一个双线性型 (Bilinear Form)，如果它对每个变量都是线性的： 对固定的 $v \\in V$，映射 $u \\mapsto g(u,v)$ 是 $U$ 上的线性泛函。$g(k u_1 + u_2, v) = k g(u_1, v) + g(u_2, v)$ 对固定的 $u \\in U$，映射 $v \\mapsto g(u,v)$ 是 $V$ 上的线性泛函。$g(u, k v_1 + v_2) = k g(u, v_1) + g(u, v_2)$ 如果 $U=V$，则称 $g$ 是 $V$ 上的双线性型。 4.2 双线性型的矩阵表示 设 $g: U \\times V \\to F$ 是双线性型。 设 $\\{u_1, \\dots, u_m\\}$ 是 $U$ 的一组基，$\\{v_1, \\dots, v_n\\}$ 是 $V$ 的一组基。 定义 $m \\times n$ 矩阵 $A = (a_{ij})$，其中 $a_{ij} = g(u_i, v_j)$。称 $A$ 是 $g$ 关于这两组基的矩阵表示。 若 $x = \\sum x_i u_i \\in U$ (坐标向量 $X = [x_1, \\dots, x_m]^T$)，$y = \\sum y_j v_j \\in V$ (坐标向量 $Y = [y_1, \\dots, y_n]^T$) 则 $g(x,y) = g(\\sum x_i u_i, \\sum y_j v_j) = \\sum_i \\sum_j x_i y_j g(u_i, v_j) = \\sum_i \\sum_j x_i a_{ij} y_j = X^T A Y$。 4.3 基变换对矩阵表示的影响 设 $U$ 中的新基 $\\{u’_k\\}$ 与旧基 $\\{u_i\\}$ 的关系为 $u’_k = \\sum_i p_{ik} u_i$ (即 $[u’_1, \\dots, u’_m] = [u_1, \\dots, u_m]P$，坐标变换 $X = PX’$）。 设 $V$ 中的新基 $\\{v’_l\\}$ 与旧基 $\\{v_j\\}$ 的关系为 $v’_l = \\sum_j q_{jl} v_j$ (即 $[v’_1, \\dots, v’_n] = [v_1, \\dots, v_n]Q$，坐标变换 $Y = QY’$）。 则 $g$ 在新基下的矩阵 $A’$ 满足 $A’ = P^T A Q$。 如果 $U=V$ 且使用相同的基变换矩阵 $P$ (即 $Q=P$)，则 $A’ = P^T A P$。称矩阵 $A$ 和 $A’$ 是合同 (Congruent) 的。 4.4 双线性型的秩 (Rank) 双线性型 $g$ 的矩阵表示 $A$ 的秩不依赖于基的选择 (因为 $P, Q$ 是可逆的)。 这个秩称为双线性型 $g$ 的秩，记作 $\\text{rank}(g)$。 定理: 设 $g: U \\times V \\to F$ 是双线性型。存在 $U$ 的基 $\\{u_i\\}$ 和 $V$ 的基 $\\{v_j\\}$ 使得 $g$ 在这些基下的矩阵为：$D_r = \\begin{pmatrix} E_r &amp; O \\\\ O &amp; O \\end{pmatrix}$其中 $E_r$ 是 $r \\times r$ 的单位矩阵，$r = \\text{rank}(g)$。这个矩阵 $D_r$ 称为 $g$ 的标准型 (Canonical Form)。 4.5 左核与右核 (Left Kernel and Right Kernel) 设 $g: U \\times V \\to F$ 是双线性型。 $g$ 的左核 (Left Kernel): $L = \\{ u \\in U \\mid g(u,v)=0 \\text{ for all } v \\in V \\}$。 $g$ 的右核 (Right Kernel): $R = \\{ v \\in V \\mid g(u,v)=0 \\text{ for all } u \\in U \\}$。 $L$ 是 $U$ 的子空间，$R$ 是 $V$ 的子空间。 $\\dim L = \\dim U - \\text{rank}(g)$ $\\dim R = \\dim V - \\text{rank}(g)$ 如果左核和右核都只包含零向量 (即 $L=\\{0\\}$ 和 $R=\\{0\\}$)，则称 $g$ 是非退化 (Non-degenerate) 的。 定理: 双线性型 $g$ 是非退化的当且仅当其矩阵表示 $A$ 是可逆的 (当 $\\dim U = \\dim V = \\text{rank}(g)$ 时)。 4.6 非退化双线性型与对偶空间的同构 若 $g: U \\times V \\to F$ 是非退化双线性型。 映射 $\\phi: U \\to V^{*}$ 定义为 $(\\phi(u))(v) = g(u,v)$ 是一个线性同构 (当 $\\dim U = \\dim V$ 时)。 线性: 显然。 单射: 若 $\\phi(u) = 0$ (零泛函)，则 $g(u,v)=0$ 对所有 $v \\in V$。由于 $g$ 非退化 (右核为0，这意味着如果 $u \\ne 0$，必有某个 $v$ 使 $g(u,v) \\ne 0$；或者说左核为0)，所以 $u=0$。 若 $\\dim U = \\dim V^{*} (= \\dim V)$，则 $\\phi$ 是同构。 类似地，映射 $\\psi: V \\to U^{*}$ 定义为 $(\\psi(v))(u) = g(u,v)$ 也是一个线性同构。 这意味着非退化双线性型在 $U$ 和 $V^{*}$ (以及 $V$ 和 $U^{*}$) 之间建立了一个自然的同构关系。 特别地，内积 $(\\cdot, \\cdot)$ 就是一个非退化双线性型 (实数域上) 或半双线性型 (复数域上)，它建立了 $V$ 和 $V^{*}$ 之间的同构 (Riesz 表示定理)。 5. 特殊类型的双线性型 ($U=V$)设 $g: V \\times V \\to F$ 是 $V$ 上的双线性型。 5.1 对称双线性型 (Symmetric Bilinear Form) 定义: $g(x,y) = g(y,x)$ 对所有 $x,y \\in V$。 矩阵表示: $A^T = A$ (对称矩阵，相对于任意基)。 正交性 (Orthogonality): 若 $g(x,y)=0$，则称 $x$ 与 $y$ 关于 $g$ 正交，记作 $x \\perp_g y$。(由于对称性，$x \\perp_g y \\iff y \\perp_g x$)。 子空间 $W \\subset V$ 的正交补 (Orthogonal Complement):$W^\\perp = \\{ v \\in V \\mid g(v,w)=0 \\text{ for all } w \\in W \\}$。$W^\\perp$ 是 $V$ 的一个子空间。 注意: 与内积空间不同，这里 $W \\cap W^\\perp$ 可能不是 $\\{0\\}$ (除非 $g$ 在 $W$ 上的限制是非退化的)。$V = W \\oplus W^\\perp$ 成立的条件是 $g$ 在 $W$ 上的限制是非退化的。 核 (Kernel) / 根 (Radical): $\\text{rad}(V) = V^\\perp = \\{ v \\in V \\mid g(v,x)=0 \\text{ for all } x \\in V \\}$。$g$ 是非退化的 $\\iff \\text{rad}(V) = \\{0\\}$。 5.2 斜对称/反称双线性型 (Skew-symmetric/Antisymmetric Bilinear Form) 定义: $g(x,y) = -g(y,x)$ 对所有 $x,y \\in V$。 矩阵表示: $A^T = -A$ (斜对称/反称矩阵，相对于任意基)。 一个重要的推论: $g(x,x) = -g(x,x) \\Rightarrow 2g(x,x)=0$。 如果数域 $F$ 的特征 $\\text{char}(F) \\ne 2$ (例如 $\\mathbb{R}, \\mathbb{C}$)，则 $g(x,x)=0$ 对所有 $x \\in V$。 当 $g(x,x)=0$ 对所有 $x \\in V$ 时，称 $g$ 是交错双线性型 (Alternating Bilinear Form)。 如果 $\\text{char}(F) \\ne 2$，则斜对称与交错是等价的。(由 $g(x+y, x+y)=0$ 展开可得 $g(x,y)+g(y,x)=0$) 5.3 双线性型与线性算子的伴随 (Adjoint of an Operator with respect to a Bilinear Form) 设 $g: V \\times V \\to F$ 是一个非退化双线性型。 对任意线性算子 $\\phi: V \\to V$，存在唯一的线性算子 $\\phi^{\\text{adj}_g}: V \\to V$，称为 $\\phi$ 关于 $g$ 的伴随 (Adjoint)，使得：$g(\\phi(x), y) = g(x, \\phi^{\\text{adj}_g}(y))$ 对所有 $x,y \\in V$。 证明: 类似内积空间伴随算子的证明，利用 $g$ 的非退化性以及 $V$ 与 $V^{*}$ 的同构。 固定 $y$，映射 $L_y(x) = g(\\phi(x),y)$ 是 $V$ 上的线性泛函。 由于 $g$ 非退化，存在同构 $\\Phi_g: V \\to V^{*}$，$(\\Phi_g(z))(x) = g(x,z)$。 $L_y \\in V^{*}$，所以存在唯一的 $z_y \\in V$ 使得 $L_y = \\Phi_g(z_y)$，即 $g(\\phi(x),y) = g(x, z_y)$。 定义 $\\phi^{\\text{adj}_g}(y) = z_y$。 若 $g$ 是对称的，且 $A$ 是 $\\phi$ 的矩阵， $B$ 是 $g$ 的矩阵（可逆），则 $\\phi^{\\text{adj}_g}$ 的矩阵 $C$ 满足 $A^T B = B C$，即 $C = B^{-1} A^T B$。(与相似变换 $P^{-1}AP$ 对比) 6. 辛空间 (Symplectic Space)6.1 定义 一个辛空间 (Symplectic Space) $(V, \\omega)$ 是一个向量空间 $V$ 配备一个非退化的、交错的 (因此也是斜对称的) 双线性型 $\\omega: V \\times V \\to F$，称为辛形式 (Symplectic Form)。 $\\omega(x,y) = -\\omega(y,x)$ (斜对称) $\\omega(x,x) = 0$ (交错) $\\text{rad}(V) = \\{0\\}$ (非退化) 6.2 辛基 (Symplectic Basis) / 达布基 (Darboux Basis) 定理: 任何有限维辛空间 $(V, \\omega)$ 都有偶数维，设为 $2r$。存在 $V$ 的一组基 $\\{u_1, v_1, \\dots, u_r, v_r\\}$ (称为辛基或达布基) 使得： $\\omega(u_i, v_i) = 1$ for $i=1, \\dots, r$ $\\omega(v_i, u_i) = -1$ for $i=1, \\dots, r$ 所有其他配对 $\\omega(u_i, u_j)$, $\\omega(v_i, v_j)$, $\\omega(u_i, v_j)$ (当 $i \\ne j$) 都为0。 $\\omega$ 在此基下的矩阵表示为 $J_{2r} = \\begin{pmatrix} O_r &amp; E_r \\\\ -E_r &amp; O_r \\end{pmatrix}$ 或 $\\begin{pmatrix} O_r &amp; -E_r \\\\ E_r &amp; O_r \\end{pmatrix}$ (取决于基的排列顺序，如 $u_1, \\dots, u_r, v_1, \\dots, v_r$)或者，如PPT中常见的形式，基排列为 $u_1, v_1, u_2, v_2, \\dots$:矩阵为 $\\text{diag}(S, S, \\dots, S)$ 其中 $S = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix}$。 证明思路 (构造性): 若 $\\omega=0$ (零形式)，则 $r=0$，结论成立。 若 $\\omega \\ne 0$，则存在 $u,v$ 使得 $\\omega(u,v) = a \\ne 0$。令 $u_1 = a^{-1}u, v_1 = v$，则 $\\omega(u_1, v_1) = 1$。$u_1, v_1$ 线性无关 (因为 $\\omega(u_1, u_1)=0$)。 令 $W_1 = \\text{span}\\{u_1, v_1\\}$。$\\omega$ 在 $W_1$ 上的限制是非退化的。所以 $V = W_1 \\oplus W_1^\\perp$ (这里 $W_1^\\perp = \\{ x \\in V \\mid \\omega(x,w)=0 \\text{ for all } w \\in W_1 \\}$) 对 $W_1^\\perp$ 和 $\\omega$ 在其上的限制重复此过程。对任意 $x \\in V$，令 $y = x - \\omega(x,v_1)u_1 + \\omega(x,u_1)v_1$。则 $\\omega(y, u_1) = \\omega(x,u_1) - \\omega(x,u_1)\\omega(v_1,u_1) = \\omega(x,u_1) - \\omega(x,u_1)(-1) = 0$? 这里计算有误。应该是 $\\omega(y,u_1) = \\omega(x,u_1) - \\omega(x,v_1)\\omega(u_1,u_1) + \\omega(x,u_1)\\omega(v_1,u_1) = \\omega(x,u_1) + \\omega(x,u_1)(-1)=0$。$\\omega(y,v_1) = \\omega(x,v_1) - \\omega(x,v_1)\\omega(u_1,v_1) + \\omega(x,u_1)\\omega(v_1,v_1) = \\omega(x,v_1) - \\omega(x,v_1)(1)=0$。所以 $y \\in W_1^\\perp$。因此 $x = y + (\\omega(x,v_1)u_1 - \\omega(x,u_1)v_1)$，其中 $y \\in W_1^\\perp$ 且第二项在 $W_1$ 中。所以 $V = W_1 + W_1^\\perp$。由于 $W_1 \\cap W_1^\\perp = \\{0\\}$ (因为 $\\omega$ 在 $W_1$ 上非退化)，所以是直和。 数学归纳法完成。 推论: 任何有限维辛空间的维数都是偶数。 任何域上的斜对称矩阵都合同于形如 $\\text{diag}(S, \\dots, S, O, \\dots, O)$ 的矩阵，其秩必为偶数 $2r$。 6.3 辛变换 (Symplectic Transformation) / 保辛变换 设 $(V, \\omega)$ 是一个辛空间。一个线性自同构 $\\phi: V \\to V$ 称为一个辛变换 (Symplectic Transformation) 或保辛变换 (Symplectomorphism)，如果它保持辛形式不变：$\\omega(\\phi(x), \\phi(y)) = \\omega(x,y)$ 对所有 $x,y \\in V$。 性质: 辛变换保持辛基为辛基 (的线性组合形式)。 所有辛变换的集合构成一个群，称为辛群 (Symplectic Group)，记作 $Sp(V, \\omega)$ 或 $Sp(2r, F)$。 恒等变换是辛变换。 辛变换的逆也是辛变换。 两个辛变换的复合也是辛变换。 7. 二次型 (Quadratic Form)7.1 定义 设 $V$ 是数域 $F$ 上的向量空间 (通常假设 $\\text{char}(F) \\ne 2$)。 一个映射 $q: V \\to F$ 称为一个二次型 (Quadratic Form)，如果存在一个双线性型 $g: V \\times V \\to F$ 使得：$q(x) = g(x,x)$ 对所有 $x \\in V$。 可以取 $g$ 为对称双线性型，称为 $q$ 的极化形式 (Polar Form):$g(x,y) = \\frac{1}{2} [q(x+y) - q(x) - q(y)]$(这由 $q(x+y) = g(x+y, x+y) = g(x,x) + g(x,y) + g(y,x) + g(y,y)$ 导出，若 $g$ 对称则 $g(x,y)+g(y,x) = 2g(x,y)$)。 二次型 $q$ 与其对称的极化形式 $g$ 之间是一一对应的。 7.2 二次型的矩阵表示 若 $g$ 的矩阵为 $A$ (对称矩阵)，则 $q(x) = X^T A X$，其中 $X$ 是 $x$ 的坐标向量。 二次型的对角化: 任何对称双线性型 $g$ (因此任何二次型 $q$) 在某个基下都可以对角化，即其矩阵表示为对角矩阵 $\\text{diag}(d_1, \\dots, d_n)$。 这意味着存在一组基 $\\{e_1, \\dots, e_n\\}$ 使得 $g(e_i, e_j)=0$ 当 $i \\ne j$ 时。 $q(x) = \\sum_{i=1}^n d_i x_i^2$ (平方和形式)。 8. 正交变换 (Orthogonal Transformation) / 等距变换 (Isometry)8.1 定义 (针对对称双线性型) 设 $V$ 是一个向量空间， $g: V \\times V \\to F$ 是一个（通常为非退化的）对称双线性型。 一个线性自同构 $\\eta: V \\to V$ 称为关于 $g$ 的正交变换 (Orthogonal Transformation) 或等距变换 (Isometry)，如果它保持 $g$ 不变：$g(\\eta(x), \\eta(y)) = g(x,y)$ 对所有 $x,y \\in V$。 特别地，如果 $g$ 是内积 (如欧氏空间)，则这就是通常意义下的正交变换。 性质: 所有关于 $g$ 的正交变换的集合构成一个群，称为正交群 (Orthogonal Group)，记作 $O(V,g)$ 或 $O(n,F)$ (如果 $g$ 是标准形式)。 恒等变换是正交变换。 正交变换的逆也是正交变换。 两个正交变换的复合也是正交变换。 8.2 镜面反射 (Reflection) / Householder 变换 设 $v \\in V$ 且 $g(v,v) \\ne 0$ (即 $v$ 不是迷向向量 / isotropic vector)。 定义镜面反射 (Reflection) (或 Householder 变换) $S_v: V \\to V$ 为：$S_v(x) = x - 2 \\frac{g(x,v)}{g(v,v)} v$ 性质: $S_v$ 是一个正交变换: $g(S_v(x), S_v(y)) = g(x,y)$。(展开验证，比较繁琐但直接)。 $S_v(v) = -v$。 如果 $u \\perp_g v$ (即 $g(u,v)=0$)，则 $S_v(u)=u$。(超平面 $\\{x \\mid g(x,v)=0\\}$ 被 $S_v$ 固定)。 $S_v^2 = I$ (恒等变换)，所以 $S_v^{-1} = S_v$。 8.3 Cartan-Dieudonné 定理 定理 (Cartan-Dieudonné): 设 $(V,g)$ 是一个 $n$ 维非退化对称双线性型空间 (在特征不为2的域上)。则任何正交变换 $\\eta \\in O(V,g)$都可以表示为至多 $n$ 个镜面反射的乘积。 证明思路 (归纳法): $n=1$: 若 $\\eta(x) = cx$，则 $g(cx,cx) = c^2 g(x,x) = g(x,x) \\Rightarrow c^2=1 \\Rightarrow c=\\pm 1$。若 $c=1$, $\\eta=I$ (0个反射)。若 $c=-1$, $\\eta = S_x$ (1个反射)。 假设对 $n-1$ 维成立。 如果存在 $u \\in V$ 使得 $\\eta(u)=u$ 且 $g(u,u) \\ne 0$。则 $\\eta$ 将 $u^\\perp$ 映射到自身。$u^\\perp$ 是 $n-1$ 维非退化空间。$\\eta|_{u^\\perp}$ 可以表示为 $u^\\perp$ 中至多 $n-1$ 个反射的乘积。这些反射可以扩展到 $V$ 上的反射。 如果对所有 $g(u,u) \\ne 0$ 的 $u$ 都有 $\\eta(u) \\ne u$。取 $u$ 使得 $g(u,u) \\ne 0$。令 $v = \\eta(u)$。则 $g(v,v)=g(u,u) \\ne 0$。 情况1: $g(u-v, u-v) \\ne 0$。令 $\\rho = S_{u-v}$。则 $\\rho(u-v) = -(u-v) = v-u$。$\\rho(u) - \\rho(v) = v-u$。由于 $g(u+v, u-v) = g(u,u) - g(u,v) + g(v,u) - g(v,v) = -g(u,v) + g(v,u) = 0$ (因为 $g(u,u)=g(v,v)$)。所以 $u+v \\perp_g u-v$。因此 $\\rho(u+v) = u+v$。$\\rho(u) + \\rho(v) = u+v$。联立解得 $\\rho(u)=v$ 且 $\\rho(v)=u$。令 $\\xi = \\rho^{-1} \\eta = \\rho \\eta$。则 $\\xi(u) = \\rho(\\eta(u)) = \\rho(v) = u$。现在 $\\xi$ 固定了 $u$，可以应用情况3的论证。$\\eta = \\rho \\xi$。 情况2: $g(u-v, u-v) = 0$。需要更复杂的处理，或者选择不同的 $u$。(如果 $F=\\mathbb{R}$ 且 $g$ 正定，则这种情况不会发生除非 $u=v$) 推论: 在 $n$ 维欧氏空间中，任何正交变换都可以表示为至多 $n$ 个关于超平面的反射的乘积。","link":"/MATH1408/Chapter8-MATH1408.html"},{"title":"策略：两阶段曝光","text":"核心思想：将一个复杂的概率问题分解成两个更简单的、独立的步骤来分析。 1. 目标 (The Goal)我们的最终目标是证明某个理想的性质 $A$ 发生的概率 $P(A)$ 非常高，高到当某个参数（比如 $n$）趋于无穷时，这个概率趋近于 1。用数学语言表示，我们想证明： $P(A) \\to 1$ 直接证明这一点可能很困难，所以引入了这个两阶段的技巧。 2. 策略：两阶段曝光 (Two-Round Exposure)将生成随机对象的过程（比如随机图）分成两轮（或两步）： 第一轮 (Round 1): 以概率 $p_1$ 随机选择一部分元素，生成一个“半成品”。 第二轮 (Round 2): 在第一轮的基础上，再以概率 $p_2$ 独立地加入更多随机元素，形成最终成品。 符号解释： $Γ$: 所有可能的基本元素组成的集合。例如，在随机图中，$Γ$ 就是所有可能的边的集合。 $A$: 我们最终想要的好性质（一个事件）。 $B$: 中间性质。我们希望这个性质在第一轮结束后就大概率出现。 $P(A)$: 经过两轮之后，性质 $A$ 发生的总概率。这是我们想要求下界的量。 $P_1(B)$: 只经过第一轮后，性质 $B$ 发生的概率。 $P_F(A)$: 条件概率。在第一轮的结果是 $F$的条件下第二轮结束后，性质 $A$ 发生的概率。 3. 核心论证这段话的核心是推导并利用下面这个不等式： $P(A) \\ge P_{F_0}(A) \\cdot P_1(B)$ 全概率公式的变体: 根据全概率公式，$P(A)$ 的概率可以分解为所有可能的第一轮结果 $F$ 的情况之和。我们只关心那些满足“有帮助的性质 $B$”的 $F$，这样可以得到一个下界：$P(A) = \\sum_{F \\subseteq \\Gamma} P_F(A) P_1(F) \\ge \\sum_{F \\in B} P_F(A) P_1(F)$ 这里的 $P_1(F)$ 指第一轮恰好得到结果 $F$ 的概率。 这个和式计算的是“经过一个好的第一轮结果，最终得到好结果A”的总概率。 找到最坏情况 : 在所有满足性质 $B$ 的第一轮结果 $F$ 中，总有一个“最不利”的，它使得第二轮后实现 $A$ 的概率 $P_F(A)$ 变得最小。我们将这个最不利的结果记为 $F_0$。 因此，对于任何一个属于 $B$ 的结果 $F$ (即 $F \\in B$)，都有 $P_F(A) \\ge P_{F_0}(A)$。 放缩不等式: 我们把上面求和式中的每一项 $P_F(A)$ 都用这个最小值 $P_{F_0}(A)$ 替换掉，这样会使总和变得更小（或不变），不等式依然成立：$\\sum_{F \\in B} P_F(A) P_1(F) \\ge \\sum_{F \\in B} P_{F_0}(A) P_1(F)$ 提取公因式: $P_{F_0}(A)$ 是一个与求和变量 $F$ 无关的固定值，可以提取出来：$= P_{F_0}(A) \\cdot \\sum_{F \\in B} P_1(F)$ 最终的不等式: 观察剩下的求和项 $\\sum_{F \\in B} P_1(F)$。这正是所有满足性质 $B$ 的结果 $F$ 的概率之和，根据概率的定义，它就是性质 $B$ 本身在第一轮发生的概率，即 $P_1(B)$。 这样我们就得到了最终的关键不等式：$P(A) \\ge P_{F_0}(A) \\cdot P_1(B)$。 4. 结论与实际操作&emsp;&emsp;$P(A) \\ge P_{F_0}(A) \\cdot P_1(B)$ 如果我们想证明 $P(A) \\to 1$，我们只需要证明不等式右边的两项都趋近于 1 即可： 证明 $P_1(B) \\to 1$: 这意味着，在第一轮（通常是随机元素比较稀疏的阶段）结束后，中间性质 $B$ 就几乎肯定会发生。 例子: 证明用较小的概率 $p_1$ 随机加边，图就几乎不可能留下孤立的顶点。 证明 $P_F(A) \\to 1$ 对所有 $F \\in B$ 成立 (尤其对最坏情况 $F_0$ 成立): 这一步是关键。我们现在可以假设第一轮已经结束，并且我们得到了一个满足性质 $B$ 的结果 $F$。 然后我们进行第二轮，加入概率为 $p_2$ 的新随机元素。 我们要证明，不管第一轮给我们的 $F$ 是 $B$ 里的哪一个（哪怕是“最坏”的那个 $F_0$），经过第二轮的“补强”后，最终性质 $A$ 都几乎肯定会发生。 例子: 假设一个图已经没有孤立顶点了（这是 $F \\in B$）。现在我们再以概率 $p_2$ 往里面加一些边，证明这几乎肯定会把整个图连通起来。","link":"/RandomGraph/2round-exposure.html"},{"title":"Chapter7 MATH1408","text":"奇异值分解1. 引言与动机 回顾特征值分解: 对于方阵 $A$ (特别是对称阵或可对角化阵)，我们有特征值分解 $A = P D P^{-1}$ (或 $A = U \\Lambda U^T$ 对于对称阵，其中 $U$ 是正交阵，$\\Lambda$ 是对角阵)。特征值分解在理解线性变换、解微分方程等方面非常有用。 问题: 对于任意的 $m \\times n$ 矩阵 $A$ (不一定是方阵，也不一定对称)，是否存在类似的分解，能够揭示其内在结构？ 目标: 找到两个正交矩阵 $U$ ($m \\times m$) 和 $V$ ($n \\times n$) 以及一个“对角”矩阵 $\\Sigma$ ($m \\times n$)，使得 $A = U \\Sigma V^T$。 这里的“对角”矩阵 $\\Sigma$ 指的是其主对角线上的元素 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r &gt; 0$ (其中 $r = \\text{rank}(A)$)，其余元素为0。这些 $\\sigma_i$ 称为奇异值 (Singular Values)。 历史: Beltrami (1873) 等人早期对二次型和双线性型进行研究时有所涉及。 几何意义 (初步): 考虑双线性型 $f(X,Y) = X^T A Y$。通过坐标变换 $X = U\\xi$ 和 $Y = V\\eta$ (其中 $U, V$ 是正交的)，我们希望 $f(X,Y) = \\xi^T (U^T A V) \\eta = \\xi^T \\Sigma \\eta$，其中 $\\Sigma$ 是对角形式。这启发了 $A = U \\Sigma V^T$ 的形式。 2. 奇异值分解定理 (SVD Theorem)2.1 定理叙述任何 $m \\times n$ 的实矩阵 $A$ (秩为 $r$) 都可以分解为：$A = U \\Sigma V^T$其中： $U$ 是一个 $m \\times m$ 的正交矩阵 (Orthogonal Matrix)。$U$ 的列向量 $u_1, u_2, \\dots, u_m$ 称为 $A$ 的左奇异向量 (Left Singular Vectors)。 $V$ 是一个 $n \\times n$ 的正交矩阵 (Orthogonal Matrix)。$V$ 的列向量 $v_1, v_2, \\dots, v_n$ 称为 $A$ 的右奇异向量 (Right Singular Vectors)。 $\\Sigma$ 是一个 $m \\times n$ 的对角矩阵，其形式为：$\\Sigma = \\begin{pmatrix}D &amp; O \\\\O &amp; O\\end{pmatrix}$其中 $D = \\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_r)$ 是一个 $r \\times r$ 的对角矩阵，且奇异值 (Singular Values) $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r &gt; 0$。$O$ 代表零矩阵块。$\\Sigma$ 的具体形式取决于 $m, n, r$ 的关系： 如果 $m=n=r$: $\\Sigma = D$ 如果 $m &gt; r, n = r$: $\\Sigma = \\begin{pmatrix} D \\\\ O \\end{pmatrix}$ 如果 $m = r, n &gt; r$: $\\Sigma = \\begin{pmatrix} D &amp; O \\end{pmatrix}$ 一般形式 (如PPT所示，假设 $m \\ge n$):$\\Sigma = \\begin{pmatrix}\\sigma_1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\0 &amp; \\sigma_2 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\0 &amp; 0 &amp; \\dots &amp; \\sigma_r &amp; 0 &amp; \\dots &amp; 0 \\\\0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\\\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\end{pmatrix}_{m \\times n}$ 2.2 奇异值与 $A^T A$ 和 $A A^T$ 的关系 考虑矩阵 $B = A^T A$ (这是一个 $n \\times n$ 的对称半正定矩阵)。 $B$ 的特征值 $\\lambda_i \\ge 0$。 由于 $V^T (A^T A) V = V^T (U \\Sigma V^T)^T (U \\Sigma V^T) V = V^T (V \\Sigma^T U^T U \\Sigma V^T) V = \\Sigma^T \\Sigma$。而 $\\Sigma^T \\Sigma$ 是一个 $n \\times n$ 的对角矩阵，其对角元素为 $\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_r^2, 0, \\dots, 0$。因此，$A^T A$ 的非零特征值恰好是 $A$ 的非零奇异值的平方，即 $\\lambda_i = \\sigma_i^2$。$V$ 的列向量 $v_i$ 是 $A^T A$ 的（标准正交）特征向量。 类似地，考虑 $A A^T$ (这是一个 $m \\times m$ 的对称半正定矩阵)。 $A A^T = (U \\Sigma V^T)(U \\Sigma V^T)^T = U \\Sigma V^T V \\Sigma^T U^T = U (\\Sigma \\Sigma^T) U^T$。$\\Sigma \\Sigma^T$ 是一个 $m \\times m$ 的对角矩阵，其对角元素为 $\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_r^2, 0, \\dots, 0$。因此，$A A^T$ 的非零特征值也恰好是 $A$ 的非零奇异值的平方。$U$ 的列向量 $u_i$ 是 $A A^T$ 的（标准正交）特征向量。 结论: $A$ 的奇异值 $\\sigma_i$ 是 $A^T A$ (或 $A A^T$) 的非零特征值的正平方根。 右奇异向量 $v_i$ 是 $A^T A$ 对应于特征值 $\\sigma_i^2$ 的特征向量。 左奇异向量 $u_i$ 是 $A A^T$ 对应于特征值 $\\sigma_i^2$ 的特征向量。 2.3 SVD定理的证明思路 (PPT中的推导) 构造 $V$: 考虑 $n \\times n$ 对称半正定矩阵 $B = A^T A$。 $B$ 可以被正交对角化: $V^T B V = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)$，其中 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$ 是 $B$ 的特征值， $V$ 是由 $B$ 的标准正交特征向量构成的正交矩阵。 由于 $\\text{rank}(A^T A) = \\text{rank}(A) = r$，所以 $B$ 恰好有 $r$ 个正特征值。令 $\\sigma_i = \\sqrt{\\lambda_i}$ for $i=1, \\dots, r$。则 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r &gt; 0$。其余 $\\lambda_{r+1} = \\dots = \\lambda_n = 0$。 将 $V$ 的列向量分块为 $V = [V_1 | V_2]$，其中 $V_1 = [v_1, \\dots, v_r]$ 对应非零特征值， $V_2 = [v_{r+1}, \\dots, v_n]$ 对应零特征值。 我们有 $A^T A V_1 = V_1 \\Lambda^2_D$ (其中 $\\Lambda_D = \\text{diag}(\\sigma_1, \\dots, \\sigma_r)$，所以 $\\Lambda_D^2 = \\text{diag}(\\sigma_1^2, \\dots, \\sigma_r^2)$) 并且 $A^T A V_2 = O$。由于 $N(A^T A) = N(A)$ (零空间相同)，所以 $A V_2 = O$。 构造 $U_1$: 令 $U_1 = A V_1 \\Lambda_D^{-1}$ (这是一个 $m \\times r$ 的矩阵)。(注意 $\\Lambda_D^{-1} = \\text{diag}(1/\\sigma_1, \\dots, 1/\\sigma_r)$ 是存在的，因为 $\\sigma_i &gt; 0$) 验证 $U_1$ 的列是标准正交的：$U_1^T U_1 = (\\Lambda_D^{-1})^T V_1^T A^T A V_1 \\Lambda_D^{-1}$$= \\Lambda_D^{-1} V_1^T (V_1 \\Lambda_D^2) \\Lambda_D^{-1}$ (因为 $V_1^T V_1 = E_r$ 且 $\\Lambda_D$ 是对角的)$= \\Lambda_D^{-1} (V_1^T V_1) \\Lambda_D^2 \\Lambda_D^{-1}$$= \\Lambda_D^{-1} E_r \\Lambda_D^2 \\Lambda_D^{-1} = \\Lambda_D^{-1} \\Lambda_D = E_r$。 所以 $U_1$ 的 $r$ 个列向量是标准正交的。 构造 $U$: $U_1$ 的列向量张成了 $A$ 的列空间 $C(A)$ 的一个标准正交基。 如果 $r &lt; m$，则 $C(A)$ 的维数是 $r$。我们可以将 $U_1$ 的 $r$ 个列向量扩展为 $m$ 维空间 $\\mathbb{R}^m$ 的一个标准正交基，得到 $U = [U_1 | U_2]$，其中 $U_2$ 是一个 $m \\times (m-r)$ 的矩阵，其列向量与 $U_1$ 的列向量正交，并且自身也是标准正交的。$U_2$ 的列张成了 $N(A^T)$ (A的左零空间)。 这样构造的 $U$ 是一个 $m \\times m$ 的正交矩阵。 验证分解: $U^T A V = \\begin{pmatrix} U_1^T \\\\ U_2^T \\end{pmatrix} A [V_1 | V_2] = \\begin{pmatrix} U_1^T A V_1 &amp; U_1^T A V_2 \\\\ U_2^T A V_1 &amp; U_2^T A V_2 \\end{pmatrix}$ $U_1^T A V_1 = (A V_1 \\Lambda_D^{-1})^T A V_1 = (\\Lambda_D^{-1})^T V_1^T A^T A V_1 = \\Lambda_D^{-1} (V_1^T V_1 \\Lambda_D^2) = \\Lambda_D^{-1} \\Lambda_D^2 = \\Lambda_D = \\text{diag}(\\sigma_1, \\dots, \\sigma_r)$。 $A V_2 = O$ (如步骤1所述)，所以 $U_1^T A V_2 = O$ 和 $U_2^T A V_2 = O$。 $U_2^T A V_1$: 由于 $U_1$ 的列张成 $C(A)$，而 $U_2$ 的列与 $U_1$ 的列正交，所以 $U_2$ 的列位于 $C(A)$ 的正交补 $N(A^T)$ 中。$A V_1 = U_1 \\Lambda_D$ (来自 $U_1$ 的定义)。$A V_1$ 的列在 $C(A)$ 中。因此 $U_2^T (A V_1) = O$。 所以 $U^T A V = \\begin{pmatrix} \\Lambda_D &amp; O \\\\ O &amp; O \\end{pmatrix} = \\Sigma$。 因此 $A = U \\Sigma V^T$。□ 2.4 奇异向量之间的关系从 $A = U \\Sigma V^T$ 可以得到： $A V = U \\Sigma$ $A^T U = V \\Sigma^T$ 写成分量形式： $A v_i = \\sigma_i u_i$ for $i=1, \\dots, r$ $A v_i = 0$ for $i=r+1, \\dots, n$ (因为 $\\sigma_i=0$ for $i&gt;r$) $A^T u_i = \\sigma_i v_i$ for $i=1, \\dots, r$ $A^T u_i = 0$ for $i=r+1, \\dots, m$ 这表明： 右奇异向量 $v_i$ (来自 $V_1$) 被 $A$ 映射到左奇异向量 $u_i$ (来自 $U_1$) 的 $\\sigma_i$ 倍。 右奇异向量 $v_i$ (来自 $V_2$) 位于 $A$ 的零空间 $N(A)$。 左奇异向量 $u_i$ (来自 $U_1$) 被 $A^T$ 映射到右奇异向量 $v_i$ (来自 $V_1$) 的 $\\sigma_i$ 倍。 左奇异向量 $u_i$ (来自 $U_2$) 位于 $A^T$ 的零空间 $N(A^T)$ (也称为 $A$ 的左零空间)。 3. SVD 的几何解释 SVD 将一个线性变换 $X \\mapsto AX$ 分解为三个几何操作： 旋转/反射 (Rotation/Reflection): $V^T X$。将输入向量 $X$ 在 $\\mathbb{R}^n$ 中进行旋转或反射，将其与 $V$ 的列向量（右奇异向量，构成标准正交基）对齐。 缩放 (Scaling): $\\Sigma (V^T X)$。将旋转后的向量的每个分量沿着新的轴（由 $V$ 的列定义）进行缩放。前 $r$ 个分量按 $\\sigma_i$ 缩放，其余分量变为0。结果向量在 $\\mathbb{R}^m$ 中。 旋转/反射 (Rotation/Reflection): $U (\\Sigma V^T X)$。将缩放后的向量在 $\\mathbb{R}^m$ 中进行旋转或反射，将其与 $U$ 的列向量（左奇异向量，构成标准正交基）对齐。 例子: 考虑 $\\mathbb{R}^2$ 中的单位圆。 $V^T$ 旋转单位圆。 $\\Sigma$ 将旋转后的圆（仍然是圆）沿着主轴拉伸/压缩成一个椭圆（如果 $\\sigma_1 \\ne \\sigma_2$）。如果某个 $\\sigma_i=0$，则降维。 $U$ 再将这个椭圆在 $\\mathbb{R}^m$ (这里是 $\\mathbb{R}^2$) 中旋转。 最终结果是，SVD 表明任何线性变换 $A$ 将 $\\mathbb{R}^n$ 中的单位球体（或超球体）映射为 $\\mathbb{R}^m$ 中的一个椭球体（或超椭球体，可能退化）。椭球体的主轴方向由左奇异向量 $u_i$ 给出，主轴的半轴长度由奇异值 $\\sigma_i$ 给出。 4. SVD 的性质与应用4.1 外积展开 (Outer Product Expansion)$A = U \\Sigma V^T = \\sum_{i=1}^r \\sigma_i u_i v_i^T$ 这是一个非常重要的性质，它将矩阵 $A$ 表示为 $r$ 个秩为1的矩阵之和。 每个 $u_i v_i^T$ 是一个 $m \\times n$ 的秩1矩阵。 这个展开式对于低秩近似非常关键。 4.2 低秩近似 (Low-Rank Approximation) - Eckart-Young-Mirsky 定理 定理: 设 $A$ 的SVD为 $A = \\sum_{i=1}^r \\sigma_i u_i v_i^T$。对于任意 $k &lt; r$，矩阵 $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$ 是秩为 $k$ 的矩阵中，与 $A$ 在 Frobenius 范数意义下最接近的矩阵。即，$\\min_{\\text{rank}(B)=k} |A-B|_F = |A-A_k|_F = \\sqrt{\\sum_{i=k+1}^r \\sigma_i^2}$。（对于谱范数也有类似结论：$\\min_{\\text{rank}(B)=k} |A-B|_2 = |A-A_k|_2 = \\sigma_{k+1}$）。 应用: 数据压缩: 图像、信号等可以表示为矩阵。通过SVD找到最重要的奇异值和奇异向量，用 $A_k$ 近似 $A$，可以大大减少存储需求。PPT中提到存储 $A_k$ 需要 $k(m+n+1)$ 个数值，而存储 $A$ 需要 $mn$ 个数值。 降噪: 较小的奇异值通常对应于数据中的噪声，将其去除可以得到更清晰的信号。 主成分分析 (PCA): SVD与PCA密切相关。$A^T A$ 的特征向量（即 $V$）是数据的主成分方向。 4.3 其他应用 伪逆 (Pseudoinverse): $A^+ = V \\Sigma^+ U^T$，其中 $\\Sigma^+$ 是将 $\\Sigma$ 中的非零奇异值 $\\sigma_i$ 替换为 $1/\\sigma_i$ 再转置得到的。伪逆用于解线性最小二乘问题。 推荐系统: SVD被用于协同过滤算法，如Netflix Prize。 数值稳定性: SVD是计算矩阵的秩、解线性方程组、最小二乘问题等最稳健的方法之一。 求解齐次线性方程组: $N(A)$ 由对应于零奇异值的右奇异向量 $v_{r+1}, \\dots, v_n$ 张成。 确定四个基本子空间: $C(A)$ (列空间) 的标准正交基是 $u_1, \\dots, u_r$。 $N(A^T)$ (左零空间) 的标准正交基是 $u_{r+1}, \\dots, u_m$。 $C(A^T)$ (行空间) 的标准正交基是 $v_1, \\dots, v_r$。 $N(A)$ (零空间) 的标准正交基是 $v_{r+1}, \\dots, v_n$。 5. 例子 (PPT中的例子) 例1: $A = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 \\end{pmatrix}$ 计算 $A^T A = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; -1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 \\\\ 1 &amp; -1 &amp; 2 \\end{pmatrix}$。(PPT这里直接给出了 $A A^T$ 或其他矩阵，应从 $A^T A$ 开始找 $V$ 和 $\\sigma_i$)正确的做法是：$A^T A = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 \\\\ 1 &amp; -1 &amp; 2 \\end{pmatrix}$ (这是一个3x3矩阵)特征值方程 $|\\lambda I - A^T A|=0$ 解出来是 $\\lambda_1=3, \\lambda_2=1, \\lambda_3=0$。所以奇异值 $\\sigma_1=\\sqrt{3}, \\sigma_2=1$。 ($r=2$)$\\Sigma = \\begin{pmatrix} \\sqrt{3} &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{pmatrix}$。找到对应的 $A^T A$ 的特征向量 $v_1, v_2, v_3$ 组成 $V$。然后 $u_1 = \\frac{1}{\\sigma_1}Av_1$, $u_2 = \\frac{1}{\\sigma_2}Av_2$ 组成 $U$ (因为 $m=2, r=2$，所以 $U_2$ 不存在)。 PPT中似乎直接给出了一个分解，需要验证。它提到了 $A = U^T \\Lambda U$ (这更像是对称阵的谱分解，如果 $A$ 是对称的)。然后又提到了 $D = \\Lambda W^T$。这部分表述比较混乱，应遵循标准的SVD步骤。 例2 (几何变换): 变换 $S_1: x^2+y^2=1$ (单位圆) 到由矩阵 $A = \\begin{pmatrix} 2 &amp; 2 \\\\ 1 &amp; -1 \\end{pmatrix}$ 作用后的图形。 需要计算 $A$ 的SVD。 $A^T A = \\begin{pmatrix} 2 &amp; 1 \\\\ 2 &amp; -1 \\end{pmatrix} \\begin{pmatrix} 2 &amp; 2 \\\\ 1 &amp; -1 \\end{pmatrix} = \\begin{pmatrix} 5 &amp; 3 \\\\ 3 &amp; 5 \\end{pmatrix}$。特征值: $(5-\\lambda)^2 - 9 = 0 \\Rightarrow \\lambda^2 - 10\\lambda + 16 = 0 \\Rightarrow (\\lambda-8)(\\lambda-2)=0$。$\\lambda_1=8, \\lambda_2=2$。奇异值 $\\sigma_1 = \\sqrt{8} = 2\\sqrt{2}$, $\\sigma_2 = \\sqrt{2}$。$\\Sigma = \\begin{pmatrix} 2\\sqrt{2} &amp; 0 \\\\ 0 &amp; \\sqrt{2} \\end{pmatrix}$。 找到 $A^T A$ 的特征向量 $v_1, v_2$ 组成 $V$。 $u_1 = \\frac{1}{2\\sqrt{2}}Av_1$, $u_2 = \\frac{1}{\\sqrt{2}}Av_2$ 组成 $U$。 变换后的椭圆的主轴由 $u_1, u_2$ 决定，半轴长为 $\\sigma_1, \\sigma_2$。 例3 (特殊矩阵): $A = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix}$ (旋转 $-\\pi/2$) $A^T A = \\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} = I$。特征值 $\\lambda_1=1, \\lambda_2=1$。奇异值 $\\sigma_1=1, \\sigma_2=1$。 $\\Sigma = I$。 $V$ 可以是任意正交矩阵，例如 $V=I$。 则 $U = A V \\Sigma^{-1} = A I I = A = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix}$。 所以 $A = U \\Sigma V^T = A I I^T = A$。 PPT中给出的分解是 $A = \\begin{pmatrix} 0 &amp; 1 \\\\ -1 &amp; 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} -1 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix}$。这里的 $U = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{pmatrix}$, $V^T = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} -1 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix} \\Rightarrow V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} -1 &amp; 1 \\\\ 1 &amp; 1 \\end{pmatrix}$。$U$ 和 $V$ 都是正交的。这个分解也是有效的，说明SVD对于某些情况（如奇异值有重根）不是唯一的 (U和V的列可以乘以-1，或者在对应相同奇异值的子空间内旋转)。 6. 总结奇异值分解 $A = U \\Sigma V^T$ 是一个极其强大的工具，它： 适用于任何 $m \\times n$ 矩阵。 揭示了矩阵的内在几何结构和代数性质 (秩、四个基本子空间)。 提供了最佳低秩近似的方法。 在数据科学、工程、统计学等领域有广泛应用。","link":"/MATH1408/Chapter7-MATH1408.html"},{"title":"MATH1408","text":"","link":"/MATH1408/index.html"},{"title":"Chapter5-2 MATH1408","text":"λ-矩阵理论与矩阵标准型1. λ-矩阵 (λ-Matrix / Polynomial Matrix)1.1 定义 一个λ-矩阵 (或称多项式矩阵) 是指其元素是关于变量 λ 的多项式的矩阵。$A(\\lambda) = \\begin{pmatrix}a_{11}(\\lambda) &amp; a_{12}(\\lambda) &amp; \\dots &amp; a_{1n}(\\lambda) \\\\a_{21}(\\lambda) &amp; a_{22}(\\lambda) &amp; \\dots &amp; a_{2n}(\\lambda) \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\a_{m1}(\\lambda) &amp; a_{m2}(\\lambda) &amp; \\dots &amp; a_{mn}(\\lambda)\\end{pmatrix}$其中 $a_{ij}(\\lambda)$ 是数域 $F$ (通常是复数域 $\\mathbb{C}$ 或实数域 $\\mathbb{R}$) 上的 λ 的多项式。 λ-矩阵的次数是指其所有元素多项式中的最高次数。 1.2 λ-矩阵的初等变换 (Elementary Operations for λ-Matrices)λ-矩阵的初等变换类似于常数矩阵的初等变换，但涉及多项式： 第一类 (行/列交换): 交换 λ-矩阵的两行 (或两列)。 第二类 (行/列倍乘): 将 λ-矩阵的某一行 (或某一列) 乘以一个非零常数 $c \\in F, c \\ne 0$。 (注意：不是乘以一个λ的多项式，除非该多项式是可逆的，即非零常数)。 第三类 (行/列倍加): 将 λ-矩阵的某一行 (或某一列) 乘以一个任意的 λ-多项式 $f(\\lambda)$ 后加到另一行 (或另一列) 上。 1.3 λ-矩阵的等价 (Equivalence of λ-Matrices) 如果 λ-矩阵 $A(\\lambda)$ 可以通过一系列有限次 λ-矩阵初等变换得到 λ-矩阵 $B(\\lambda)$，则称 $A(\\lambda)$ 与 $B(\\lambda)$ 等价，记为 $A(\\lambda) \\sim B(\\lambda)$ 或 $A(\\lambda) \\xrightarrow{\\text{初等变换}} B(\\lambda)$。 等价关系具有反身性、对称性和传递性。 1.4 可逆λ-矩阵 (Unimodular λ-Matrix / Invertible λ-Matrix) 一个 $n \\times n$ 的 λ-方阵 $A(\\lambda)$ 称为可逆λ-矩阵 (或幺模λ-矩阵)，如果存在另一个 $n \\times n$ 的 λ-方阵 $B(\\lambda)$ 使得 $A(\\lambda)B(\\lambda) = B(\\lambda)A(\\lambda) = E_n$ (单位矩阵)。 定理: $n \\times n$ λ-方阵 $A(\\lambda)$ 是可逆λ-矩阵的充要条件是其行列式 $|A(\\lambda)|$ 是一个非零常数。 证明思路: ($\\Rightarrow$) 若 $A(\\lambda)B(\\lambda)=E_n$，则 $|A(\\lambda)||B(\\lambda)| = |E_n|=1$。因为 $|A(\\lambda)|$ 和 $|B(\\lambda)|$ 都是 λ 的多项式，它们相乘为1，则它们必须都是非零常数。 ($\\Leftarrow$) 若 $|A(\\lambda)|=c \\ne 0$ (常数)，则 $A(\\lambda)^{-1} = \\frac{1}{|A(\\lambda)|} \\text{adj}(A(\\lambda)) = \\frac{1}{c} \\text{adj}(A(\\lambda))$。由于伴随矩阵 $\\text{adj}(A(\\lambda))$ 的元素也是 λ 的多项式，所以 $A(\\lambda)^{-1}$ 是一个 λ-矩阵。□ 定理: 任何可逆λ-矩阵都可以表示为有限个初等λ-矩阵的乘积。初等λ-矩阵是指由单位矩阵经过一次λ-矩阵初等变换得到的矩阵。 定理: λ-矩阵 $A(\\lambda)$ 与 $B(\\lambda)$ 等价的充要条件是存在可逆λ-矩阵 $P(\\lambda)$ 和 $Q(\\lambda)$ 使得 $P(\\lambda)A(\\lambda)Q(\\lambda) = B(\\lambda)$。 1.5 矩阵多项式 (Polynomials of Matrices / Matrix Polynomials in a different sense)(这部分PPT的表述似乎混淆了“元素是多项式的矩阵”和“以矩阵为变量的多项式”，这里按前者理解，但提到了带余除法，这通常用于后者) 定义: 形如 $M(\\lambda) = M_m \\lambda^m + M_{m-1} \\lambda^{m-1} + \\dots + M_1 \\lambda + M_0$ 的表达式，其中 $M_i$ 是 $n \\times n$ 的常数矩阵，$M_m \\ne O$ (零矩阵)（如果 $m&gt;0$）。$m$ 称为该矩阵多项式的次数。 带余除法: 对于矩阵多项式 $M(\\lambda)$ 和 $N(\\lambda) = \\lambda E - A$ (其中 $A$ 是常数矩阵)，存在唯一的矩阵多项式 $Q(\\lambda)$ (商式) 和常数矩阵 $R$ (余式) 使得：$M(\\lambda) = Q(\\lambda)(\\lambda E - A) + R$ (右除法)且存在唯一的矩阵多项式 $S(\\lambda)$ (商式) 和常数矩阵 $T$ (余式) 使得：$N(\\lambda) = (\\lambda E - B)S(\\lambda) + T$ (这里应该是 $M(\\lambda) = (\\lambda E - B)S(\\lambda) + T$ 左除法) 证明思路 (右除法):设 $M(\\lambda) = M_m \\lambda^m + \\dots + M_0$。如果 $m=0$, $M(\\lambda)=M_0$, 则 $Q(\\lambda)=O, R=M_0$。如果 $m \\ge 1$。令 $Q_1(\\lambda) = M_m \\lambda^{m-1}$。$M(\\lambda) - Q_1(\\lambda)(\\lambda E - A) = (M_m \\lambda^m + \\dots) - (M_m \\lambda^m - M_m A \\lambda^{m-1}) = (M_{m-1} + M_m A)\\lambda^{m-1} + \\dots$这是一个次数严格小于 $m$ 的矩阵多项式。重复此过程，直到余式的次数为0 (即常数矩阵) 或余式为零矩阵。□ 推论 (矩阵余数定理): 矩阵多项式 $M(\\lambda)$ 右除以 $\\lambda E - A$ 的余矩阵是 $M(A) = M_m A^m + \\dots + M_0$ (即把 $A$ “代入” $M(\\lambda)$，但这里是右代入)。$M(\\lambda) = Q(\\lambda)(\\lambda E - A) + M(A)_{right}$左除以 $\\lambda E - A$ 的余矩阵是 $M(A)_{left}$ (左代入)。 2. 相似关系与λ-矩阵的等价 定理: $n \\times n$ 常数矩阵 $A$ 与 $B$ 相似 ($A \\sim B$) 的充要条件是它们的特征矩阵 $\\lambda E - A$ 与 $\\lambda E - B$ 作为 λ-矩阵是等价的。即，$A \\sim B \\iff \\lambda E - A \\sim \\lambda E - B$。 证明:($\\Rightarrow$) 若 $A \\sim B$，则存在可逆常数矩阵 $P$ 使得 $P^{-1}AP = B$。则 $P^{-1}(\\lambda E - A)P = \\lambda P^{-1}EP - P^{-1}AP = \\lambda E - B$。由于 $P$ 和 $P^{-1}$ 都是常数矩阵，它们也是可逆λ-矩阵 (行列式为非零常数)。因此 $\\lambda E - A \\sim \\lambda E - B$。($\\Leftarrow$) 若 $\\lambda E - A \\sim \\lambda E - B$，则存在可逆λ-矩阵 $M(\\lambda)$ 和 $N(\\lambda)$ 使得$M(\\lambda)(\\lambda E - A)N(\\lambda) = \\lambda E - B$ ()PPT中有一段较复杂的证明，试图说明 $M(\\lambda)$ 和 $N(\\lambda)$ 实际上必须是常数矩阵且 $N(\\lambda) = M(\\lambda)^{-1}$。关键步骤是利用带余除法：$M(\\lambda) = (\\lambda E - B)Q_M(\\lambda) + R_M$$N(\\lambda) = Q_N(\\lambda)(\\lambda E - A) + R_N$ (这里应该是 $(\\lambda E - B)Q_N(\\lambda) + R_N$ 或 $Q_N(\\lambda)(\\lambda E - A)$ for $N(\\lambda)^{-1}$)代入()并比较系数。PPT 的推导过程比较难以辨认，但最终结论是 $M(\\lambda)$ 和 $N(\\lambda)$ 必须是常数可逆矩阵 $P$ 和 $P^{-1}$。更标准的证明通常依赖于不变因子或初等因子理论：$\\lambda E - A \\sim \\lambda E - B$ 意味着它们有相同的Smith标准型，这进一步意味着它们有相同的不变因子，从而 $A$ 和 $B$ 相似。□ 3. λ-矩阵的Smith标准型 (Smith Normal Form) 定理 (Smith标准型定理): 任何一个非零的 $m \\times n$ λ-矩阵 $A(\\lambda)$ 都等价于一个唯一的对角形λ-矩阵：$S(\\lambda) = \\text{diag}(d_1(\\lambda), d_2(\\lambda), \\dots, d_r(\\lambda), 0, \\dots, 0)$$S(\\lambda) = \\begin{pmatrix}d_1(\\lambda) &amp; &amp; &amp; &amp; &amp; \\\\&amp; d_2(\\lambda) &amp; &amp; &amp; &amp; \\\\&amp; &amp; \\ddots &amp; &amp; &amp; \\\\&amp; &amp; &amp; d_r(\\lambda) &amp; &amp; \\\\&amp; &amp; &amp; &amp; 0 &amp; \\\\&amp; &amp; &amp; &amp; &amp; \\ddots \\\\&amp; &amp; &amp; &amp; &amp; &amp; 0\\end{pmatrix}$其中： $r = \\text{rank}(A(\\lambda))$ 是 $A(\\lambda)$ 的秩 (秩是指其行列式不恒为零的最高阶子式的阶数)。 每个 $d_i(\\lambda)$ 是首项系数为1的λ-多项式 (monic polynomial)。 $d_i(\\lambda)$ 整除 $d_{i+1}(\\lambda)$ (即 $d_i(\\lambda) | d_{i+1}(\\lambda)$) 对 $i=1, 2, \\dots, r-1$ 成立。 这个唯一的对角形矩阵 $S(\\lambda)$ 称为 $A(\\lambda)$ 的 Smith标准型。 $d_i(\\lambda)$ 称为 $A(\\lambda)$ 的不变因子 (invariant factors)。 对于特征矩阵 $\\lambda E - A$ ($A$ 是 $n \\times n$ 常数矩阵):其Smith标准型为：$S(\\lambda) = \\text{diag}(1, \\dots, 1, d_1(\\lambda), d_2(\\lambda), \\dots, d_m(\\lambda))$其中 $d_i(\\lambda)$ 是次数 $\\ge 1$ 的首1多项式，且 $d_i(\\lambda) | d_{i+1}(\\lambda)$。这里，前 $n-m$ 个不变因子是1。最后一个不变因子 $d_m(\\lambda)$ 实际上等于 $A$ 的最小多项式 $m_A(\\lambda)$ (如果规范化为首1)。所有不变因子的乘积 $\\prod d_i(\\lambda)$ (包括那些为1的) 等于 $\\lambda E - A$ 的行列式 (即 $A$ 的特征多项式 $f_A(\\lambda)$)，除了一个常数因子外。如果 $d_i(\\lambda)$ 都是首1的，则 $\\prod d_i(\\lambda) = f_A(\\lambda)$。 例子:PPT中给出了一个例子：$A = \\begin{pmatrix} 0 &amp; 1 &amp; -1 \\\\ 3 &amp; -2 &amp; 0 \\\\ -1 &amp; 1 &amp; -1 \\end{pmatrix}$$\\lambda E - A = \\begin{pmatrix} \\lambda &amp; -1 &amp; 1 \\\\ -3 &amp; \\lambda+2 &amp; 0 \\\\ 1 &amp; -1 &amp; \\lambda+1 \\end{pmatrix}$通过初等变换化为 Smith 标准型：$\\sim \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; (\\lambda-1)(\\lambda^2+4\\lambda+2) \\end{pmatrix}$(PPT中的结果是 $(\\lambda-1)(\\lambda^2+4\\lambda+2)$, 可能计算有误或抄录有误，标准做法应是 $(\\lambda-1)(\\lambda^2+4\\lambda+7)$ 或者其他形式，具体取决于 $A$。这里的多项式是 $f_A(\\lambda)$)。不变因子是: $1, 1, (\\lambda-1)(\\lambda^2+4\\lambda+2)$。 4. 行列式因子、不变因子与初等因子4.1 k阶行列式因子 ($D_k(\\lambda)$) 定义: λ-矩阵 $A(\\lambda)$ 的 $k$ 阶行列式因子 $D_k(\\lambda)$ 是 $A(\\lambda)$ 中所有 $k \\times k$ 子式的最大公因式 (GCD)，并规范化为首项系数为1。如果 $A(\\lambda)$ 中所有 $k \\times k$ 子式都为0，则 $D_k(\\lambda)=0$。约定 $D_0(\\lambda) = 1$。 性质: λ-矩阵初等变换不改变其各阶行列式因子 (这是一个非常重要的性质，用于证明Smith标准型的唯一性)。 $D_k(\\lambda) | D_{k+1}(\\lambda)$。 4.2 不变因子 ($d_k(\\lambda)$) 与行列式因子的关系 不变因子可以通过行列式因子计算得到：$d_1(\\lambda) = D_1(\\lambda)$$d_k(\\lambda) = \\frac{D_k(\\lambda)}{D_{k-1}(\\lambda)}$ for $k=2, \\dots, r$ (其中 $r = \\text{rank}(A(\\lambda))$) 这就是为什么 $d_k(\\lambda)$ 也被称为“不变的”，因为 $D_k(\\lambda)$ 在初等变换下不变。 4.3 初等因子 (Elementary Divisors) 定义: 将 λ-矩阵 $A(\\lambda)$ (通常指 $\\lambda E - A$) 的每个次数 $\\ge 1$ 的不变因子 $d_i(\\lambda)$ 在复数域 (或适当的数域) 上分解为互不相同的一次不可约因式 (即 $x-\\lambda_j$) 的幂的乘积：$d_i(\\lambda) = ( \\lambda - \\lambda_1 )^{e_{i1}} ( \\lambda - \\lambda_2 )^{e_{i2}} \\dots ( \\lambda - \\lambda_s )^{e_{is}}$所有这些形如 $(\\lambda - \\lambda_j)^{e_{ij}}$ 且 $e_{ij} &gt; 0$ 的因式称为 $A(\\lambda)$ (或对应矩阵 $A$) 的初等因子。 同一个 $(\\lambda - \\lambda_j)$ 的不同次幂若出现多次，则它们都是初等因子。 性质: 相似的常数矩阵有相同的初等因子集合 (因为它们有相同的不变因子)。 反之，如果两个常数矩阵有相同的初等因子集合，则它们相似 (在复数域上)。 不变因子与初等因子的关系: $d_r(\\lambda)$ (最后一个不变因子，对应最小多项式) 是所有不同根的初等因子的最高次幂的乘积。例如，如果初等因子是 $(\\lambda-\\lambda_1)^{k_1}, (\\lambda-\\lambda_1)^{j_1}, (\\lambda-\\lambda_2)^{k_2}$ (其中 $k_1 \\ge j_1$)，则 $d_r(\\lambda)$ 包含 $(\\lambda-\\lambda_1)^{k_1}$ 和 $(\\lambda-\\lambda_2)^{k_2}$。 $d_{r-1}(\\lambda)$ 是除去每个根的最高次幂后，剩余初等因子中每个根的最高次幂的乘积。 以此类推，直到 $d_1(\\lambda)$。 简单来说，对每个素因子 $p(\\lambda) = \\lambda - \\lambda_j$，将其在所有不变因子分解式中的指数 $e_{ij}$ 按非增序列排列，这些 $p(\\lambda)^{e_{ij}}$ ($e_{ij}&gt;0$) 就是初等因子。 5. Frobenius 标准型 (Rational Canonical Form / Frobenius Normal Form) 友矩阵 (Companion Matrix): 对于首1多项式 $p(\\lambda) = \\lambda^r + a_{r-1}\\lambda^{r-1} + \\dots + a_1\\lambda + a_0$，其友矩阵定义为：$F_p = \\begin{pmatrix}0 &amp; 0 &amp; \\dots &amp; 0 &amp; -a_0 \\\\1 &amp; 0 &amp; \\dots &amp; 0 &amp; -a_1 \\\\0 &amp; 1 &amp; \\dots &amp; 0 &amp; -a_2 \\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\0 &amp; 0 &amp; \\dots &amp; 1 &amp; -a_{r-1}\\end{pmatrix}_{r \\times r}$(友矩阵有几种形式，这是常见的一种。另一种是将负系数放在第一行)。 性质: 友矩阵 $F_p$ 的特征多项式和最小多项式都是 $p(\\lambda)$。其不变因子是 $1, \\dots, 1, p(\\lambda)$。 Frobenius 标准型定理: 任何一个 $n \\times n$ 复 (或实) 矩阵 $A$ 都相似于一个分块对角矩阵 $F = \\text{diag}(F_1, F_2, \\dots, F_k)$，其中每个 $F_i$ 是对应于 $A$ 的一个非常数不变因子 $d_i(\\lambda)$ (即次数 $\\ge 1$ 的那些) 的友矩阵。$F = \\begin{pmatrix} F_{d_1(\\lambda)} &amp; &amp; \\\\ &amp; F_{d_2(\\lambda)} &amp; \\\\ &amp; &amp; \\ddots \\\\ &amp; &amp; &amp; F_{d_k(\\lambda)} \\end{pmatrix}$其中 $d_1(\\lambda) | d_2(\\lambda) | \\dots | d_k(\\lambda)$ 是 $\\lambda E - A$ 的非常数首1不变因子。这个 $F$ 称为 $A$ 的 Frobenius 标准型 (或有理标准型，因为它可以在定义矩阵的域 $F$ 上实现，不需要扩域)。 Frobenius 标准型在不计块的排列顺序下是唯一的。 $A$ 的最小多项式是 $d_k(\\lambda)$ (最后一个最大的不变因子)。 $A$ 的特征多项式是 $\\prod_{i=1}^k d_i(\\lambda)$。 例子:如果 $A$ 的不变因子是 $1, 1, \\lambda-1, (\\lambda-1)^2, (\\lambda-1)^2(\\lambda+1)$。非常数不变因子是 $d_1(\\lambda) = \\lambda-1$, $d_2(\\lambda) = (\\lambda-1)^2 = \\lambda^2-2\\lambda+1$, $d_3(\\lambda) = (\\lambda-1)^2(\\lambda+1) = (\\lambda^2-2\\lambda+1)(\\lambda+1) = \\lambda^3 - \\lambda^2 - \\lambda + 1$。Frobenius 标准型为:$F = \\text{diag} \\left(\\begin{pmatrix} 1 \\end{pmatrix},\\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 2 \\end{pmatrix},\\begin{pmatrix} 0 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\end{pmatrix}\\right)$(这里友矩阵的系数是对应多项式中除去最高次项后各项系数的相反数)。 6. Jordan 标准型 (Jordan Canonical Form / Jordan Normal Form)6.1 从不变因子到初等因子 (回顾)如前所述，将每个不变因子 $d_i(\\lambda)$ 分解成素多项式 (在复数域上是 $\\lambda - \\lambda_j$) 的幂。这些幂就是初等因子。 6.2 Jordan 块 对应于初等因子 $(\\lambda - \\lambda_0)^r$ 的 Jordan 块是一个 $r \\times r$ 矩阵：$J_r(\\lambda_0) = \\begin{pmatrix}\\lambda_0 &amp; 1 &amp; &amp; &amp; \\\\&amp; \\lambda_0 &amp; 1 &amp; &amp; \\\\&amp; &amp; \\ddots &amp; \\ddots &amp; \\\\&amp; &amp; &amp; \\lambda_0 &amp; 1 \\\\&amp; &amp; &amp; &amp; \\lambda_0\\end{pmatrix}$ Jordan 块 $J_r(\\lambda_0)$ 的唯一初等因子是 $(\\lambda - \\lambda_0)^r$。其不变因子也是 $1, \\dots, 1, (\\lambda-\\lambda_0)^r$。 6.3 Jordan 标准型定理 任何一个 $n \\times n$ 复矩阵 $A$ (或定义在代数闭域上的矩阵) 都相似于一个分块对角矩阵 $J$，称为 $A$ 的 Jordan 标准型:$J = \\text{diag}(J_1, J_2, \\dots, J_s)$其中每个 $J_k$ 是一个 Jordan 块，对应于 $A$ 的一个初等因子。具体地，如果 $A$ 的初等因子集合是 $\\{ (\\lambda-\\lambda_1)^{r_1}, (\\lambda-\\lambda_2)^{r_2}, \\dots, (\\lambda-\\lambda_s)^{r_s} \\}$ (这里 $\\lambda_i$ 可能相同)，则 Jordan 标准型由 Jordan 块 $J_{r_1}(\\lambda_1), J_{r_2}(\\lambda_2), \\dots, J_{r_s}(\\lambda_s)$ 组成。 Jordan 标准型在不计 Jordan 块排列顺序的情况下是唯一的。 $A$ 的特征多项式是所有初等因子的乘积。 $A$ 的最小多项式是每个不同根 $(\\lambda-\\lambda_j)$ 对应的最高次幂初等因子的乘积。 6.4 从 Smith 标准型/不变因子直接得到 Jordan 标准型如果 $\\lambda E - A$ 的 Smith 标准型是 $\\text{diag}(1, \\dots, 1, d_1(\\lambda), \\dots, d_m(\\lambda))$。对每个非常数不变因子 $d_i(\\lambda)$，将其分解为 $d_i(\\lambda) = \\prod_j (\\lambda - \\lambda_{ij})^{e_{ij}}$。则 $A$ 的 Jordan 标准型由对应于每个 $(\\lambda - \\lambda_{ij})^{e_{ij}}$ 的 Jordan 块 $J_{e_{ij}}(\\lambda_{ij})$ 构成。(这种说法不完全准确，更准确的是通过所有初等因子来构造)。正确做法是： 求出 $\\lambda E - A$ 的所有初等因子。 对每个初等因子 $(\\lambda - \\lambda_0)^k$，Jordan 标准型中就有一个 $k \\times k$ 的 Jordan 块 $J_k(\\lambda_0)$。 6.5 例子如果 $A$ 的初等因子是 $(\\lambda-1)^2, (\\lambda-1)^2, \\lambda+2, (\\lambda-1)$。(这里特征值 1 对应几何重数为3，代数重数为 $2+2+1=5$；特征值 -2 对应几何重数和代数重数都为1)。Jordan 标准型是 (不计顺序):$J = \\text{diag}( J_2(1), J_2(1), J_1(-2), J_1(1) )$$J = \\text{diag} \\left(\\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix},\\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix},\\begin{pmatrix} -2 \\end{pmatrix},\\begin{pmatrix} 1 \\end{pmatrix}\\right)$ 6.6 寻找相似变换矩阵 P ($P^{-1}AP=J$)PPT中提到了如何通过解方程组 $(A-\\lambda_i E)x = 0$ (找特征向量) 和 $(A-\\lambda_i E)x = y$ (找广义特征向量，形成Jordan链) 来构造矩阵 $P$。对于每个 Jordan 块 $J_k(\\lambda_0)$，对应 $P$ 中的 $k$ 列向量 $v_1, \\dots, v_k$ (Jordan 链)，满足：$(A-\\lambda_0 E)v_1 = 0$$(A-\\lambda_0 E)v_2 = v_1$…$(A-\\lambda_0 E)v_k = v_{k-1}$然后 $P = (v_1, v_2, \\dots, v_k, \\dots)$。 总结与关系 λ-矩阵的初等变换 是核心工具。 Smith标准型 给出了λ-矩阵在等价关系下的最简形式，其对角线元素是不变因子。 行列式因子 用于计算不变因子。 不变因子 分解得到初等因子。 对于常数矩阵 $A$，其特征矩阵 $\\lambda E - A$ 的 不变因子 决定了 $A$ 的 Frobenius 标准型。 初等因子 决定了 $A$ 的 Jordan 标准型 (在代数闭域上)。 Frobenius 标准型在原域上总存在，Jordan 标准型需要域是代数闭的 (例如复数域)。","link":"/MATH1408/Chapter5-2-MATH1408.html"},{"title":"图论","text":"图论核心概念1. 基本假设与符号 默认图类型: 除非特别说明，所有讨论的图 $G$ 都是简单图 (simple) 和无向图 (undirected)。 顶点集 (Vertex Set): 用 $V(G)$ 表示图 $G$ 中所有顶点的集合。 边集 (Edge Set): 用 $E(G)$ 表示图 $G$ 中所有边的集合。 顶点数 (Number of Vertices): 用 $v_G$ 或 $v(G)$ 表示，其值为 $|V(G)|$。这也被称为图的阶 (order)。 边数 (Number of Edges): 用 $e_G$ 或 $e(G)$ 表示，其值为 $|E(G)|$。 重要约定:一个图 $G$ 的 尺寸 (size) 特指其顶点数 $v(G)$。这需要特别注意，在其他文献中，“size”有时也可能指边数 $e(G)$。 2. 图的密度 (Density) 密度 (Density) 定义: $d(G) = e_G / v_G$ 含义: 平均每个顶点拥有多少条边。 注意: 这个值等于图的平均度数的一半。有些作者将密度定义为平均度数，其值会是这里的两倍。 最大密度 (Maximum Density) 定义: $m(G) = \\max_{H \\subseteq G} d(H)$ 含义: 在图 $G$ 的所有子图 $H$ 中，能找到的最大的密度值。 相对密度 (Relative Density) 定义: $\\rho(G) = e(G) / \\binom{v(G)}{2}$ 含义: 图中实际存在的边数与一个具有相同顶点数的完全图 (complete graph) 的边数之比。这个值总是在 0 和 1 之间，直观地反映了图的“稠密程度”。 3. 重要的图参数 最小度 (Minimum Degree): $\\delta(G)$，图中所有顶点度的最小值。 最大度 (Maximum Degree): $\\Delta(G)$，图中所有顶点度的最大值。 色数 (Chromatic Number): $\\chi(G)$，对图的顶点进行正常着色（相邻顶点颜色不同）所需的最少颜色数。 退化数 (Degeneracy Number): $D(G) = \\max_{H \\subseteq G} \\delta(H)$，所有子图的最小度的最大值。 稳定数/独立数 (Stability/Independence Number): $\\alpha(G)$，图中最大独立集的大小（独立集是指一个顶点子集，其中任意两个顶点之间都没有边）。 自同构数 (Number of Automorphisms): $\\text{aut}(G)$，图的对称性度量，指保持图结构不变的顶点置换的数量。 4. 邻域与度 (Neighborhoods and Degrees)这些概念描述了顶点及其直接连接。 邻域 (Neighborhood) 一个顶点 $v$ 的（开）邻域 $N(v)$ 或 $N_G(v)$ 是所有与 $v$ 相邻的顶点的集合。 数学定义: $N(v) = \\{w \\in V(G) : vw \\in E(G)\\}$ 度 (Degree) 一个顶点 $v$ 的度 $\\text{deg}(v)$ 是其邻域的大小，即与它直接相连的边的数量。 关系: $\\text{deg}(v) = |N(v)|$ 集合的邻域 (Neighborhood of a Set) 一个顶点集 $S \\subseteq V(G)$ 的邻域 $N_G(S)$ 是指在 $S$ 之外，但与 $S$ 中至少一个顶点相邻的所有顶点的集合。 数学定义: $N_G(S) = \\left( \\bigcup_{v \\in S} N_G(v) \\right) \\setminus S$ 闭邻域 (Closed Neighborhoods) “闭”邻域只是在“开”邻域的基础上加上了其自身。 顶点的闭邻域: $\\bar{N}_G(v) = N_G(v) \\cup \\{v\\}$ 集合的闭邻域: $\\bar{N}_G(S) = N_G(S) \\cup S$ 5. 特殊图类与子图基本图类型 空图 (Empty Graph): 指有顶点但没有任何边的图。 零图 (Null Graph): 用 $\\emptyset$ 表示，指既没有顶点也没有边的图。 常见的图家族 (Graph Families) 完全图 (Complete Graph, $K_n$): 包含 $n$ 个顶点，且任意两个不同的顶点之间都有一条边相连。 完全二分图 (Complete Bipartite Graph, $K_{m,n}$): 包含 $m+n$ 个顶点，这些顶点被划分为两个集合（大小分别为 $m$ 和 $n$），其中第一个集合中的每个顶点都与第二个集合中的每个顶点相连，但同一集合内的顶点之间没有连接。 圈 (Cycle, $C_k$): 包含 $k$ 个顶点，排列成一个闭合的环路。 路 (Path, $P_k$): 包含 $k$ 条边和 $k+1$ 个顶点的图，形成一条不闭合的路径。 星图 (Star): 任何形如 $K_{1,n}$ ($n \\ge 0$) 的图。它有一个中心顶点，连接着 $n$ 个叶子顶点。 匹配 (Matching): 一个由若干条孤立的边组成的森林。可以表示为 $jK_2$ 的形式，其中 $j \\ge 0$ 是边的数量。 不交并 (Vertex-Disjoint Union, $jG$): 由 $j$ 个顶点互不相交的图 $G$ 的副本组成的图。 6. 子图 (Subgraphs)子图的两种主要类型 导出子图 (Induced Subgraph, $G[V]$): 也被称为由顶点集 $V$ 生成 (spanned) 的子图。 定义: 给定一个顶点子集 $V \\subseteq V(G)$，其导出子图 $G[V]$ 的顶点集就是 $V$，边集则包含原图 $G$ 中两个端点都在 $V$ 中的所有边。 边集: $E(G) \\cap [V]^2$。 核心思想: 选择一部分顶点，并保留这些顶点之间所有的原始连接。 生成子图 (Spanning Subgraph, $G[E]$): 定义: 给定一个边子集 $E’ \\subseteq E(G)$，其生成子图 $G[E’]$ 拥有与原图 $G$ 完全相同的顶点集 $V(G)$，但其边集被限制为 $E(G) \\cap E’$。 核心思想: 保留所有的顶点，但只保留一部分边。最著名的例子是生成树 (Spanning Tree)。 相关记法与概念 集合内的边数: $e_G(V)$ 或 $e(V)$ 表示导出子图 $G[V]$ 中的边数。 集合间的边数: 对于两个不相交的顶点集 $A, B \\subseteq V(G)$，$e_G(A,B)$ 表示一个端点在 $A$ 中、另一个端点在 $B$ 中的边的数量。 副本 (Copy): 在一个大图 $F$ 中，图 $G$ 的一个副本是指 $F$ 的一个子图，该子图与 $G$ 同构 (isomorphic)。这个子图不一定是导出子图。 导出副本 (Induced Copy): $F$ 的一个子图，它既与 $G$ 同构，同时也是 $F$ 的一个导出子图。这是一个更强的条件，意味着副本的顶点之间不存在不属于该副本的“额外”边。","link":"/RandomGraph/graphtheory.html"},{"title":"贝尔曼最优公式","text":"Bellman optimality equation (elementwise form) (BOE) $$ \\begin{aligned} v(s)&=\\max_{\\pi}\\sum_{a}\\pi(a|s)[\\sum_r p(r|s,a)r+\\gamma\\sum_{s'}p(s'|s,a)v(s')]\\\\ &=\\max_{\\pi}\\sum_a\\pi(a|s)q(s,a) \\end{aligned} $$ 类似Bellman公式，但这里$\\pi(s)$是未知的，需要求解得到最优策略 Bellman optimality equation (matrix-vector form) $$ \\begin{aligned} v=\\max_{\\pi}(r_{\\pi}+\\gamma P_{\\pi}v) \\end{aligned} $$ where the elements corresponding to $s$ or $s’$ are $$ \\begin{aligned} [r_{\\pi}]_s &\\triangleq\\sum_a\\pi(a|s)\\sum_r p(r|s,a)r,\\\\ [P_{\\pi}]_{s,s'}=p(s|s')&\\triangleq\\sum_a\\pi(a|s)\\sum_{s'}p(s'|s,a) \\end{aligned} $$ Contraction mapping(压缩映射):f is a contraction mapping if $||f(x_1)-f(x_2)||\\leq\\gamma||x_1-x_2||$ where $\\gamma\\in (0,1)$.Contraction mapping theorem(压缩映射原理):For any equation that has the form of $x=f(x)$, if $f$ is a contraction mapping, then: Existence: there exists a fixed point $x^{*}$ satisfying $f(x^{*})=x^{*}$. Uniqueness: the fixed point $x^{*}$ is unique. Algorithm: consider a sequence $\\{x_k\\}$ where $x_{k+1}=f(x_k)$, then $x_k\\rightarrow x^{*}$ as $k\\rightarrow\\infty$. Moreover, the convergence rate is exponentially fast. For BOE, \\begin{aligned} v=f(v)=\\max_{\\pi}(r_{\\pi}+\\gamma P_{\\pi}v) \\end{aligned}Theorem(Contraction Property)$f(v)$ is a contraction mapping satisfying $||f(v_1)-f(v_2)||\\leq \\gamma||v_1-v_2||$ where $\\gamma$ is the discount rate. Theorem(Exitence,Uniqueness, and Algorithm)For the BOE $v=f(v)=\\max_{\\pi}(r_{\\pi}+\\gamma P_{\\pi}v)$, there always exists a solution $v^{*}$ and the solution is unique. The solution could be solved iteratively by \\begin{aligned} v_{k+1}=f(v_k)=\\max_{\\pi}(r_{\\pi}+\\gamma P_{\\pi}v_k) \\end{aligned}This sequence $\\{v_k\\}$ converges to $v^{*}$ exponentially fast given any initial guess $v_0$. The convergence rate is determined by $\\gamma$. \\begin{aligned} v^*=r_{\\pi}^*+\\gamma P_{\\pi^*}v^* \\end{aligned}Theorem(Policy Optimality)Suppose that $v^{*}$ is the unique solution to $v=\\max_{\\pi}(r_{\\pi}+\\gamma P_{\\pi}v)$, and $v_{\\pi}$ is the state value function satisfying $v_{\\pi}=r_{\\pi}+\\gamma P_{\\pi}v_{\\pi}$ for any given policy $\\pi$, then $v^{*}\\geq v_{\\pi},\\forall\\pi$ Optimal polity $\\pi^*$ has the form:For any $s\\in S$, the deterministic greedy policy $$ \\begin{aligned} \\pi^*(a|s)=\\begin{cases} 1,&a=a^*(s)\\\\0,&a\\neq a^*(s) \\end{cases} \\end{aligned} $$ is an optimal policy solving the BOE.Here $a^{*}(s)=\\arg\\max_aq^{*}(a,s)$, where $q^{*}(s,a):=\\sum_r p(r|s,a)r+\\gamma\\sum_{s’}p(s’|s,a)v^{*}(s’)$.","link":"/RL/BOE.html"},{"title":"ODE专栏","text":"会更新就有鬼了💩","link":"/ode/index.html"},{"title":"随机图","text":"","link":"/RandomGraph/index.html"},{"title":"值函数的近似","text":"idea: Approximate the state and action values using parameterized functions: $\\hat{v}(s,w)\\approx v_\\pi(s)$, where $w\\in R^m$ is the parameter vector. Advantage:(1) Storage: The dimension of $w$ may be much less than $|\\mathcal{S}|$.(2) Generalization: the learned value can generated to unvisited states. Algorithm for state value estimationObjective function$v_\\pi(s)$: the true value.$\\hat{v}(s,w)$: a function of approximation. The objective function is $J(w)=E[(v_\\pi(S)-\\hat{v}(S,w))^2]$ goal: to find the best $w$ that can minimize $J(w)$ the first way is to use a uniform distribution.setting the probability of each state as $1/|S|$.the objective function becomes $J(w)=1/|S|\\sum_{s\\in S}(v_\\pi(S)-\\hat{v}(S,w))^2$缺点：所有状态同等重要（并不符合实际情况） The second way is to use the stationary distribution. it describes the long-run behavior. Let $\\{d_\\pi(s)\\}_{s\\in S}$ denote the stationary distribution of the Markov process unger policy $\\pi$.反映了达到平稳状态后某个状态被访问的概率为多大(需要算法运行很长时间后才可以达到) The objective function can be rewritten as $J(w)=E[(v_\\pi(S)-\\hat{v}(S,w))^2]=\\sum_{s\\in S}d_\\pi(s)(v_\\pi(s)-\\hat{v}(s,w))^2$.(这里是加权平均，权重大小是该状态在稳态过程中被访问的概率大小) Optimization algorithm该算法用于估计一个给定策略的state value.Use the graident-descent algorithm: $w_{k+1}=w_k-\\alpha_k\\nabla_w J(w_k)$The true gradient is $$\\begin{aligned} \\nabla_w J(w)&=\\nabla_w E[(v_\\pi(S)-\\hat{v}(S,w))^2]\\\\ &= E[\\nabla_w(v_\\pi(S)-\\hat{v}(S,w))^2]\\\\ &=2 E[(v_\\pi(S)-\\hat{v}(S,w))(-\\nabla_w\\hat{v}(S,w))] \\\\&=-2 E[(v_\\pi(S)-\\hat{v}(S,w))(\\nabla_w\\hat{v}(S,w))] \\end{aligned} $$ use stochastic gradient to replace the true gradient:$w_{t+1}=w_t+\\alpha_t (v_\\pi(s_t)-\\hat{v}(s_t,w_t))\\nabla_w\\hat{v}(s_t,w_t)$这个算法需要$v_\\pi(s_t)$的真实值，所以是不可行的Usa Monte Carlo or TD learning with function approximation to approximate $v_t(s_t)$ Pseudocode:TD learning with function approximation: Initialization: A function $\\hat{v}(s,w)$ that is a differentiable in $w$. Iniitial parameter $w_0$. Aim: Approximate the true state values of a given policy $\\pi$. For each epsiode generated following the policy $\\pi$, do&emsp;For each step $(s_t,r_{t+1},s_{t+1})$, do&emsp;&emsp;In general case, $w_{t+1}=w_{t}+\\alpha_t[r_{t+1}+\\gamma\\hat{v}(s_{t+1},w_t)-\\hat{v}(s_t,w_t)]\\nabla_w \\hat{v}(s_t,w_t)$&emsp;&emsp;In linear case, $w_{t+1}=w_{t}+\\alpha_t[r_{t+1}+\\gamma \\phi ^T(s_{t+1})w_t-\\phi^T(s_t)w_t]\\phi(s_t)$. Selection of function approximators use a linear function $\\hat{v}(s,w)=\\phi^T(s)w$ to approximate approximate $\\hat{v}(s,w)$$\\nabla_w \\hat{v(s,w)}=\\phi(s)$.劣势：需要选取何时的feature vectors.优势：理论性质可以被比较好的分析；有比较强的表征能力，可以表征大部分函数. 神经网络（非线性） Sarsa with function approximation估计action value.The sarsa algorithm with value function approximation is:$w_{t+1}=w_t+\\alpha_t[r_{t+1}+\\gamma \\hat{q}(s_{t+1},a_{t+1},w_t)-\\hat{q}(s_t,a_t,w_t)]\\nabla_w\\hat q(s_t,a_t,w_t)$. Pseudocode:Sarsa with function approximation: Aim: Search a policy that can lead the agent to the target from an initial state-action pair $(s_0,a_0)$. For each epsiode do&emsp;If the current $s_t$ is not the target state, do&emsp;&emsp;Take action $a_t$ following $\\pi_t(s_t)$, generate $r_{t+1},s_{t+1}$, and then take action $a_{t+1}$ following $\\pi_t(s_{t+1})$.&emsp;&emsp;Value update (parameter update):&emsp;&emsp;$w_{t+1}=w_t+\\alpha_t[r_{t+1}+\\gamma \\hat{q}(s_{t+1},a_{t+1},w_t)-\\hat{q}(s_t,a_t,w_t)]\\nabla_w\\hat q(s_t,a_t,w_t)$.&emsp;&emsp;Policy update:&emsp;&emsp;$\\pi_{t+1}(a|s_t)=1-\\frac{\\epsilon}{|\\mathcal{A}(s)|}(|\\mathcal{A}(s)|-1)$ if $a=argmax_{a\\in\\mathcal{A}(s_t)}\\hat q(s_t,a,w_{t+1})$.&emsp;&emsp;$\\pi_{t+1}(a|s_t)=\\frac{\\epsilon}{|\\mathcal{A}(s)|}$ otherwise. Q-learning with function approximationThe q-value update rule is $w_{t+1}=w_t+\\alpha_t[r_{t+1}+\\gamma \\underset{a\\in\\mathcal{A}(s_{t+1})}\\max\\hat{q}(s_{t+1},a,w_t)-\\hat{q}(s_t,a_t,w_t)]\\nabla_w\\hat q(s_t,a_t,w_t)$. Pseudocode:Sarsa with function approximation: Aim: Search a policy that can lead the agent to the target from an initial state-action pair $(s_0,a_0)$. For each epsiode do&emsp;If the current $s_t$ is not the target state, do&emsp;&emsp;Take action $a_t$ following $\\pi_t(s_t)$, generate $r_{t+1},s_{t+1}$.&emsp;&emsp;Value update (parameter update):&emsp;&emsp;$w_{t+1}=w_t+\\alpha_t[r_{t+1}+\\gamma \\underset{a\\in\\mathcal{A}(s_{t+1})}\\max\\hat{q}(s_{t+1},a,w_t)-\\hat{q}(s_t,a_t,w_t)]\\nabla_w\\hat q(s_t,a_t,w_t)$.&emsp;&emsp;Policy update:&emsp;&emsp;$\\pi_{t+1}(a|s_t)=1-\\frac{\\epsilon}{|\\mathcal{A}(s)|}(|\\mathcal{A}(s)|-1)$ if $a=argmax_{a\\in\\mathcal{A}(s_t)}\\hat q(s_t,a,w_{t+1})$.&emsp;&emsp;$\\pi_{t+1}(a|s_t)=\\frac{\\epsilon}{|\\mathcal{A}(s)|}$ otherwise. Deep Q-learning(DQN)最成功的将深度神经网络引入强化学习的方法.神经网络扮演的角色是非线性函数. DQN aims to minimize the objective function/loss function:$J(w)=E\\Big[\\Big(R+\\gamma\\underset{a\\in\\mathcal{A(S’)}}\\sum \\hat q(S’,a,w)-\\hat q(S,A,w)\\Big)^2\\Big]$, where $(S,A,R,S’)$ are random variables.Let $y:=R+\\gamma\\underset{a\\in\\mathcal{A(S’)}}\\sum \\hat q(S’,a,w)$. For the sake of simplicity, we can assume that $w$ in $y$ is fixed(at least for a while) when we calculate the gradient. To do that, we can introduce 2 networks: a main network representing $\\hat q(s,a,w)$. a target network $\\hat q(s,a,w_T)$.The objective function in this case degenerated to $J=E\\Big[\\Big(R+\\gamma\\underset{a\\in\\mathcal{A(S’)}}\\sum \\hat q(S’,a,w_T)-\\hat q(S,A,w)\\Big)^2\\Big]$ where $w_T$ is the target network parameter.When $w_T$ is fixed, the gradient of $J$ can be easily obtained as$\\nabla_w J=E\\Big[\\Big(R+\\gamma\\underset{a\\in\\mathcal{A(S’)}}\\sum \\hat q(S’,a,w_T)-\\hat q(S,A,w)\\Big)\\nabla_w\\hat{q}(S,A,w)\\Big]$ Implementation details: Let $w$ and $w_T$ denote the parameters of the main and target networks, respectively. They are set to be the same initially. In every iteration, we draw a mini-batch of samples $\\{(s,a,r,s’)\\}$ from the replay buffer. The inputs of the networks include state $s$ and action $a$. The target output is $y_T=R+\\gamma \\max_{a\\in\\mathcal{A}}\\hat q(s’,a,w_T)$. Then we directly minimize the TD error or called loss function $(y_T-\\hat q(s,a,w))^2$ over the mini-match $\\{(s,a,y_T)\\}$. Experience replay:将所有数据放在一个集合中$\\mathcal{B}:=\\{(s,a,r,s’)\\}$每次需要训练神经网络，我们从该集合中取数据.为什么要用经验回放：$J(w)=E\\Big[\\Big(R+\\gamma\\underset{a\\in\\mathcal{A(S’)}}\\sum \\hat q(S’,a,w)-\\hat q(S,A,w)\\Big)^2\\Big]$ $(S,A)\\sim d$ $R\\sim p(R|S,A),S’\\sim p(S’|S,A)$ The distribution of $(S,A)$ is assumed to be uniform. But the samples are not uniformly collected. So we can break the correlation between consequent samples by experience replay method.","link":"/RL/DQL.html"},{"title":"对角高斯策略详解","text":"这份笔记将详细解释在强化学习中，什么是对角高斯策略，以及它的两种主流实现方式、采样方法和对数似然计算。 1. 基础概念：从多维高斯到对角高斯 多维高斯分布 (Multivariate Gaussian Distribution): 一个通用的多维高斯分布由两个核心参数描述： 一个均值向量 $μ$，它表示分布的中心位置。 一个协方差矩阵 $Σ$，它描述了各个维度之间的相关性。 对角高斯分布 (Diagonal Gaussian Distribution): 这是多维高斯分布的一个特例和简化。 在这种分布中，我们假设所有维度都是相互独立的。 这意味着它的协方差矩阵 $Σ$ 只有对角线上的元素（各个维度的方差）不为零，其余所有元素都为零。 因此，我们不再需要一个完整的矩阵来表示它，只需要一个向量（例如，标准差向量 $σ$）就足够了。这个简化使得学习过程变得更加容易和高效。 2. 对角高斯策略的核心结构在强化学习中，一个对角高斯策略总是包含一个神经网络。这个网络的核心任务是： 接收一个环境观察 (observation) $s$，并输出对应这个状态下的“最佳”平均动作，即均值向量 $μ_θ(s)$。 这里的下标 $θ$ 代表神经网络的参数。策略的学习过程就是调整 $θ$，使得网络输出的 $μ_θ(s)$ 越来越好。 3. 如何表示标准差 $σ$：两种主流方法均值 $μ_θ(s)$ 由网络输出，但标准差 $σ$ (它决定了探索的范围) 是如何产生的呢？主要有两种方式： 方法一：与状态无关的对数标准差 (Standalone Parameters) 核心思想: 网络的任务只是计算均值 $μ_θ(s)$。标准差 $σ$ 与当前的状态 $s$ 无关，它是一个独立的、全局共享的可学习参数。 具体实现: 创建一个神经网络，将观察 $s$ 映射到均值动作 $μ_θ(s)$。 另外，独立创建一个向量参数 $log σ$。这个向量的维度与动作空间维度相同，它会像网络权重一样，在训练过程中被优化器更新。 特点: 无论智能体处于什么状态，其探索的“不确定性”或“随机性”程度都是固定的。 应用: 这种方法简单且稳定，是 VPG、TRPO 和 PPO 等经典算法的标准实现方式。 方法二：由网络输出的、与状态相关的对数标准差 核心思想: 标准差 $σ$ 也依赖于当前状态 $s$。这意味着智能体可以学会在不同的状态下采取不同程度的探索。 具体实现: 神经网络接收观察 $s$ 后，通常会有两个“输出头”(output heads)。 一个头输出均值动作 $μ_θ(s)$。 另一个头输出对数标准差 $log σ_θ(s)`。 这两个头可以共享网络前面的大部分层（特征提取层）。 特点: 策略更加灵活。例如，在熟悉的、安全的环境中，网络可以学会输出一个很小的 $σ$（低探索，高确定性）；在陌生的、危险的环境中，则可以输出一个较大的 $σ$（高探索，高不确定性）。 应用: SAC (Soft Actor-Critic) 等现代算法常采用此方法。 4. 关键技巧：为什么使用对数标准差 ($log σ$)？这是一个非常重要的实现细节。我们让网络输出 $log σ$ 而不是直接输出 $σ$，原因如下： 约束问题: 标准差 $σ$ 的定义要求它必须是非负数 ($σ ≥ 0$)。如果让神经网络直接输出 $σ$，我们就必须在网络的最后一层加一个激活函数（比如 ReLU 或 Softplus）来保证输出为正。这会给训练带来一些麻烦（比如梯度消失）。 log 技巧: 对数 $log σ$ 的取值范围是 $(-\\infty, \\infty)$。这正好是神经网络线性层输出的自然范围！因此可以让网络毫无约束地输出 $log σ$。 恢复 $σ$: 当我们拿到网络输出的 $log σ$ 后，只需要做一个指数运算 $σ = \\exp(log σ)$ 就可以得到真正的、为正数的 $σ$。指数函数 $\\exp(x)$ 的值域永远是正数，所以这样得到的 $σ$ 就自动满足了大于零的约束。 这个技巧解除了对网络输出的约束，让训练过程更简单、更稳定。 5. 如何从策略中采样动作 (Reparameterization Trick)一旦我们有了均值 $μ_θ(s)$ 和标准差 $σ_θ(s)$，就可以通过以下步骤来采样一个具体的动作 $a$: 从一个标准正态分布（均值为0，标准差为1）中采样一个与动作维度相同的噪声向量 $z$。即 $z \\sim \\mathcal{N}(0, I)$，其中 $I$ 是单位矩阵。 使用下面的公式计算最终的动作 $a$:$a = μ_θ(s) + σ_θ(s) \\odot z$这里的 $\\odot$ 表示逐元素相乘 (elementwise product)。 这个方法被称为重参数化技巧 (Reparameterization Trick)。它的巨大优势在于，它将随机性（来自 $z$）与网络参数 $θ$ 分离开来，使得梯度能够顺利地从动作 $a$ 反向传播到网络参数 $θ$，这对训练至关重要。 在PyTorch中，你可以使用 torch.normal() 或者 torch.distributions.Normal 对象来轻松实现这一过程。 6. 计算对数似然 (Log-Likelihood)在很多策略梯度算法中，我们需要计算在给定状态 $s$ 下，采取某个动作 $a$ 的对数概率 $log π_θ(a|s)$。对于一个 $k$ 维的对角高斯策略，其对数似然由以下公式给出： $\\log \\pi_{\\theta}(a|s) = -\\frac{1}{2}\\left(\\sum_{i=1}^k \\left(\\frac{(a_i - \\mu_i)^2}{\\sigma_i^2} + 2 \\log \\sigma_i \\right) + k \\log 2\\pi \\right)$ 这个公式直观上衡量了动作 $a$ 与分布中心 $μ$ 的“距离”（由 $σ$ 标准化后），并考虑了分布本身的“宽度”（$log σ_i$ 项）。这个值在策略更新时至关重要。","link":"/RL/Gassian.html"},{"title":"值迭代与策略迭代","text":"Value iteration: it is the iterative algorithm solving the Bellman optimality: given an initial value $v_0$, $$ \\begin{aligned} v_{k+1}=\\max_\\pi(r_{\\pi}+\\gamma P_{\\pi}v_{\\pi})\\\\ \\begin{cases} Policy\\ update:\\pi_{k+1}=argmax_{\\pi}(r_{\\pi}+\\gamma P_{\\pi}v_k)\\\\ Value\\ update:v_{k+1}=r_{\\pi_{k+1}}+\\gamma P_{\\pi_{k+1}}v_k \\end{cases} \\end{aligned} $$ Policy iteration: given an initial policy $\\pi_0$, $$ \\begin{aligned} \\begin{cases} Policy\\ evaluation:v_{\\pi_k}=r_{\\pi_k}+\\gamma P_{\\pi_k}v_{\\pi_k}\\\\ Policy\\ improvement:\\pi_{k+1}=argmax_{\\pi}(r_{\\pi}+\\gamma P_\\pi v_{\\pi_k}) \\end{cases} \\end{aligned} $$ Compare value iteration and policy iteration In policy iteration, solving $v_{\\pi_k}=r_{\\pi_k}+\\gamma P_{\\pi_k}v_{\\pi_k}$ requires an iterative algorithm (an infinite number of iterations). In value iteration,$v_{k+1}=r_{\\pi_{k+1}}+\\gamma P_{\\pi_{k+1}}v_k$ is a one-step iteration. 注意到Policy iteration算法在计算$v_{\\pi_k}=r_{\\pi_k}+\\gamma P_{\\pi_k}v_{\\pi_k}$要迭代无穷多次才可以逼近$v^{*}$，所以该算法只在理论上可行. 如果将迭代次数控制在$j$次，就产生了Truncated policy iteration算法. Pseudocode of Truncated policy iterationInitialization: The probability model $p(r|s,a)$ and $p(s’|s,a)$ for all $(s,a)$ are known. Initial guess $\\pi_0$.Aim: Search for the optimal state value and an optimal policy.While the policy has not converged, for the $k$-th iteration, do Policy evaluation:Initialization: select the initial guess as $v_k^{(0)}=v_{k-1}$. The maximum iteration is set to be $j_{truncate}$.While $j\\lt j_{truncate}$, do&emsp;For every state $s\\in S$, do&emsp;&emsp;$v_k^{(j+1)}(s)=\\sum_a\\pi_k(a|s)[\\sum_r p(r|s,a)r+\\gamma\\sum_{s’}p(s’|s,a)v_k^{(j)}(s’)]$ Set $v_k=v_k^{(j_{truncate})}$. Policy improvement:For every state $s\\in S$, do&emsp;For every action $a\\in \\mathcal{A}(s)$, do&emsp;&emsp;$q_k(s,a)=\\sum_r p(r|s,a)r+\\gamma\\sum_{s’}p(s’|s,a)v_k(s’)$&emsp;&emsp;$a_k^{*}(s)=argmax_a q_k(s,a)$ &emsp;&emsp;$\\pi_{k+1}(a|s)=1$ if $a=a_k^{*}$, and $\\pi_{k+1}(a|s)=0$ otherwise. 可以看到Value iteration和Policy iteration是Truncated policy iteration将$j_{truncated}$设置为$1$和$\\infty$的两个特殊版本.可以证明，一般的$j_{truncated}$值下每轮迭代计算得到的优化结果位于Value iteration和Policy iteration之间，因此也是收敛的.","link":"/RL/PIVI.html"},{"title":"核心优化问题","text":"1. 强化学习的最终目标无论我们如何衡量回报（无论是无限时间范围的折扣回报，还是有限时间范围的无折扣回报），也无论我们选择何种策略，强化学习（RL）的最终目标始终是： 找到一个策略，当智能体依据这个策略行动时，能够最大化其期望回报。 这个目标是所有强化学习算法驱动力的核心。 2. 理解期望回报：轨迹的概率为了讨论“期望回报”，我们首先必须能够量化一个“轨迹”（trajectory）发生的可能性。一个轨迹 $τ$ 是一个状态和动作的序列，例如 $τ = (s_0, a_0, s_1, a_1, …)$。 假设环境的动态（状态转移）和智能体的策略都是随机的。在这种情况下，一个包含 $T$ 个时间步的轨迹 $τ$ 发生的概率，取决于三个关键部分： 初始状态的概率 每一步状态转移的概率 每一步选择动作的概率 将这三者结合，我们得到一个轨迹的完整概率公式： $P(\\tau|\\pi) = \\rho_0(s_0) \\prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t)$ 让我们分解这个公式的每个部分： $\\rho_0(s_0)$: 初始状态分布。它描述了在最开始时，环境处于状态 $s_0$ 的概率。 $P(s_{t+1} | s_t, a_t)$: 环境动力学模型。它描述了在状态 $s_t$ 下执行动作 $a_t$ 后，环境转移到下一个状态 $s_{t+1}$ 的概率。这部分由环境决定，智能体无法改变。 $π(a_t | s_t)$: 智能体的策略。它描述了在状态 $s_t$ 下，智能体选择动作 $a_t$ 的概率。这是我们通过学习来优化的部分。 3. 期望回报的正式定义有了轨迹的概率，我们就可以正式地定义期望回报 $J(π)$。它是在策略 $π$ 下，所有可能轨迹的回报 $R(τ)$ 的加权平均值，权重就是每个轨迹发生的概率 $P(τ|π)$。 其数学表达有两种等价形式： 积分形式:$J(\\pi) = \\int_{\\tau} P(\\tau|\\pi) R(\\tau)$这表示对所有可能的轨迹 $τ$ 进行积分（或求和），将每个轨迹的回报 $R(τ)$ 与其发生的概率 $P(τ|π)$ 相乘。 期望形式 (更紧凑的写法):$J(\\pi) = \\underset{\\tau \\sim \\pi}{\\mathbb{E}}[R(\\tau)]$ 4. RL的核心优化问题综上所述，强化学习的中心优化问题可以被简洁地表述为： $\\pi^{*} = \\arg \\max_{\\pi} J(\\pi)$ 这里的符号含义是： $\\pi^{*}$：代表最优策略 (optimal policy)。 $\\arg \\max_{\\pi}$：这个算子意味着我们要找到一个参数（在这里是策略 $π$），使得后面的表达式（在这里是期望回报 $J(π)$）达到最大值。 简而言之，强化学习的全部工作，就是寻找那个能使期望回报 $J(π)$ 最大化的最优策略 $\\pi^{*}$。","link":"/RL/RLPro.html"},{"title":"蒙特卡洛方法","text":"Three algorithm: MC Basic MC Exploring Starts MC $\\epsilon$-Greedy MA Basic2 expression of action value: requires the model:&emsp;$q_{\\pi_k}(s,a)=\\underset{r}{\\sum}p(r|s,a)r+\\gamma\\underset{s’}{\\sum}p(s’|s,a)v_{\\pi_k}(s’)$ does not require the model(model-free):&emsp;$q_{\\pi_k}(s,a)=E[G_t|S_t=s,A_t=a]$&emsp;We can use expression 2 to calculate $q_{\\pi_k}(s,a)$ based on data (samples or experiences)总结：计算Action value的两种方法，条件和数据必须二者有其一. The procedure of Monte Carlo estimation of action values: Starting from $(s,a)$, following policy $\\pi_k$, generate an episode. The return of this episode is $g(s,a)$ $g(s,a)$ is a sample of $G_t$ in $q_{\\pi_k}(s,a)=E[G_t|S_t=s,A_t=a]$ Suppose we have a set of episodes and hence $\\{g^{(j)}(s,a)\\}. Then, $q_{\\pi_k}(s,a)=E[G_t|S_t=s,A_t=a]\\approx\\frac{1}{N}\\underset{i=1}{\\overset{N}\\sum}g^{(i)}(s,a)$.用数据估计$q_{\\pi_k}(s,a)$. 对于每个$(s,a)$都需要有足够多的episodes计算得到$q_{\\pi_k}(s,a)$ MC Basic效率很低，在实际中用处不大. MC Exploring Starts想要优化MC Basic算法可以从两方面进行调整： 1. Use data more efficiently尝试更加充分的利用每个episode种被访问到的数据.Visit: every time a state-action appears in the episode, it is called a visited of that state-action pair. Initial-visit method:只估计initial visit的action value Data-efficient method: first-visit method: 只有一个episode中$(s,a)$的第一次出现可以被用来估计$(s,a)$的action value. every-visit method: 只要在episode中出现了，就可以被用来估计anction value. 2. Update value estimate more efficiently The first method: 等从一个$(s,a)$出发的所有episodes都被收集完成，再进行$(s,a)$action value的估算，然后使用计算结果更新$\\pi$.&emsp; MC Basic 使用该方法. 需要等待所有episodes收集完成，费时间，效率低. The second method：使用单个episodes的返回值去近似action value，每个episode都可以改进策略，效率提升. Pseudocode: MC Exploring Starts(a sample-efficient variant of MC Basic) Initialization: Initial guess $\\pi_0$. Aim: Search for an optimal policy. For each episode,do Episode generation:","link":"/RL/MC.html"},{"title":"随即近似与随机梯度下降","text":"Mean estimation: compute $E[X]$ using $\\{x_k\\}$, $w_{k+1}=w_k-\\frac{1}{k}(w_k-x_k).$ RM algorithm: solve $g(w)=0$ using $\\{\\widetilde{g}(w_k,\\eta_k)\\}$, $w_{k+1}=w_k-a_k\\widetilde{g}(w_k,\\eta_k)$.Theorem(Robbins-Monro Theorem)In the Robbins-Monro algorithm, if(1) $0\\lt c_1\\leq\\nabla_wg(w)\\leq c_2,\\forall w$;(2) $\\underset{k=1}{\\overset{\\infty}\\sum}a_k=\\infty$ and $\\underset{k=1}{\\overset{\\infty}\\sum}a_k^2\\lt\\infty$;(3) $E[\\eta_k|\\mathcal{H}_k]=0$ and $E[\\eta_k^2|\\mathcal{H}_k]\\lt \\infty$;where $\\mathcal{H}_k=\\{w_k,w_{k-1},\\cdots\\}$, then $w_k$ converges with probability $1$ to the root $w^{*}$ satisfying $g(w^{*})=0$. SGD algorithm: minimize $J(w)=E[f(w,X)]$ using $\\{\\nabla_w f(w_k,x_k)\\}$, $w_{k+1}=w_k-\\alpha_k\\nabla_w f(w_k,x_k)$. Suppose we would like to minimize $J(w)=E[f(w,X)]$ given a set of random samples $\\{x_i\\}_{i=1}^n$ of $X$. The BGD,SGD,MBGD algorithms solving this problem are, respectively,(1) BGD: $w_{k+1}=w_k-\\alpha_k\\frac{1}{n}\\underset{i=1}{\\overset{n}\\sum}\\nabla_w f(w_k,x_i)$.(2) MBGD: $w_{k+1}=w_k-\\alpha_k\\frac{1}{m}\\underset{j\\in\\mathcal{I}_k}\\sum\\nabla_w f(w_k,x_j)$.(3) SGD: $w_{k+1}=w_k-\\alpha_k\\nabla_kf(w_k,x_k)$. In the BGD algorithm, all the samples are used in every iteration. when n is large,$\\frac{1}{n}\\underset{i=1}{\\overset{n}\\sum}\\nabla_w f(w_k,x_i)$ is close to the true-gradient.In the MBGD algorithm, $\\mathcal{I}_k$ is a subset of $\\{1,\\cdots, n\\}$ withe size as $|\\mathcal{I}_k|=m$. The set $\\mathcal{I}_k$ is obtained by $m$ times i.d.d samplings.In the SGD algorithm, $x_k$ is randomly sampled from $\\{x_i\\}_{i=1}^n$ at time $k$. If $m=1$, MBGD becomes SGD.If $m=n$, MBGD does not become BGD strictly speaking because MBGD uses randomly fetched $n$ samples whereas BGD usws all $n$ numbers. In particular, MBGD may use a value in $\\{x_i\\}_i=1^n$ multiple times whereas BGD uses each number one.","link":"/RL/SGD.html"},{"title":"时序差分算法","text":"1. TD learning of state valueThe data/experience required by the algorithm: $(s_0,r_1,s_1,\\cdots,s_t,r_{t+1},s_{t+1})$ or $\\{(s_t,r_{t+1},s_{t+1})\\}_t$ renerated following the given policy $\\pi$. The TD learning algorithm is:&emsp;$v_{t+1}(s_t)=v_t(s_t)-\\alpha_t(s_t)\\Big[v_t(s_t)-[r_{t+1}+\\gamma v_t(s_{t+1})]\\Big]$,(1)&emsp;$v_{t+1}(s)=v_t(s),\\forall s\\neq s_t$,(2)where $t=0,1,2,\\cdots$. Here, $v_t(s_t)$ is the estimated state value of $v_{\\pi}(s_t)$; $\\alpha_t(s_t)$ is the learning rate of $s_t$ at time $t$. This algorithm drives $v(s_t)$ towards $\\bar v_t$.Other property :The algorithm only estimate state values. It can’t estimate the action values and optimal values. Theorem (Convergence of TD learning)By the TD algorithm(1), $v_t(s)$ converges with probability 1 to $v_\\pi(s)$ for all $s\\in \\mathcal{S}$ as $t\\rightarrow \\infty$ if $\\sum_t\\alpha_t(s)=\\infty$ and $\\sum_t\\alpha_t^2(s)\\lt\\infty$ for all $s\\in\\mathcal{S}$. TD/Sarsa learning vs MC learning: TD/Sarsa MC learning Online: it can update the state/action values immediately after receiving a reward. Offline: has to wait until an epsiode has been completely collection Continuing tasks: handle both episodic and continuing tasks Episodic tasks: only handle tasks that has terminate states. Bootstrapping: TD bootstraps because the update of a value relies on the previous estimate of this value. Hence it require initial guess. Non-bootstrapping: MC is not bootstrapping, because it can directly estimate state/action values without any initial guess. Low estimation variance（方差小）: there are fewer random variables. High estimation variance（方差大）: need a lot of random variables. 2. TD learning of action values: SarsaSuppose we have some experience $\\{(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})\\}$$q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\\alpha_t(s_t,a_t)\\Big[q_t(s_t,a_t)-[r_{t+1}+\\gamma q_t(s_{t+1},a_{t+1})]\\Big]$,(1)&emsp;$q_{t+1}(s,a)=q_t(s,a),\\forall (s,a)\\neq (s_t,a_t)$,(2)where $t=0,1,2,\\cdots$. Here, $q_t(s_t,a_t)$ is the estimated state value of $q_{\\pi}(s_t,a_t)$; $\\alpha_t(s_t,a_t)$ is the learning rate of $s_t,a_t$ at time $t$. Sarsa is the abbreviation of state-action-reward-state-action.Sarsa is an action-value version of the TD algorithm. Theorem (Convergence of Sarsa learning)By the Sarsa algorithm, $q_t(s,a)$ converges with probability $1$ to $q_\\pi(s,a)$ as $t\\rightarrow \\infty$ for all $(s,a) $ if $\\sum_t\\alpha_t(s,a)=\\infty$ and $\\sum_t\\alpha_t^2(s,a)\\lt\\infty$ for all $(s,a)$. 3. Expected Sarsa$q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\\alpha_t(s_t,a_t)\\Big[q_t(s_t,a_t)-(r_{t+1}+\\gamma E[q_t(s_{t+1},A)])\\Big]$,$q_{t+1}(s,a)=q_t(s,a),where $E[q_t(s_{t+1},A)]=\\underset{a}\\sum \\pi_t(a|s_{t+1})q_t(s_{t+1},a)=v_t(s_{t+1}) is the expected value of $q_t(s_{t+1},a)$ under policy $\\pi_t$. Need more computation than Sarsa. But it is beneficial in the sense that it reduces the estimation variances because it reduces random variables in Sarsa from $\\{s_t,a_t,r_{t+1},s_{t+1},a_{t+1}\\}$ to $\\{s_t,a_t,r_{t+1},s_{t+1}\\}$. Expected Sarsa is a stochastic approximation for solving the following equation:$q_\\pi(s,a)=E\\Big[R_{t+1}+\\gamma E_{A_{t+1}\\sim\\pi(S_{t+1})}[q_\\pi(S_{t+1},A_{t+1})]\\Big|S_t=s,A_t=a\\Big],\\forall s,a$. n-step SarsaIt can unify Sarsa and Monte Carlo learning.Sarsa aims to solve: $q_\\pi(s,a)=E[G_t^{(1)}|s,a]=E[R_{t+1}+\\gamma q_\\pi(S_{t+1},A_{t+1})|s,a]$.MC learning aims to solve: $q_\\pi(s,a)=E[G_t^{(\\infty)}|s,a]=E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\cdots|s,a]$.An intermediate algorithm called n-step Sarsa aims to solve:$q_\\pi(s,a)=E[G_t^{(n)}|s,a]=E[R_{t+1}+\\gamma R_{t+2}+\\cdots+\\gamma^nq_\\pi(S_{t+n},A_{t+n}|s,a]$. The algorithm of n-step Sarsa is:$q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\\alpha_t(s_t,a_t)\\Big[q_t(s_t,a_t)-[r_{t+1}+\\gamma r_{t+2}+\\cdots+ \\gamma^n q_t(s_{t+n},a_{t+n})])\\Big]$.n-step Sarsa is more general because it becomes the (one-step) Sarsa algorithm when $n=1$ and the MC learning algorithm becomes MC when $n=\\infty$. n-step Sarsa needs $(s_t,a_t,r_{t+1},s_{t+1},a_{t+1},\\cdots,r_{t+n},s_{t+n},a_{t+n})$. TD learning odf optimal action value: Q-learning直接估计最优action value.The Q-learning algorithm is&emsp;$q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\\alpha_t(s_t,a_t)\\Big[q_t(s_t,a_t)-[r_{t+1}+\\gamma \\underset{a\\in\\mathcal{A}}\\max q_t(s_{t+1},a)]\\Big]$,&emsp;$q_{t+1}(s,a)=q_t(s,a),\\forall (s,a)\\neq (s_t,a_t). It aims to solve $q(s,a)=E[R_{t+1}+\\gamma\\underset{a}q(S_{t+1},a)\\Big|S_t=s,A_t=a],\\forall s,a$.This is the Bellman optimality equation expressed in trems of action values. There exists 2 policies in TD learning task: The behavior policy is used to generate experience samples. The target policy is constantly update toward an optimal policy. On-policy vs off-policy: When the behavior policy is the same as the target policy, such king of learning is called on policy.(Sarsa) When they are different, the learning is called off-policy.(Q-learning) Advantages: It can search for optimal policies based on the experience samples generated by any other policies. Sarsa is on-policy: Sarsa aims to solve the Bellman equation of a given policy $\\pi$. The algorithm requires $(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})$. $\\pi_t$ is both the target policy and the behavior policy. Mont Carlo learning is on-policy: aims to estimate action values with a long epsiode. $\\pi\\rightarrow exp$ A policy is used to generate samples, which is further used to estimate the action values of the policy. Based on the action values, we can improved the policy(target). Q-learning is off-policy: Aims to solve the Bellman optimality equation(显式不含有任何策略$\\pi$) requires$(s_t,a_t,r_{t+1},s_{t+1})$. The behavior policy to generate $a_t$ from $s_t$ can be anything. The target policy is the optimal policy. Q-learning can be implemented in either off-policy or on-policy fashion. A unified point of viewuniform expression:&emsp;$q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\\alpha_t(s_t,a_t)[q_t(s_t,a_t)-\\bar{q}_t]$,where $\\bar{q}_t is the TD target. Different TD algorithm have different $\\bar{q}_t$. Sarsa: $\\bar{q}_t=r_{t+1}+\\gamma q_t(s_{t+1},a_{t+1})$ $n$-step Sarsa: $\\bar{q}_t=r_{t+1}+\\gamma r_{t+2}+\\cdots+\\gamma^nq_t(s_{t+n},a_{t+n})$ Expected Sarsa: $\\bar{q}_t=r_{t+1}+\\gamma \\sum_a \\pi_t(a|s_{t+1})q_t(s_{t+1},a)$. Q-learning: $\\bar{q}_t=r_{t+1}+\\gamma \\max_a q_t(s_{t+1},a)$. Monte Carlo: $\\bar{q}_t=r_{t+1}+\\gamma r_{t+2}+\\cdots$.","link":"/RL/TDL.html"},{"title":"贝尔曼公式","text":"State value: $v_{\\pi}(s)=E[G_t|S_t=s]$Action value: $q_{\\pi}(s,a)=E[G_t|S_t=s,A_t=a]$ The Bellman equation(elementwise form): $$ \\begin{aligned} v_{\\pi}(s) &=\\sum_{a}\\pi(a|s)[\\sum_r p(r|s,a)r+\\gamma\\sum_{s'}p(s'|s,a)v_{\\pi}(s')]\\\\ &=\\sum_a\\pi(a|s)q_{\\pi}(s,a) \\end{aligned} $$ The Bellman equation(matrix-vector form) $$ \\begin{aligned} v_{\\pi}=r_{\\pi}+\\gamma P_{\\pi}v_{\\pi} \\end{aligned} $$ How to solve the Bellman equation: closed-form solution:需要求逆，不推荐 iterative solution: 构造迭代序列满足关系$v_{k+1}=r_{\\pi}+\\gamma P_{\\pi}v_k$，序列极限即为所求","link":"/RL/bellman.html"},{"title":"高等代数专栏","text":"专栏简介（本专栏收录了主包MATH1408课程的全部笔记）高等代数这门课程涉及范围很广，内容涵盖多项式理论、矩阵理论、欧几里得空间等。初学者可能觉得这门课缺乏系统性 (主包个人感觉部分章节内部前后逻辑也很难串通). 好在cz的考试比课堂难度低很多，虽然涉及的知识点比较广（建议考前遍历ppt），但是不太核心的内容掌握度要求并不高，基本上只要知道概念就能做对. 本专栏收录的笔记几乎涵盖ppt中全部知识点，部分内容有所拓展。本人在期末速通期间感觉这篇专栏用起来还是蛮顺手的，遂留存，希望对选修MATH1408的同学在课程学习或期末复习有所帮助。 专栏目录 第一章：多项式的基本概念 第二章：线代基础 第三章：行列式与矩阵理论进阶 第四章：映射与线性映射 第五章：矩阵的相似标准形 5.1 矩阵多项式与Jordan标准型 5.2 λ-矩阵理论与矩阵标准型 第六章：矩阵函数及其应用 第七章：奇异值分解 第八章：对偶空间、伴随算子与双线性型 参考资料 《高等代数》 丘维声著 另建议大家期末复习时买一本高代考研题辅助（期末卷偏考研风，但会比考研卷简单很多）","link":"/gaoshu/index.html"},{"title":"强化学习专栏","text":"专栏简介强化学习记录贴~ 专栏目录 贝尔曼公式 贝尔曼最优公式 值迭代与策略迭代 蒙特卡洛方法 随机近似与随机梯度下降 时序差分方法 值函数的近似 OpenAI Spinning Up Program英文教程链接友善一点的中文版教程链接（非官方） Part 1:Key Concepts in RL 核心优化问题 对角高斯策略详解 价值函数 最优策略、贝尔曼方程与优势函数","link":"/RL/index.html"},{"title":"最优策略、贝尔曼方程与优势函数","text":"1. 最优Q函数与最优动作最优动作价值函数 $Q^{*}(s,a)$ 与最优策略所选择的动作之间存在一个至关重要的联系。根据定义，$Q^{*}(s,a)$ 是指：从状态 $s$ 开始，执行一个（任意）动作 $a$，然后永远遵循最优策略行动，所能获得的期望回报。 因此，在状态 $s$ 下的最优策略，会选择那个能够最大化从 $s$ 出发期望回报的动作。这意味着，如果我们拥有最优Q函数 $Q^{*}$，我们就能直接地、贪心地得到最优动作 $a^{*}(s)$。 $a^{*}(s) = \\arg \\max_a Q^{*} (s,a)$ 注意: 可能会有多个动作都能使 $Q^{*}(s,a)$ 达到最大值。在这种情况下，这些动作都是最优的，最优策略可以随机选择其中任意一个。但总存在一个最优策略，它会确定性地选择其中一个动作。 2. 贝尔曼方程 (Bellman Equations)之前介绍的四种价值函数都遵循一种特殊的自洽方程，称为贝尔曼方程。贝尔曼方程背后的基本思想是： 一个点的价值 = 从该点获得的即时奖励 + 你将到达的下一个点的价值。 在策略 (On-Policy) 价值函数的贝尔曼方程 状态价值函数 $V^{\\pi}(s)$:$V^{\\pi}(s) = \\underset{a \\sim \\pi, s’\\sim P}{\\mathbb{E}}[r(s,a) + \\gamma V^{\\pi}(s’)]$ 动作价值函数 $Q^{\\pi}(s,a)$:$Q^{\\pi}(s,a) = \\underset{s’\\sim P}{\\mathbb{E}}[r(s,a) + \\gamma \\underset{a’\\sim \\pi}{\\mathbb{E}}[Q^{\\pi}(s’,a’)]]$ 最优 (Optimal) 价值函数的贝尔曼方程 最优状态价值函数 $V^{*}(s)$:$V^{*}(s) = \\max_a \\underset{s’\\sim P}{\\mathbb{E}}[r(s,a) + \\gamma V^{*}(s’)]$ 最优动作价值函数 $Q^{*}(s,a)$:$Q^{*}(s,a) = \\underset{s’\\sim P}{\\mathbb{E}}[r(s,a) + \\gamma \\max_{a’} Q^{*}(s’,a’)]$ 公式简写说明: $s’ \\sim P$ 是 $s’ \\sim P(\\cdot |s,a)$ 的简写，表示下一个状态 $s’$ 是根据环境的转移概率采样的。 $a \\sim \\pi$ 是 $a \\sim \\pi(\\cdot|s)$ 的简写，表示动作 $a$ 是根据策略 $π$ 在状态 $s$ 下采样的。 $a’ \\sim \\pi$ 是 $a’ \\sim \\pi(\\cdot|s’)$ 的简写，表示下一个动作 $a’$ 是根据策略 $π$ 在状态 $s’$ 下采样的。 $\\gamma$ 是折扣因子 (discount factor)。 核心区别:在策略贝尔曼方程和最优贝尔曼方程之间的关键区别在于是否存在对动作的 $\\max$ 操作。$\\max$ 操作的出现，反映了一个事实：为了达到最优，智能体在任何可以选择动作的时刻，都必须选择那个能带来最高价值的动作。 关键知识点：贝尔曼备份 (Bellman Backup)在强化学习文献中，“贝尔曼备份”是一个非常常见的术语。一个状态或状态-动作对的贝尔曼备份，指的就是其贝尔曼方程的右侧部分，即“奖励 + 折扣后的下一状态价值”。 3. 优势函数 (Advantage Functions)在强化学习中，有时我们不需要知道一个动作在绝对意义上有多好，而只需要知道它比当前策略的平均水平好多少。换言之，我们关心的是这个动作的相对优势。这个概念通过优势函数来精确描述。 对应于策略 $π$ 的优势函数 $A^{\\pi}(s,a)$，描述了在状态 $s$ 下，采取特定动作 $a$ 比遵循策略 $π$ 随机选择一个动作要好多少（假设之后都遵循策略 $π$ 行动）。 数学定义:$A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$","link":"/RL/optimalfunc.html"},{"title":"价值函数 (Value Functions)","text":"1. 什么是价值函数？在强化学习中，评估一个状态（state）或者一个状态-动作对（state-action pair）有多“好”是非常有用的。这里的“好”，我们用价值（value）来衡量，其具体含义是： 从某个状态或状态-动作对开始，遵循一个特定策略，未来能够获得的期望回报。 价值函数是几乎所有强化学习算法的基石，以各种形式被广泛使用。 2. 四种主要的价值函数这里有四种核心的价值函数需要我们了解： 1. 在策略状态价值函数 (On-Policy Value Function) 符号: $V^{\\pi}(s)$ 定义: 如果从状态 $s$ 开始，并且始终遵循策略 $π$ 来行动，所能获得的期望回报。 公式:$V^{\\pi}(s) = \\underset{\\tau \\sim \\pi}E\\{R(\\tau)\\left| s_0 = s\\right.\\}$ 2. 在策略动作价值函数 (On-Policy Action-Value Function) 符号: $Q^{\\pi}(s,a)`` 定义: 如果从状态 $s$ 开始，执行一个任意的动作 $a$（这个动作 $a$ 不一定来自策略 $π$），然后从那以后永远遵循策略 $π$ 行动，所能获得的期望回报。 公式:$Q^{\\pi}(s,a) = \\underset{\\tau \\sim \\pi}E\\{R(\\tau)\\left| s_0 = s, a_0 = a\\right.\\}$ 3. 最优状态价值函数 (Optimal Value Function) 符号: $V^{*}(s)$ 定义: 如果从状态 $s$ 开始，并且始终遵循环境中的最优策略行动，所能获得的期望回报。它代表了从状态 $s$ 出发能获得的最大可能的期望回报。 公式:$V^{*}(s) = \\max_{\\pi} \\underset{\\tau \\sim \\pi}E\\{R(\\tau)\\left| s_0 = s\\right.\\}$ 4. 最优动作价值函数 (Optimal Action-Value Function) 符号: $Q^{*}(s,a)$ 定义: 如果从状态 $s$ 开始，执行一个任意动作 $a$，然后从那以后永远遵循最优策略行动，所能获得的期望回报。它代表了从状态 $s$ 执行动作 $a$ 后能获得的最大可能的期望回报。 公式:$Q^{*}(s,a) = \\max_{\\pi} \\underset{\\tau \\sim \\pi}E\\{R(\\tau)\\left| s_0 = s, a_0 = a\\right.\\}$ 3. 关键知识点关键点一：关于时间依赖性 当我们讨论价值函数而没有特别提及时间依赖时，我们通常默认指的是无限时间范围的折扣回报 (infinite-horizon discounted return)。 对于有限时间范围的无折扣回报 (finite-horizon undiscounted return)，价值函数则必须将时间作为一个参数。在有限时间范围问题中，离终点越近，能获得的回报就越少，因此一个状态的价值是依赖于当前是第几步的。） 关键点二：V函数与Q函数的核心关联状态价值函数（V-function）和动作价值函数（Q-function）之间存在两个非常重要的联系： 在策略（On-Policy）情况下的关联:$V^{\\pi}(s) = \\underset{a\\sim \\pi}E\\{Q^{\\pi}(s,a)\\}$解释: 一个状态的价值，等于在该状态下，遵循策略 $π$ 采取所有可能动作的Q值的期望。换句话说，就是将每个动作的Q值与其被策略选中的概率相乘，然后求和（或积分）。 最优（Optimal）情况下的关联:$V^{*}(s) = \\max_a Q^{*} (s,a)$解释: 一个状态的最优价值，等于在该状态下，采取所有可能动作中Q值最大的那一个动作的Q值。这里不再是求期望，而是直接选择最好的那一个。","link":"/RL/valuefunc.html"}]}